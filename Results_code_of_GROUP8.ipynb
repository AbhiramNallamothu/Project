{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbhiramNallamothu/Project/blob/main/Results_code_of_GROUP8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtXNx8d_1jop",
        "outputId": "deb29c8e-7054-43af-8af3-3a1e22dc7eac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: bert-score in /usr/local/lib/python3.10/dist-packages (0.3.13)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: summa in /usr/local/lib/python3.10/dist-packages (1.2.0)\n",
            "Requirement already satisfied: bert-extractive-summarizer in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bert-score) (2.2.1+cu121)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert-score) (2.0.3)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from bert-score) (4.35.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bert-score) (1.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert-score) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.10/dist-packages (from bert-score) (4.66.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert-score) (3.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert-score) (24.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (from bert-extractive-summarizer) (3.7.4)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.8.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert-score) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert-score) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert-score) (2024.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.0.0->bert-score) (12.4.127)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (0.17.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (0.4.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (3.1.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (8.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score) (2024.2.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (6.4.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (2.7.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (67.7.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (3.4.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy->bert-extractive-summarizer) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->bert-extractive-summarizer) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->bert-extractive-summarizer) (2.18.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy->bert-extractive-summarizer) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy->bert-extractive-summarizer) (0.1.4)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy->bert-extractive-summarizer) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->bert-score) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->bert-score) (1.3.0)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->bert-extractive-summarizer) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install rouge bert-score scikit-learn summa bert-extractive-summarizer rouge-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vTGyK2E72WoV"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.translate.meteor_score import single_meteor_score\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from rouge_score import rouge_scorer\n",
        "from summa import summarizer\n",
        "from summarizer import Summarizer\n",
        "from bert_score import score\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "\n",
        "\n",
        "# Path to the Excel file\n",
        "file_path = \"/content/INFO5731_GROUP8.xlsx\"\n",
        "\n",
        "# Read the Excel file into a DataFrame\n",
        "df = pd.read_excel(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8TNWqpw2ZFO",
        "outputId": "5354a3d1-901e-4407-c3b0-bee5dbe9b924"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       patent_id                                    patent document  \\\n",
            "0      US9615024   In various aspects systems and methods for co...   \n",
            "1      US9619138   Service providers and device manufacturers (e...   \n",
            "2      US9619863  Various aspects of examples of examples embodi...   \n",
            "3      US9621542  Thus improving security of UID and how to util...   \n",
            "4       S9621796  According to various but not necessarily all e...   \n",
            "...          ...                                                ...   \n",
            "1094   US8214649  The present invention discloses a system and m...   \n",
            "1095   US8218421   In one embodiment of the present invention a ...   \n",
            "1096   US8223227  These and other problems are generally solved ...   \n",
            "1097   US8230035  An apparatus for capturing an electronic image...   \n",
            "1098   US8230212  The present invention is directed to addressin...   \n",
            "\n",
            "                                                summary  \n",
            "0     The patent describes systems and methods for p...  \n",
            "1     The patent describes a method and apparatus fo...  \n",
            "2     The patent US9619863 discloses a method, appar...  \n",
            "3     The invention detailed in the provided text ad...  \n",
            "4     The patent describes a method, apparatus, and ...  \n",
            "...                                                 ...  \n",
            "1094  The patent, US8214649, describes a system and ...  \n",
            "1095  Patent Number US8218421 describes a method, ap...  \n",
            "1096  The patent US8179999 pertains to methods and a...  \n",
            "1097  The patent (US8223227) describes an apparatus ...  \n",
            "1098  The patent US8230035 describes a method for au...  \n",
            "\n",
            "[1099 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "# Display the DataFrame\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UR3ct3L52a0g",
        "outputId": "c8f96ffe-34f7-4f6e-b8c9-dd58ac9995c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "sMMuTpK60PCW"
      },
      "outputs": [],
      "source": [
        "# Define preprocess_text function\n",
        "def preprocess_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Tokenize text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Reconstruct text\n",
        "    text = ' '.join(tokens)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Tokenize and preprocess the text\n",
        "df['preprocessed_patent'] = df['patent document'].apply(preprocess_text)\n",
        "df['preprocessed_summary'] = df['summary'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rgmp8yj4-ZE"
      },
      "source": [
        "# **ROUGE SCORE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "fbMQ8IoB_iM4"
      },
      "outputs": [],
      "source": [
        " # Compute ROUGE score\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "rouge_scores = []\n",
        "rouge2_scores = []\n",
        "for index, row in df.iterrows():\n",
        "    rouge = scorer.score(row['preprocessed_patent'], row['preprocessed_summary'])\n",
        "    rouge_scores.append(rouge['rouge1'][2])\n",
        "    rouge2_scores.append(rouge['rouge2'][2])\n",
        "\n",
        "df['rouge_scores'] = rouge_scores\n",
        "df['rouge2_scores'] = rouge2_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "mXoKm3c627XJ",
        "outputId": "a32b9e79-2771-4d69-b4cf-4cfdf52fa7cf"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'float' object is not subscriptable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-04759bab5c7d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Extracting Rouge scores from dictionaries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrouge1_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rouge1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rouge_scores'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mrouge2_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rouge2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rouge_scores'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrougeL_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rougeL'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rouge_scores'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-04759bab5c7d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Extracting Rouge scores from dictionaries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrouge1_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rouge1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rouge_scores'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mrouge2_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rouge2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rouge_scores'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrougeL_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rougeL'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rouge_scores'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'float' object is not subscriptable"
          ]
        }
      ],
      "source": [
        "# Extracting Rouge scores from dictionaries\n",
        "rouge1_scores = [score['rouge1'][2] for score in df['rouge_scores']]\n",
        "rouge2_scores = [score['rouge2'][2] for score in df['rouge_scores']]\n",
        "rougeL_scores = [score['rougeL'][2] for score in df['rouge_scores']]\n",
        "\n",
        "# Plotting Rouge scores\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(rouge1_scores, bins=20, color='blue', alpha=0.7, label='ROUGE-1')\n",
        "plt.hist(rouge2_scores, bins=20, color='red', alpha=0.7, label='ROUGE-2')\n",
        "plt.hist(rougeL_scores, bins=20, color='green', alpha=0.7, label='ROUGE-L')\n",
        "plt.title('Distribution of ROUGE Scores')\n",
        "plt.xlabel('ROUGE Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SUMMAC SCORES"
      ],
      "metadata": {
        "id": "s6xvpNp_MaNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install summac"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FBmOUyYMRtN",
        "outputId": "0a78589d-3086-4b5c-eb1a-4dac3f30ccca"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: summac in /usr/local/lib/python3.10/dist-packages (0.0.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from summac) (2.2.1+cu121)\n",
            "Requirement already satisfied: transformers>=4.24.0 in /usr/local/lib/python3.10/dist-packages (from summac) (4.35.2)\n",
            "Requirement already satisfied: nltk>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from summac) (3.8.1)\n",
            "Requirement already satisfied: huggingface-hub<=0.17.0 in /usr/local/lib/python3.10/dist-packages (from summac) (0.17.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from summac) (0.1.99)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from summac) (3.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<=0.17.0->summac) (3.14.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<=0.17.0->summac) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<=0.17.0->summac) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<=0.17.0->summac) (4.66.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<=0.17.0->summac) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<=0.17.0->summac) (4.11.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<=0.17.0->summac) (24.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.6.6->summac) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.6.6->summac) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.6.6->summac) (2023.12.25)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.24.0->summac) (1.25.2)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.24.0->summac) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.24.0->summac) (0.4.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->summac) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->summac) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->summac) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->summac) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->summac) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->summac) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->summac) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->summac) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->summac) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->summac) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->summac) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->summac) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->summac) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->summac) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->summac) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->summac) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->summac) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<=0.17.0->summac) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<=0.17.0->summac) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<=0.17.0->summac) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<=0.17.0->summac) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->summac) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from summac.model_summac import SummaCZS\n",
        "\n",
        "model_zs = SummaCZS(granularity=\"sentence\", model_name=\"vitc\", device=\"cuda\") # GPU -> device=\"cuda\", CPU -> device=\"cpu\"\n",
        "score_zs1 = model_zs.score(df[\"preprocessed_patent\"], df[\"preprocessed_summary\"])\n",
        "print(score_zs1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEdVXFZgL2zz",
        "outputId": "0b0e8c20-5f58-4f0e-abd9-fbd90abb428a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'scores': [0.69085693359375, 0.23065185546875, 0.0718994140625, 0.118865966796875, 0.0321044921875, 0.05865478515625, 0.240020751953125, 0.19793701171875, 0.72186279296875, 0.02447509765625, 0.356109619140625, 0.028228759765625, 0.4222869873046875, 0.238006591796875, 0.1130828857421875, 0.063812255859375, 0.023040771484375, 0.27874755859375, 0.15771484375, 0.1813201904296875, 0.333648681640625, -0.003875732421875, 0.17059326171875, 0.126251220703125, 0.6371612548828125, 0.182647705078125, 0.090576171875, 0.3489990234375, 0.69677734375, 0.24627685546875, 0.190643310546875, 0.084716796875, 0.7009048461914062, 0.1357421875, 0.0947265625, 0.0301513671875, 0.081756591796875, -0.0489044189453125, 0.461456298828125, 0.598785400390625, -0.39404296875, 0.1522216796875, 0.22979736328125, 0.784942626953125, 0.24688720703125, 0.4996337890625, 0.40277099609375, 0.10302734375, -0.13970947265625, 0.0120849609375, -0.04986572265625, -0.0123291015625, 0.259521484375, 0.4701385498046875, 0.0465087890625, 0.022705078125, 0.005279541015625, 0.246307373046875, 0.02655029296875, 0.103607177734375, 0.02557373046875, -0.0372314453125, 0.0657958984375, 0.5293121337890625, 0.45947265625, 0.348663330078125, 0.06268310546875, 0.62841796875, 0.061065673828125, 0.475830078125, 0.030731201171875, 0.5945663452148438, 0.187744140625, -0.01092529296875, -0.0264892578125, -0.136993408203125, 0.220184326171875, 0.0380859375, 0.3172607421875, -0.0395355224609375, -0.049957275390625, 0.2423858642578125, 0.25274658203125, 0.066650390625, -0.1119384765625, -0.058135986328125, 0.07122802734375, 0.149658203125, -0.14581298828125, 0.32989501953125, 0.214111328125, 0.2430419921875, 0.1190643310546875, 0.19097900390625, 0.301239013671875, -0.063995361328125, 0.3119354248046875, 0.5432281494140625, 0.354095458984375, 0.46405029296875, 0.04473876953125, 0.171630859375, 0.0032958984375, 0.15765380859375, -0.04376220703125, -0.06719970703125, -0.09051513671875, 0.34417724609375, 0.133544921875, 0.1839599609375, -0.05755615234375, 0.06903076171875, 0.02593994140625, 0.15570068359375, 0.11004638671875, 0.269805908203125, 0.46075439453125, -0.00872802734375, 0.011505126953125, 0.112548828125, 0.106231689453125, -0.0323486328125, 0.05010986328125, 0.06201171875, -0.12139892578125, 0.0648193359375, -0.032073974609375, 0.5455322265625, 0.1280517578125, 0.139007568359375, 0.1429443359375, -0.01361083984375, 0.055267333984375, 0.02679443359375, -0.026580810546875, 0.0416259765625, -0.022613525390625, -0.01171875, 0.00189208984375, 0.8260345458984375, 0.11138916015625, 0.38262939453125, 0.01409912109375, 0.540374755859375, -0.0025634765625, 0.05126953125, -0.04315185546875, 0.267608642578125, 0.0946044921875, 0.3170166015625, 0.007720947265625, 0.05029296875, -0.0225830078125, -0.029205322265625, -0.0264129638671875, 0.5138092041015625, -0.0047607421875, 0.238433837890625, 0.0616455078125, 0.236297607421875, 0.08599853515625, -0.00946044921875, 0.717254638671875, 0.235504150390625, 0.8096466064453125, 0.650848388671875, 0.044403076171875, 0.28399658203125, 0.0830078125, 0.08685302734375, 0.11614990234375, 0.370819091796875, 0.014404296875, 0.2047119140625, 0.02252197265625, -0.08624267578125, 0.00909423828125, 0.241729736328125, 0.210968017578125, -0.061279296875, 0.163330078125, -0.094329833984375, 0.0040283203125, 0.000396728515625, 0.122802734375, 0.211212158203125, 0.078582763671875, 0.248779296875, 0.058624267578125, 0.0008544921875, 0.00103759765625, 0.06085205078125, -0.031494140625, 0.020233154296875, 0.057891845703125, 0.00762939453125, 0.4520111083984375, 0.02239990234375, 0.26318359375, 0.102142333984375, -0.014007568359375, 0.35498046875, 0.01519775390625, 0.073699951171875, 0.2154541015625, 0.106597900390625, 0.18212890625, 0.190521240234375, 0.01556396484375, -0.004547119140625, 0.131103515625, 0.140625, 0.81878662109375, 0.25628662109375, 0.02838134765625, 0.07769775390625, 0.0092620849609375, 0.158660888671875, 0.5473175048828125, 0.11126708984375, 0.05926513671875, 0.0360107421875, 0.448486328125, 0.253326416015625, -0.1536102294921875, 0.029571533203125, 0.025115966796875, 0.016754150390625, 0.3373260498046875, 0.11651611328125, 0.0225677490234375, 0.0940093994140625, 0.22442626953125, 0.016021728515625, 0.30517578125, 0.03680419921875, 0.2945556640625, 0.01837158203125, 0.0166015625, 0.1138916015625, 0.099853515625, 0.309234619140625, 0.7186126708984375, 0.5221405029296875, 0.04449462890625, 0.18450927734375, 0.18768310546875, 0.754791259765625, 0.05413818359375, 0.607513427734375, 0.128204345703125, 0.243438720703125, 0.090362548828125, 0.00518798828125, 0.0921630859375, 0.02197265625, 0.0791015625, 0.103271484375, -0.0806884765625, 0.073394775390625, -0.0838775634765625, 0.5905609130859375, 0.0792236328125, 0.00787353515625, 0.410491943359375, 0.11767578125, 0.04443359375, 0.01470947265625, 0.2225341796875, 0.03118896484375, 0.35943603515625, 0.271514892578125, 0.7816162109375, 0.01910400390625, 0.120941162109375, 0.252593994140625, 0.199951171875, -0.0089111328125, -0.12487030029296875, 0.11773681640625, 0.04901123046875, 0.0972900390625, 0.02313232421875, 0.15509033203125, 0.072509765625, 0.03155517578125, 0.1229248046875, 0.3507537841796875, 0.123046875, 0.05657958984375, 0.197021484375, 0.01654052734375, 0.1053466796875, 0.0260009765625, -0.02799224853515625, 0.055572509765625, 0.07843017578125, 0.2857666015625, 0.607177734375, -0.02191162109375, 0.38140869140625, 0.22052001953125, 0.05169677734375, -0.0263824462890625, -0.00433349609375, 0.06207275390625, 0.435150146484375, 0.117584228515625, 0.0289306640625, 0.1126708984375, 0.381317138671875, 0.657745361328125, 0.0954132080078125, 0.0325927734375, -0.009918212890625, 0.072479248046875, 0.009552001953125, 0.222747802734375, -0.08984375, 0.47796630859375, -0.024444580078125, 0.0037841796875, 0.270538330078125, -0.078155517578125, 0.1885986328125, 0.081817626953125, 0.1077880859375, 0.0545806884765625, 0.436248779296875, 0.21844482421875, 0.0670166015625, 0.10699462890625, 0.487518310546875, 0.076202392578125, 0.025054931640625, 0.186431884765625, 0.17401123046875, 0.087158203125, 0.020843505859375, 0.00982666015625, 0.770782470703125, 0.935760498046875, 0.06390380859375, 0.50640869140625, 0.0189208984375, -0.13350677490234375, 0.41339111328125, 0.0579833984375, 0.403564453125, 0.382537841796875, 0.01171875, 0.09503173828125, -0.14227294921875, 0.22906494140625, 0.00140380859375, 0.1566162109375, 0.071990966796875, 0.091522216796875, 0.059478759765625, 0.0383758544921875, 0.05780029296875, 0.057159423828125, 0.080352783203125, -0.017822265625, 0.27471923828125, 0.067413330078125, 0.285186767578125, 0.1180419921875, 0.0177001953125, 0.7516021728515625, -0.0369873046875, 0.165802001953125, -0.01190185546875, 0.349151611328125, 0.08770751953125, 0.0526123046875, 0.368804931640625, 0.017730712890625, 0.0007476806640625, -0.00189208984375, 0.0792236328125, 0.146728515625, 0.03619384765625, 0.12200927734375, -0.02923583984375, 0.05242919921875, 0.459625244140625, 0.067840576171875, -0.0899658203125, 0.03497314453125, 0.2320556640625, 0.24310302734375, -0.02227783203125, 0.227203369140625, 0.1494140625, 0.417327880859375, 0.140655517578125, -0.00762939453125, 0.02435302734375, 0.5489349365234375, -0.0205078125, 0.376922607421875, 0.012420654296875, -0.04998779296875, 0.05328369140625, 0.07891845703125, -0.024444580078125, 0.056488037109375, 0.328948974609375, 0.03656005859375, 0.10888671875, 0.109130859375, 0.0859375, 0.09130859375, 0.3243408203125, 0.152191162109375, 0.014068603515625, 0.09521484375, 0.176055908203125, 0.007720947265625, 0.080657958984375, 0.023895263671875, 0.0125732421875, 0.31988525390625, 0.74896240234375, 0.59027099609375, 0.1837158203125, 0.2685394287109375, 0.0050048828125, 0.13934326171875, 0.62164306640625, 0.1641845703125, 0.31011962890625, 0.03460693359375, -0.052642822265625, 0.071136474609375, -0.22405242919921875, 0.30767822265625, 0.01025390625, 0.04681396484375, 0.019866943359375, -0.0877227783203125, 0.0679931640625, 0.099761962890625, 0.0384063720703125, 0.550537109375, 0.64459228515625, 0.236083984375, 0.235992431640625, -0.0035400390625, 0.08123779296875, 0.0106201171875, 0.27508544921875, 0.150634765625, 0.307769775390625, -0.028961181640625, 0.458526611328125, 0.4129638671875, 0.204132080078125, 0.14892578125, 0.30328369140625, 0.5589447021484375, 0.05145263671875, 0.030853271484375, 0.006317138671875, 0.133453369140625, -0.0789337158203125, 0.480316162109375, 0.023345947265625, 0.011932373046875, 0.104248046875, 0.08294677734375, 0.05303955078125, 0.0355224609375, 0.044708251953125, 0.34185791015625, 0.4517822265625, 0.0149993896484375, -0.15074920654296875, 0.02032470703125, 0.1578369140625, 0.042236328125, 0.099517822265625, 0.01251220703125, 0.1453857421875, 0.475677490234375, 0.279876708984375, -0.085784912109375, 0.098480224609375, 0.4390869140625, -0.00439453125, 0.750457763671875, 0.225128173828125, 0.33306884765625, 0.0279541015625, 0.11669921875, 0.09228515625, 0.059234619140625, 0.2929840087890625, 0.0755615234375, 0.1273193359375, 0.6470947265625, 0.01690673828125, 0.000946044921875, 0.31182861328125, 0.1810302734375, 0.133392333984375, 0.1984710693359375, 0.051513671875, 0.2386474609375, 0.34014892578125, 0.14825439453125, 0.57952880859375, 0.08123779296875, 0.3980712890625, 0.408355712890625, 0.0653076171875, 0.62176513671875, 0.021453857421875, 0.3724212646484375, 0.12744140625, 0.12896728515625, 0.249908447265625, -0.00445556640625, 0.7230377197265625, 0.1300048828125, 0.050567626953125, -0.030609130859375, 0.08331298828125, -0.0759124755859375, -0.017822265625, -0.0721435546875, -0.008026123046875, 0.15850830078125, 0.51361083984375, 0.107940673828125, 0.05291748046875, -0.02288818359375, 0.306060791015625, 0.05859375, 0.2969970703125, 0.38800048828125, 0.220123291015625, 0.14105224609375, 0.259124755859375, -0.02581787109375, 0.7674102783203125, -0.038726806640625, 0.171875, 0.0679931640625, 0.2623291015625, 0.00164794921875, 0.04437255859375, 0.356781005859375, 0.0777587890625, 0.2489013671875, -0.09918212890625, 0.0889892578125, 0.129608154296875, 0.017822265625, -0.0567474365234375, 0.119720458984375, -0.060943603515625, -0.003509521484375, 0.036376953125, 0.0322265625, 0.0384521484375, 0.17999267578125, 0.12823486328125, 0.133148193359375, 0.0224609375, 0.390838623046875, 0.336639404296875, 0.0025634765625, 0.0511474609375, 0.5820770263671875, 0.092254638671875, 0.1939697265625, 0.001953125, 0.202301025390625, 0.11505126953125, 0.23626708984375, 0.16748046875, 0.18719482421875, 0.6961517333984375, 0.1529541015625, 0.02783203125, 0.302001953125, -0.0067138671875, 0.033843994140625, -0.03302001953125, 0.7178802490234375, 0.175018310546875, 0.01519775390625, -0.0357666015625, 0.2230224609375, 0.23895263671875, 0.0535888671875, 0.44586181640625, 0.18328857421875, -0.016632080078125, 0.7437973022460938, 0.158294677734375, -0.005279541015625, 0.1429443359375, 0.24920654296875, 0.6344146728515625, 0.033203125, 0.171966552734375, 0.071319580078125, 0.36187744140625, 0.45843505859375, 0.05517578125, 0.14569091796875, 0.0284881591796875, -0.01495361328125, 0.07000732421875, -0.0111846923828125, 0.143035888671875, 0.059173583984375, 0.00433349609375, -0.0205078125, 0.14666748046875, 0.326324462890625, -0.079010009765625, -0.0030517578125, -0.0006103515625, -0.0035400390625, 0.0535888671875, -0.040924072265625, 0.00238037109375, 0.159454345703125, 0.124664306640625, 0.116546630859375, -0.0702972412109375, 0.645751953125, 0.9114456176757812, 0.008544921875, 0.5693359375, 0.42279052734375, 0.08417510986328125, 0.0487060546875, 0.23297119140625, 0.06292724609375, 0.11004638671875, 0.528839111328125, 0.03375244140625, 0.158935546875, 0.2796630859375, 0.10595703125, 0.037109375, 0.24395751953125, 0.084075927734375, 0.59600830078125, 0.11065673828125, 0.2125701904296875, 0.255035400390625, 0.20538330078125, 0.0521240234375, 0.700347900390625, 0.023956298828125, 0.15789794921875, 0.547210693359375, 0.181640625, 0.8034515380859375, -0.0104827880859375, 0.7335205078125, 0.0363311767578125, 0.0258026123046875, 0.5269775390625, 0.11444091796875, 0.347930908203125, 0.04754638671875, 0.0142974853515625, 0.2997894287109375, 0.50897216796875, -0.010589599609375, 0.23785400390625, 0.16357421875, 0.3280029296875, 0.24346923828125, 0.0540771484375, 0.02691650390625, 0.2464599609375, 0.028717041015625, 0.49676513671875, 0.11431884765625, 0.04925537109375, 0.05303955078125, 0.2223052978515625, 0.322509765625, 0.2662353515625, 0.509735107421875, 0.509521484375, 0.02874755859375, 0.155792236328125, 0.1298828125, 0.05828857421875, 0.2581787109375, 0.6190032958984375, 0.589111328125, 0.482025146484375, 0.244354248046875, 0.6453857421875, 0.290802001953125, 0.8579254150390625, 0.147552490234375, 0.1446533203125, 0.346527099609375, 0.605010986328125, 0.085052490234375, 0.05963134765625, 0.74261474609375, 0.40875244140625, 0.100006103515625, 0.1365966796875, 0.384429931640625, 0.2210693359375, -0.001068115234375, 0.419281005859375, 0.347320556640625, 0.071136474609375, 0.8124008178710938, 0.078094482421875, 0.5513153076171875, 0.01336669921875, 0.329254150390625, 0.148895263671875, 0.270111083984375, 0.322845458984375, 0.1162109375, 0.169891357421875, 0.543914794921875, -0.03607177734375, 0.0151824951171875, 0.472808837890625, 0.0684051513671875, 0.4794158935546875, 0.685089111328125, 0.7365570068359375, 0.176849365234375, 0.172454833984375, 0.398712158203125, 0.299896240234375, 0.6563262939453125, 0.33905029296875, 0.7171478271484375, 0.5646514892578125, 0.43511962890625, 0.646148681640625, 0.148895263671875, 0.7630157470703125, 0.4782562255859375, -0.003082275390625, 0.038665771484375, -0.0198974609375, 0.7287445068359375, 0.8837356567382812, 0.00775146484375, 0.333251953125, 0.6279296875, 0.0029296875, 0.19403076171875, 0.299896240234375, 0.386474609375, 0.144683837890625, 0.573272705078125, 0.203948974609375, 0.73333740234375, 0.6197662353515625, 0.41693115234375, 0.296295166015625, 0.1746826171875, 0.41986083984375, 0.4874267578125, 0.137115478515625, 0.0061187744140625, 0.1318359375, -0.073822021484375, 0.1673583984375, 0.1987457275390625, 0.369476318359375, 0.264892578125, -0.00678253173828125, 0.162811279296875, 0.05712890625, 0.0098876953125, 0.4818115234375, -0.02044677734375, 0.12506103515625, 0.8224029541015625, -0.03094482421875, 0.1878509521484375, 0.2606201171875, 0.022247314453125, 0.153564453125, 0.308013916015625, 0.1883544921875, 0.7861785888671875, 0.49151611328125, 0.4322662353515625, 0.19683837890625, 0.271026611328125, 0.1326904296875, -0.016143798828125, 0.0728759765625, 0.167755126953125, 0.77825927734375, 0.16583251953125, 0.00042724609375, 0.237640380859375, 0.369842529296875, 0.137176513671875, -0.02008056640625, 0.0456695556640625, 0.099609375, 0.136505126953125, 0.0107421875, 0.422119140625, 0.5628204345703125, -0.006744384765625, 0.202392578125, 0.291412353515625, 0.8369522094726562, 0.12432861328125, 0.1271514892578125, 0.01617431640625, 0.12701416015625, 0.46722412109375, 0.0202789306640625, 0.0093994140625, 0.2081298828125, 0.3990631103515625, 0.344268798828125, 0.10833740234375, -0.127838134765625, 0.05755615234375, 0.268341064453125, 0.16534423828125, 0.6199493408203125, 0.63092041015625, 0.01544189453125, 0.8318634033203125, 0.2149658203125, 0.1142578125, 0.31292724609375, 0.005279541015625, 0.1044921875, 0.072418212890625, 0.33111572265625, 0.32855224609375, 0.1925048828125, 0.1171875, 0.20208740234375, 0.342437744140625, 0.39923095703125, 0.0411376953125, 0.272216796875, 0.2038726806640625, 0.346588134765625, 0.0543060302734375, -0.02475738525390625, 0.1751708984375, 0.435272216796875, 0.118896484375, 0.04632568359375, 0.132476806640625, 0.0357666015625, -0.07098388671875, -0.0218505859375, 0.1949462890625, 0.074737548828125, 0.0341796875, 0.57781982421875, 0.5452880859375, 0.6068878173828125, 0.662353515625, 0.02667236328125, -0.031768798828125, 0.38592529296875, 0.024505615234375, 0.19873046875, 0.37152099609375, 0.672210693359375, -0.0222625732421875, 0.030914306640625, 0.138916015625, 0.281890869140625, 0.1019134521484375, -0.0018310546875, 0.495880126953125, 0.11322021484375, 0.031158447265625, 0.222930908203125, 0.067535400390625, 0.17034912109375, 0.109222412109375, 0.14019775390625, 0.479736328125, 0.082275390625, 0.4101715087890625, 0.0113525390625, 0.34149169921875, 0.4073486328125, 0.599578857421875, 0.634033203125, 0.5244598388671875, 0.2548828125, 0.018829345703125, -0.015960693359375, 0.8513031005859375, 0.1027679443359375, 0.7118988037109375, 0.136688232421875, 0.0795440673828125, 0.141326904296875, -0.041839599609375, 0.03125, 0.14764404296875, 0.0013427734375, 0.452423095703125, 0.281494140625, 0.0157470703125, 0.37579345703125, 0.462646484375, 0.10455322265625, 0.490081787109375, -0.019588470458984375, 0.36358642578125, 0.25341796875, -0.01129150390625, 0.090667724609375, 0.00799560546875, 0.23211669921875, 0.635833740234375, 0.042388916015625, -0.017822265625, 0.737884521484375, 0.1996307373046875, 0.01727294921875, 0.47869873046875, 0.12921142578125, 0.4167633056640625, 0.079986572265625, -0.032958984375, 0.06219482421875, 0.703704833984375, 0.215057373046875, 0.1162109375, 0.1947021484375, 0.3189697265625, -0.1135101318359375, 0.15740966796875, 0.22772216796875, 0.055267333984375, 0.07330322265625, 0.401702880859375, 0.0263671875, 0.4642333984375, 0.163299560546875, 0.582672119140625, 0.5488433837890625, 0.098602294921875, 0.452545166015625, 0.707244873046875, 0.058624267578125, 0.01708984375, 0.024322509765625, 0.6922454833984375, 0.10595703125, 0.483367919921875, 0.29833984375, 0.05926513671875, 0.18804931640625, 0.094573974609375, 0.296630859375, 0.34515380859375, 0.406707763671875, 0.005096435546875, -0.0014495849609375, -0.007965087890625, 0.0228271484375, 0.14593505859375, 0.094207763671875, 0.5889892578125, 0.42779541015625, 0.0662841796875, 0.253814697265625, 0.2469329833984375, -0.0526123046875, 0.159637451171875, 0.320648193359375, 0.477142333984375, 0.0525970458984375, 0.785400390625, 0.021026611328125, 0.204071044921875, -0.05035400390625, -0.0042724609375, 0.758544921875, 0.120635986328125, -0.026275634765625, 0.8653488159179688, 0.049346923828125, 0.038970947265625, 0.285980224609375, 0.229095458984375, 0.062286376953125, 0.8200302124023438, 0.4474334716796875, 0.534271240234375, 0.065277099609375, 0.01763916015625, 0.540557861328125, -0.0318603515625, 0.635406494140625, 0.443267822265625, 0.012969970703125, -0.0207061767578125, 0.222381591796875, 0.1180419921875, 0.101837158203125, 0.228240966796875, -0.0557861328125, 0.301910400390625, -0.05115509033203125, 0.64398193359375, 0.3548583984375, 0.4334716796875, 0.09521484375, 0.103546142578125, 0.00177001953125, 0.21453857421875, 0.502166748046875, 0.4818115234375, 0.066162109375, 0.061187744140625, 0.637176513671875, 0.06414794921875, 0.8369598388671875, 0.106353759765625, 0.2408447265625, -0.0487823486328125, 0.17169189453125, 0.313812255859375, -0.02166748046875, 0.119842529296875, 0.593658447265625, 0.201507568359375, 0.5498046875, 0.659332275390625, 0.78045654296875, 0.5724029541015625, -0.0022430419921875, -0.1624298095703125, 0.000732421875, 0.06683349609375, 0.2808837890625, -0.01104736328125, 0.1339111328125, 0.142486572265625, 0.04107666015625, 0.0146484375, 0.5187530517578125, 0.456878662109375, 0.12396240234375, 0.067901611328125, 0.496795654296875, 0.55596923828125, 0.015716552734375, 0.12701416015625, 0.185546875, 0.125, 0.075775146484375, 0.001861572265625, 0.009521484375, 0.477264404296875, 0.0091400146484375, 0.04302978515625, 0.1165771484375, 0.165802001953125, 0.1016845703125, 0.0135955810546875, 0.057403564453125, 0.18310546875, 0.1488037109375, 0.04193115234375, 0.018829345703125, 0.016204833984375, 0.1527099609375, 0.021728515625, 0.72674560546875, 0.07598876953125, 0.0645751953125, 0.7479095458984375, -0.02685546875, -0.0047607421875, 0.041595458984375, 0.06048583984375], 'images': [array([[[0.73242188]],\n",
            "\n",
            "       [[0.04156494]],\n",
            "\n",
            "       [[0.22607422]]]), array([[[0.26318359]],\n",
            "\n",
            "       [[0.03253174]],\n",
            "\n",
            "       [[0.70410156]]]), array([[[0.11468506]],\n",
            "\n",
            "       [[0.04278564]],\n",
            "\n",
            "       [[0.84277344]]]), array([[[0.16723633]],\n",
            "\n",
            "       [[0.04837036]],\n",
            "\n",
            "       [[0.78417969]]]), array([[[0.11206055]],\n",
            "\n",
            "       [[0.07995605]],\n",
            "\n",
            "       [[0.80810547]]]), array([[[0.12414551]],\n",
            "\n",
            "       [[0.06549072]],\n",
            "\n",
            "       [[0.81054688]]]), array([[[0.29248047]],\n",
            "\n",
            "       [[0.05245972]],\n",
            "\n",
            "       [[0.65478516]]]), array([[[0.24609375]],\n",
            "\n",
            "       [[0.04815674]],\n",
            "\n",
            "       [[0.70556641]]]), array([[[0.73339844]],\n",
            "\n",
            "       [[0.01153564]],\n",
            "\n",
            "       [[0.25512695]]]), array([[[0.0881958 ]],\n",
            "\n",
            "       [[0.0637207 ]],\n",
            "\n",
            "       [[0.84814453]]]), array([[[0.40991211]],\n",
            "\n",
            "       [[0.05380249]],\n",
            "\n",
            "       [[0.53613281]]]), array([[[0.08465576]],\n",
            "\n",
            "       [[0.056427  ]],\n",
            "\n",
            "       [[0.85888672]]]), array([[[0.45068359]],\n",
            "\n",
            "       [[0.02839661]],\n",
            "\n",
            "       [[0.52099609]]]), array([[[0.27783203]],\n",
            "\n",
            "       [[0.03982544]],\n",
            "\n",
            "       [[0.68261719]]]), array([[[0.13964844]],\n",
            "\n",
            "       [[0.02656555]],\n",
            "\n",
            "       [[0.83398438]]]), array([[[0.09558105]],\n",
            "\n",
            "       [[0.0317688 ]],\n",
            "\n",
            "       [[0.87255859]]]), array([[[0.0838623 ]],\n",
            "\n",
            "       [[0.06082153]],\n",
            "\n",
            "       [[0.85546875]]]), array([[[0.33251953]],\n",
            "\n",
            "       [[0.05377197]],\n",
            "\n",
            "       [[0.61376953]]]), array([[[0.18933105]],\n",
            "\n",
            "       [[0.03161621]],\n",
            "\n",
            "       [[0.77880859]]]), array([[[0.21032715]],\n",
            "\n",
            "       [[0.02900696]],\n",
            "\n",
            "       [[0.76074219]]]), array([[[0.38964844]],\n",
            "\n",
            "       [[0.05599976]],\n",
            "\n",
            "       [[0.55419922]]]), array([[[0.03567505]],\n",
            "\n",
            "       [[0.03955078]],\n",
            "\n",
            "       [[0.92480469]]]), array([[[0.21887207]],\n",
            "\n",
            "       [[0.04827881]],\n",
            "\n",
            "       [[0.73291016]]]), array([[[0.16564941]],\n",
            "\n",
            "       [[0.03939819]],\n",
            "\n",
            "       [[0.79492188]]]), array([[[0.6484375 ]],\n",
            "\n",
            "       [[0.01127625]],\n",
            "\n",
            "       [[0.34033203]]]), array([[[0.21398926]],\n",
            "\n",
            "       [[0.03134155]],\n",
            "\n",
            "       [[0.75488281]]]), array([[[0.15454102]],\n",
            "\n",
            "       [[0.06396484]],\n",
            "\n",
            "       [[0.78125   ]]]), array([[[0.41918945]],\n",
            "\n",
            "       [[0.07019043]],\n",
            "\n",
            "       [[0.51074219]]]), array([[[0.73339844]],\n",
            "\n",
            "       [[0.03662109]],\n",
            "\n",
            "       [[0.23010254]]]), array([[[0.27758789]],\n",
            "\n",
            "       [[0.03131104]],\n",
            "\n",
            "       [[0.69140625]]]), array([[[0.24035645]],\n",
            "\n",
            "       [[0.04971313]],\n",
            "\n",
            "       [[0.70996094]]]), array([[[0.1776123 ]],\n",
            "\n",
            "       [[0.09289551]],\n",
            "\n",
            "       [[0.72949219]]]), array([[[0.71142578]],\n",
            "\n",
            "       [[0.01052094]],\n",
            "\n",
            "       [[0.27807617]]]), array([[[0.22070312]],\n",
            "\n",
            "       [[0.08496094]],\n",
            "\n",
            "       [[0.69433594]]]), array([[[0.11975098]],\n",
            "\n",
            "       [[0.02502441]],\n",
            "\n",
            "       [[0.85498047]]]), array([[[0.09307861]],\n",
            "\n",
            "       [[0.06292725]],\n",
            "\n",
            "       [[0.84423828]]]), array([[[0.11663818]],\n",
            "\n",
            "       [[0.03488159]],\n",
            "\n",
            "       [[0.84863281]]]), array([[[0.02055359]],\n",
            "\n",
            "       [[0.06945801]],\n",
            "\n",
            "       [[0.91015625]]]), array([[[0.49462891]],\n",
            "\n",
            "       [[0.03317261]],\n",
            "\n",
            "       [[0.47216797]]]), array([[[0.62744141]],\n",
            "\n",
            "       [[0.02865601]],\n",
            "\n",
            "       [[0.34399414]]]), array([[[0.07592773]],\n",
            "\n",
            "       [[0.4699707 ]],\n",
            "\n",
            "       [[0.45410156]]]), array([[[0.20910645]],\n",
            "\n",
            "       [[0.05688477]],\n",
            "\n",
            "       [[0.73388672]]]), array([[[0.31347656]],\n",
            "\n",
            "       [[0.0836792 ]],\n",
            "\n",
            "       [[0.60302734]]]), array([[[0.80126953]],\n",
            "\n",
            "       [[0.0163269 ]],\n",
            "\n",
            "       [[0.18225098]]]), array([[[0.32104492]],\n",
            "\n",
            "       [[0.07415771]],\n",
            "\n",
            "       [[0.60498047]]]), array([[[0.56640625]],\n",
            "\n",
            "       [[0.06677246]],\n",
            "\n",
            "       [[0.36669922]]]), array([[[0.43603516]],\n",
            "\n",
            "       [[0.03326416]],\n",
            "\n",
            "       [[0.53076172]]]), array([[[0.15380859]],\n",
            "\n",
            "       [[0.05078125]],\n",
            "\n",
            "       [[0.79541016]]]), array([[[0.03399658]],\n",
            "\n",
            "       [[0.17370605]],\n",
            "\n",
            "       [[0.79248047]]]), array([[[0.10949707]],\n",
            "\n",
            "       [[0.09741211]],\n",
            "\n",
            "       [[0.79296875]]]), array([[[0.03240967]],\n",
            "\n",
            "       [[0.08227539]],\n",
            "\n",
            "       [[0.88525391]]]), array([[[0.0881958 ]],\n",
            "\n",
            "       [[0.1005249 ]],\n",
            "\n",
            "       [[0.81103516]]]), array([[[0.31738281]],\n",
            "\n",
            "       [[0.05786133]],\n",
            "\n",
            "       [[0.62451172]]]), array([[[0.49633789]],\n",
            "\n",
            "       [[0.02619934]],\n",
            "\n",
            "       [[0.47729492]]]), array([[[0.10144043]],\n",
            "\n",
            "       [[0.05493164]],\n",
            "\n",
            "       [[0.84375   ]]]), array([[[0.09869385]],\n",
            "\n",
            "       [[0.07598877]],\n",
            "\n",
            "       [[0.82519531]]]), array([[[0.04733276]],\n",
            "\n",
            "       [[0.04205322]],\n",
            "\n",
            "       [[0.91064453]]]), array([[[0.3034668 ]],\n",
            "\n",
            "       [[0.05715942]],\n",
            "\n",
            "       [[0.63964844]]]), array([[[0.04367065]],\n",
            "\n",
            "       [[0.01712036]],\n",
            "\n",
            "       [[0.93945312]]]), array([[[0.14477539]],\n",
            "\n",
            "       [[0.04116821]],\n",
            "\n",
            "       [[0.81396484]]]), array([[[0.13049316]],\n",
            "\n",
            "       [[0.10491943]],\n",
            "\n",
            "       [[0.76464844]]]), array([[[0.11547852]],\n",
            "\n",
            "       [[0.15270996]],\n",
            "\n",
            "       [[0.73193359]]]), array([[[0.14746094]],\n",
            "\n",
            "       [[0.08166504]],\n",
            "\n",
            "       [[0.77099609]]]), array([[[0.54736328]],\n",
            "\n",
            "       [[0.01805115]],\n",
            "\n",
            "       [[0.43432617]]]), array([[[0.52197266]],\n",
            "\n",
            "       [[0.0625    ]],\n",
            "\n",
            "       [[0.41552734]]]), array([[[0.40649414]],\n",
            "\n",
            "       [[0.05783081]],\n",
            "\n",
            "       [[0.53564453]]]), array([[[0.1427002 ]],\n",
            "\n",
            "       [[0.08001709]],\n",
            "\n",
            "       [[0.77734375]]]), array([[[0.65380859]],\n",
            "\n",
            "       [[0.02539062]],\n",
            "\n",
            "       [[0.32055664]]]), array([[[0.10723877]],\n",
            "\n",
            "       [[0.0461731 ]],\n",
            "\n",
            "       [[0.84667969]]]), array([[[0.4987793 ]],\n",
            "\n",
            "       [[0.02294922]],\n",
            "\n",
            "       [[0.47827148]]]), array([[[0.08740234]],\n",
            "\n",
            "       [[0.05667114]],\n",
            "\n",
            "       [[0.85595703]]]), array([[[0.60986328]],\n",
            "\n",
            "       [[0.01529694]],\n",
            "\n",
            "       [[0.37475586]]]), array([[[0.22802734]],\n",
            "\n",
            "       [[0.0402832 ]],\n",
            "\n",
            "       [[0.73144531]]]), array([[[0.11407471]],\n",
            "\n",
            "       [[0.125     ]],\n",
            "\n",
            "       [[0.76074219]]]), array([[[0.06829834]],\n",
            "\n",
            "       [[0.0947876 ]],\n",
            "\n",
            "       [[0.83691406]]]), array([[[0.01620483]],\n",
            "\n",
            "       [[0.15319824]],\n",
            "\n",
            "       [[0.83056641]]]), array([[[0.26586914]],\n",
            "\n",
            "       [[0.04568481]],\n",
            "\n",
            "       [[0.68847656]]]), array([[[0.1038208 ]],\n",
            "\n",
            "       [[0.06573486]],\n",
            "\n",
            "       [[0.83056641]]]), array([[[0.36865234]],\n",
            "\n",
            "       [[0.0513916 ]],\n",
            "\n",
            "       [[0.58007812]]]), array([[[0.02540588]],\n",
            "\n",
            "       [[0.06494141]],\n",
            "\n",
            "       [[0.90966797]]]), array([[[0.03201294]],\n",
            "\n",
            "       [[0.08197021]],\n",
            "\n",
            "       [[0.88623047]]]), array([[[0.27124023]],\n",
            "\n",
            "       [[0.02885437]],\n",
            "\n",
            "       [[0.69970703]]]), array([[[0.34545898]],\n",
            "\n",
            "       [[0.0927124 ]],\n",
            "\n",
            "       [[0.56152344]]]), array([[[0.16503906]],\n",
            "\n",
            "       [[0.09838867]],\n",
            "\n",
            "       [[0.73632812]]]), array([[[0.04785156]],\n",
            "\n",
            "       [[0.15979004]],\n",
            "\n",
            "       [[0.79248047]]]), array([[[0.05459595]],\n",
            "\n",
            "       [[0.11273193]],\n",
            "\n",
            "       [[0.83251953]]]), array([[[0.16992188]],\n",
            "\n",
            "       [[0.09869385]],\n",
            "\n",
            "       [[0.73144531]]]), array([[[0.27124023]],\n",
            "\n",
            "       [[0.12158203]],\n",
            "\n",
            "       [[0.60742188]]]), array([[[0.07745361]],\n",
            "\n",
            "       [[0.2232666 ]],\n",
            "\n",
            "       [[0.69921875]]]), array([[[0.35913086]],\n",
            "\n",
            "       [[0.02923584]],\n",
            "\n",
            "       [[0.61181641]]]), array([[[0.26513672]],\n",
            "\n",
            "       [[0.05102539]],\n",
            "\n",
            "       [[0.68408203]]]), array([[[0.28759766]],\n",
            "\n",
            "       [[0.04455566]],\n",
            "\n",
            "       [[0.66796875]]]), array([[[0.14709473]],\n",
            "\n",
            "       [[0.0280304 ]],\n",
            "\n",
            "       [[0.82470703]]]), array([[[0.26293945]],\n",
            "\n",
            "       [[0.07196045]],\n",
            "\n",
            "       [[0.66503906]]]), array([[[0.35864258]],\n",
            "\n",
            "       [[0.05740356]],\n",
            "\n",
            "       [[0.58398438]]]), array([[[0.05734253]],\n",
            "\n",
            "       [[0.12133789]],\n",
            "\n",
            "       [[0.82128906]]]), array([[[0.33740234]],\n",
            "\n",
            "       [[0.02546692]],\n",
            "\n",
            "       [[0.63720703]]]), array([[[0.56494141]],\n",
            "\n",
            "       [[0.02171326]],\n",
            "\n",
            "       [[0.41333008]]]), array([[[0.40405273]],\n",
            "\n",
            "       [[0.04995728]],\n",
            "\n",
            "       [[0.54589844]]]), array([[[0.5078125 ]],\n",
            "\n",
            "       [[0.04376221]],\n",
            "\n",
            "       [[0.44873047]]]), array([[[0.12341309]],\n",
            "\n",
            "       [[0.07867432]],\n",
            "\n",
            "       [[0.79785156]]]), array([[[0.24133301]],\n",
            "\n",
            "       [[0.06970215]],\n",
            "\n",
            "       [[0.68896484]]]), array([[[0.03491211]],\n",
            "\n",
            "       [[0.03161621]],\n",
            "\n",
            "       [[0.93359375]]]), array([[[0.23742676]],\n",
            "\n",
            "       [[0.07977295]],\n",
            "\n",
            "       [[0.68261719]]]), array([[[0.04772949]],\n",
            "\n",
            "       [[0.0914917 ]],\n",
            "\n",
            "       [[0.86083984]]]), array([[[0.04449463]],\n",
            "\n",
            "       [[0.11169434]],\n",
            "\n",
            "       [[0.84375   ]]]), array([[[0.07745361]],\n",
            "\n",
            "       [[0.16796875]],\n",
            "\n",
            "       [[0.75439453]]]), array([[[0.38671875]],\n",
            "\n",
            "       [[0.0425415 ]],\n",
            "\n",
            "       [[0.57080078]]]), array([[[0.19995117]],\n",
            "\n",
            "       [[0.06640625]],\n",
            "\n",
            "       [[0.73339844]]]), array([[[0.30419922]],\n",
            "\n",
            "       [[0.12023926]],\n",
            "\n",
            "       [[0.57568359]]]), array([[[0.04656982]],\n",
            "\n",
            "       [[0.10412598]],\n",
            "\n",
            "       [[0.84912109]]]), array([[[0.12359619]],\n",
            "\n",
            "       [[0.05456543]],\n",
            "\n",
            "       [[0.82177734]]]), array([[[0.04818726]],\n",
            "\n",
            "       [[0.02224731]],\n",
            "\n",
            "       [[0.9296875 ]]]), array([[[0.23193359]],\n",
            "\n",
            "       [[0.07623291]],\n",
            "\n",
            "       [[0.69189453]]]), array([[[0.16052246]],\n",
            "\n",
            "       [[0.05047607]],\n",
            "\n",
            "       [[0.7890625 ]]]), array([[[0.30981445]],\n",
            "\n",
            "       [[0.04000854]],\n",
            "\n",
            "       [[0.65039062]]]), array([[[0.49023438]],\n",
            "\n",
            "       [[0.02947998]],\n",
            "\n",
            "       [[0.48022461]]]), array([[[0.05963135]],\n",
            "\n",
            "       [[0.06835938]],\n",
            "\n",
            "       [[0.87207031]]]), array([[[0.06427002]],\n",
            "\n",
            "       [[0.05276489]],\n",
            "\n",
            "       [[0.8828125 ]]]), array([[[0.17175293]],\n",
            "\n",
            "       [[0.0592041 ]],\n",
            "\n",
            "       [[0.76904297]]]), array([[[0.14257812]],\n",
            "\n",
            "       [[0.03634644]],\n",
            "\n",
            "       [[0.82128906]]]), array([[[0.11254883]],\n",
            "\n",
            "       [[0.14489746]],\n",
            "\n",
            "       [[0.74267578]]]), array([[[0.08642578]],\n",
            "\n",
            "       [[0.03631592]],\n",
            "\n",
            "       [[0.87744141]]]), array([[[0.13903809]],\n",
            "\n",
            "       [[0.07702637]],\n",
            "\n",
            "       [[0.78369141]]]), array([[[0.05377197]],\n",
            "\n",
            "       [[0.1751709 ]],\n",
            "\n",
            "       [[0.77099609]]]), array([[[0.1270752 ]],\n",
            "\n",
            "       [[0.06225586]],\n",
            "\n",
            "       [[0.81054688]]]), array([[[0.04135132]],\n",
            "\n",
            "       [[0.07342529]],\n",
            "\n",
            "       [[0.88525391]]]), array([[[0.59716797]],\n",
            "\n",
            "       [[0.05163574]],\n",
            "\n",
            "       [[0.35131836]]]), array([[[0.23291016]],\n",
            "\n",
            "       [[0.1048584 ]],\n",
            "\n",
            "       [[0.66210938]]]), array([[[0.18432617]],\n",
            "\n",
            "       [[0.0453186 ]],\n",
            "\n",
            "       [[0.77050781]]]), array([[[0.19970703]],\n",
            "\n",
            "       [[0.0567627 ]],\n",
            "\n",
            "       [[0.74365234]]]), array([[[0.04943848]],\n",
            "\n",
            "       [[0.06304932]],\n",
            "\n",
            "       [[0.88769531]]]), array([[[0.09869385]],\n",
            "\n",
            "       [[0.04342651]],\n",
            "\n",
            "       [[0.85791016]]]), array([[[0.08764648]],\n",
            "\n",
            "       [[0.06085205]],\n",
            "\n",
            "       [[0.8515625 ]]]), array([[[0.01119995]],\n",
            "\n",
            "       [[0.03778076]],\n",
            "\n",
            "       [[0.95117188]]]), array([[[0.10388184]],\n",
            "\n",
            "       [[0.06225586]],\n",
            "\n",
            "       [[0.83398438]]]), array([[[0.05374146]],\n",
            "\n",
            "       [[0.07635498]],\n",
            "\n",
            "       [[0.87011719]]]), array([[[0.09039307]],\n",
            "\n",
            "       [[0.10211182]],\n",
            "\n",
            "       [[0.80761719]]]), array([[[0.06225586]],\n",
            "\n",
            "       [[0.06036377]],\n",
            "\n",
            "       [[0.87744141]]]), array([[[0.84375   ]],\n",
            "\n",
            "       [[0.01771545]],\n",
            "\n",
            "       [[0.13867188]]]), array([[[0.1706543 ]],\n",
            "\n",
            "       [[0.05926514]],\n",
            "\n",
            "       [[0.77001953]]]), array([[[0.4387207 ]],\n",
            "\n",
            "       [[0.05609131]],\n",
            "\n",
            "       [[0.50537109]]]), array([[[0.04971313]],\n",
            "\n",
            "       [[0.03561401]],\n",
            "\n",
            "       [[0.91455078]]]), array([[[0.56982422]],\n",
            "\n",
            "       [[0.02944946]],\n",
            "\n",
            "       [[0.40039062]]]), array([[[0.0368042 ]],\n",
            "\n",
            "       [[0.03936768]],\n",
            "\n",
            "       [[0.92382812]]]), array([[[0.10961914]],\n",
            "\n",
            "       [[0.05834961]],\n",
            "\n",
            "       [[0.83203125]]]), array([[[0.04638672]],\n",
            "\n",
            "       [[0.08953857]],\n",
            "\n",
            "       [[0.86425781]]]), array([[[0.31176758]],\n",
            "\n",
            "       [[0.04415894]],\n",
            "\n",
            "       [[0.64404297]]]), array([[[0.16101074]],\n",
            "\n",
            "       [[0.06640625]],\n",
            "\n",
            "       [[0.77246094]]]), array([[[0.34106445]],\n",
            "\n",
            "       [[0.02404785]],\n",
            "\n",
            "       [[0.63476562]]]), array([[[0.06549072]],\n",
            "\n",
            "       [[0.05776978]],\n",
            "\n",
            "       [[0.87695312]]]), array([[[0.11328125]],\n",
            "\n",
            "       [[0.06298828]],\n",
            "\n",
            "       [[0.82373047]]]), array([[[0.06451416]],\n",
            "\n",
            "       [[0.08709717]],\n",
            "\n",
            "       [[0.84814453]]]), array([[[0.01126099]],\n",
            "\n",
            "       [[0.04046631]],\n",
            "\n",
            "       [[0.94824219]]]), array([[[0.02308655]],\n",
            "\n",
            "       [[0.04949951]],\n",
            "\n",
            "       [[0.92724609]]]), array([[[0.53320312]],\n",
            "\n",
            "       [[0.01939392]],\n",
            "\n",
            "       [[0.44750977]]]), array([[[0.04748535]],\n",
            "\n",
            "       [[0.05224609]],\n",
            "\n",
            "       [[0.90039062]]]), array([[[0.2722168 ]],\n",
            "\n",
            "       [[0.03378296]],\n",
            "\n",
            "       [[0.69384766]]]), array([[[0.13256836]],\n",
            "\n",
            "       [[0.07092285]],\n",
            "\n",
            "       [[0.79638672]]]), array([[[0.25976562]],\n",
            "\n",
            "       [[0.02346802]],\n",
            "\n",
            "       [[0.71679688]]]), array([[[0.11791992]],\n",
            "\n",
            "       [[0.03192139]],\n",
            "\n",
            "       [[0.85009766]]]), array([[[0.08221436]],\n",
            "\n",
            "       [[0.0916748 ]],\n",
            "\n",
            "       [[0.82617188]]]), array([[[0.73486328]],\n",
            "\n",
            "       [[0.01760864]],\n",
            "\n",
            "       [[0.24768066]]]), array([[[0.27832031]],\n",
            "\n",
            "       [[0.04281616]],\n",
            "\n",
            "       [[0.67871094]]]), array([[[0.82861328]],\n",
            "\n",
            "       [[0.01896667]],\n",
            "\n",
            "       [[0.15246582]]]), array([[[0.67822266]],\n",
            "\n",
            "       [[0.02737427]],\n",
            "\n",
            "       [[0.29443359]]]), array([[[0.09753418]],\n",
            "\n",
            "       [[0.0531311 ]],\n",
            "\n",
            "       [[0.84912109]]]), array([[[0.34619141]],\n",
            "\n",
            "       [[0.06219482]],\n",
            "\n",
            "       [[0.59130859]]]), array([[[0.14953613]],\n",
            "\n",
            "       [[0.06652832]],\n",
            "\n",
            "       [[0.78369141]]]), array([[[0.1517334 ]],\n",
            "\n",
            "       [[0.06488037]],\n",
            "\n",
            "       [[0.78320312]]]), array([[[0.20263672]],\n",
            "\n",
            "       [[0.08648682]],\n",
            "\n",
            "       [[0.7109375 ]]]), array([[[0.41430664]],\n",
            "\n",
            "       [[0.04348755]],\n",
            "\n",
            "       [[0.54199219]]]), array([[[0.06002808]],\n",
            "\n",
            "       [[0.04562378]],\n",
            "\n",
            "       [[0.89453125]]]), array([[[0.2590332 ]],\n",
            "\n",
            "       [[0.05432129]],\n",
            "\n",
            "       [[0.68652344]]]), array([[[0.09960938]],\n",
            "\n",
            "       [[0.0770874 ]],\n",
            "\n",
            "       [[0.82324219]]]), array([[[0.04693604]],\n",
            "\n",
            "       [[0.13317871]],\n",
            "\n",
            "       [[0.81982422]]]), array([[[0.06222534]],\n",
            "\n",
            "       [[0.0531311 ]],\n",
            "\n",
            "       [[0.88476562]]]), array([[[0.30029297]],\n",
            "\n",
            "       [[0.05856323]],\n",
            "\n",
            "       [[0.64111328]]]), array([[[0.25488281]],\n",
            "\n",
            "       [[0.04391479]],\n",
            "\n",
            "       [[0.70117188]]]), array([[[0.01873779]],\n",
            "\n",
            "       [[0.08001709]],\n",
            "\n",
            "       [[0.90136719]]]), array([[[0.24511719]],\n",
            "\n",
            "       [[0.08178711]],\n",
            "\n",
            "       [[0.67333984]]]), array([[[0.01589966]],\n",
            "\n",
            "       [[0.11022949]],\n",
            "\n",
            "       [[0.87402344]]]), array([[[0.07409668]],\n",
            "\n",
            "       [[0.07006836]],\n",
            "\n",
            "       [[0.85595703]]]), array([[[0.03201294]],\n",
            "\n",
            "       [[0.03161621]],\n",
            "\n",
            "       [[0.93652344]]]), array([[[0.29003906]],\n",
            "\n",
            "       [[0.16723633]],\n",
            "\n",
            "       [[0.54296875]]]), array([[[0.25219727]],\n",
            "\n",
            "       [[0.04098511]],\n",
            "\n",
            "       [[0.70703125]]]), array([[[0.11694336]],\n",
            "\n",
            "       [[0.0383606 ]],\n",
            "\n",
            "       [[0.84472656]]]), array([[[0.29077148]],\n",
            "\n",
            "       [[0.04199219]],\n",
            "\n",
            "       [[0.66748047]]]), array([[[0.09356689]],\n",
            "\n",
            "       [[0.03494263]],\n",
            "\n",
            "       [[0.87158203]]]), array([[[0.07659912]],\n",
            "\n",
            "       [[0.07574463]],\n",
            "\n",
            "       [[0.84765625]]]), array([[[0.06500244]],\n",
            "\n",
            "       [[0.06396484]],\n",
            "\n",
            "       [[0.87109375]]]), array([[[0.15588379]],\n",
            "\n",
            "       [[0.09503174]],\n",
            "\n",
            "       [[0.74902344]]]), array([[[0.04187012]],\n",
            "\n",
            "       [[0.07336426]],\n",
            "\n",
            "       [[0.88476562]]]), array([[[0.05407715]],\n",
            "\n",
            "       [[0.03384399]],\n",
            "\n",
            "       [[0.91210938]]]), array([[[0.11407471]],\n",
            "\n",
            "       [[0.05618286]],\n",
            "\n",
            "       [[0.82958984]]]), array([[[0.09210205]],\n",
            "\n",
            "       [[0.08447266]],\n",
            "\n",
            "       [[0.82324219]]]), array([[[0.47827148]],\n",
            "\n",
            "       [[0.02626038]],\n",
            "\n",
            "       [[0.49560547]]]), array([[[0.10864258]],\n",
            "\n",
            "       [[0.08624268]],\n",
            "\n",
            "       [[0.80517578]]]), array([[[0.30859375]],\n",
            "\n",
            "       [[0.04541016]],\n",
            "\n",
            "       [[0.64599609]]]), array([[[0.16186523]],\n",
            "\n",
            "       [[0.0597229 ]],\n",
            "\n",
            "       [[0.77832031]]]), array([[[0.03631592]],\n",
            "\n",
            "       [[0.05032349]],\n",
            "\n",
            "       [[0.91357422]]]), array([[[0.3840332 ]],\n",
            "\n",
            "       [[0.02905273]],\n",
            "\n",
            "       [[0.58691406]]]), array([[[0.06130981]],\n",
            "\n",
            "       [[0.04611206]],\n",
            "\n",
            "       [[0.89257812]]]), array([[[0.13439941]],\n",
            "\n",
            "       [[0.06069946]],\n",
            "\n",
            "       [[0.8046875 ]]]), array([[[0.28149414]],\n",
            "\n",
            "       [[0.06604004]],\n",
            "\n",
            "       [[0.65234375]]]), array([[[0.15722656]],\n",
            "\n",
            "       [[0.05062866]],\n",
            "\n",
            "       [[0.79199219]]]), array([[[0.2364502 ]],\n",
            "\n",
            "       [[0.05432129]],\n",
            "\n",
            "       [[0.70947266]]]), array([[[0.24511719]],\n",
            "\n",
            "       [[0.05459595]],\n",
            "\n",
            "       [[0.70019531]]]), array([[[0.07659912]],\n",
            "\n",
            "       [[0.06103516]],\n",
            "\n",
            "       [[0.86230469]]]), array([[[0.03671265]],\n",
            "\n",
            "       [[0.04125977]],\n",
            "\n",
            "       [[0.921875  ]]]), array([[[0.18139648]],\n",
            "\n",
            "       [[0.05029297]],\n",
            "\n",
            "       [[0.76855469]]]), array([[[0.20532227]],\n",
            "\n",
            "       [[0.06469727]],\n",
            "\n",
            "       [[0.72998047]]]), array([[[0.83544922]],\n",
            "\n",
            "       [[0.0166626 ]],\n",
            "\n",
            "       [[0.14782715]]]), array([[[0.30102539]],\n",
            "\n",
            "       [[0.04473877]],\n",
            "\n",
            "       [[0.65429688]]]), array([[[0.09344482]],\n",
            "\n",
            "       [[0.06506348]],\n",
            "\n",
            "       [[0.84130859]]]), array([[[0.1751709 ]],\n",
            "\n",
            "       [[0.09747314]],\n",
            "\n",
            "       [[0.72753906]]]), array([[[0.03561401]],\n",
            "\n",
            "       [[0.02635193]],\n",
            "\n",
            "       [[0.93798828]]]), array([[[0.21679688]],\n",
            "\n",
            "       [[0.05813599]],\n",
            "\n",
            "       [[0.72509766]]]), array([[[0.57275391]],\n",
            "\n",
            "       [[0.0254364 ]],\n",
            "\n",
            "       [[0.40161133]]]), array([[[0.13989258]],\n",
            "\n",
            "       [[0.02862549]],\n",
            "\n",
            "       [[0.83154297]]]), array([[[0.15515137]],\n",
            "\n",
            "       [[0.09588623]],\n",
            "\n",
            "       [[0.74902344]]]), array([[[0.12359619]],\n",
            "\n",
            "       [[0.08758545]],\n",
            "\n",
            "       [[0.7890625 ]]]), array([[[0.51953125]],\n",
            "\n",
            "       [[0.07104492]],\n",
            "\n",
            "       [[0.40942383]]]), array([[[0.28198242]],\n",
            "\n",
            "       [[0.02865601]],\n",
            "\n",
            "       [[0.68945312]]]), array([[[0.01960754]],\n",
            "\n",
            "       [[0.17321777]],\n",
            "\n",
            "       [[0.80712891]]]), array([[[0.06512451]],\n",
            "\n",
            "       [[0.03555298]],\n",
            "\n",
            "       [[0.89941406]]]), array([[[0.06976318]],\n",
            "\n",
            "       [[0.04464722]],\n",
            "\n",
            "       [[0.88574219]]]), array([[[0.04376221]],\n",
            "\n",
            "       [[0.02700806]],\n",
            "\n",
            "       [[0.92919922]]]), array([[[0.36376953]],\n",
            "\n",
            "       [[0.02644348]],\n",
            "\n",
            "       [[0.60986328]]]), array([[[0.18469238]],\n",
            "\n",
            "       [[0.06817627]],\n",
            "\n",
            "       [[0.74707031]]]), array([[[0.0430603 ]],\n",
            "\n",
            "       [[0.02049255]],\n",
            "\n",
            "       [[0.93652344]]]), array([[[0.12432861]],\n",
            "\n",
            "       [[0.03031921]],\n",
            "\n",
            "       [[0.84521484]]]), array([[[0.34936523]],\n",
            "\n",
            "       [[0.12493896]],\n",
            "\n",
            "       [[0.52539062]]]), array([[[0.06231689]],\n",
            "\n",
            "       [[0.04629517]],\n",
            "\n",
            "       [[0.89160156]]]), array([[[0.37963867]],\n",
            "\n",
            "       [[0.07446289]],\n",
            "\n",
            "       [[0.54589844]]]), array([[[0.11340332]],\n",
            "\n",
            "       [[0.07659912]],\n",
            "\n",
            "       [[0.81005859]]]), array([[[0.36083984]],\n",
            "\n",
            "       [[0.06628418]],\n",
            "\n",
            "       [[0.57275391]]]), array([[[0.11444092]],\n",
            "\n",
            "       [[0.09606934]],\n",
            "\n",
            "       [[0.78955078]]]), array([[[0.06658936]],\n",
            "\n",
            "       [[0.04998779]],\n",
            "\n",
            "       [[0.88330078]]]), array([[[0.18359375]],\n",
            "\n",
            "       [[0.06970215]],\n",
            "\n",
            "       [[0.74658203]]]), array([[[0.16943359]],\n",
            "\n",
            "       [[0.06958008]],\n",
            "\n",
            "       [[0.76074219]]]), array([[[0.35888672]],\n",
            "\n",
            "       [[0.0496521 ]],\n",
            "\n",
            "       [[0.59179688]]]), array([[[0.73925781]],\n",
            "\n",
            "       [[0.02064514]],\n",
            "\n",
            "       [[0.24023438]]]), array([[[0.54589844]],\n",
            "\n",
            "       [[0.02375793]],\n",
            "\n",
            "       [[0.43041992]]]), array([[[0.15441895]],\n",
            "\n",
            "       [[0.10992432]],\n",
            "\n",
            "       [[0.73583984]]]), array([[[0.23510742]],\n",
            "\n",
            "       [[0.05059814]],\n",
            "\n",
            "       [[0.71435547]]]), array([[[0.23352051]],\n",
            "\n",
            "       [[0.0458374 ]],\n",
            "\n",
            "       [[0.72070312]]]), array([[[0.76757812]],\n",
            "\n",
            "       [[0.01278687]],\n",
            "\n",
            "       [[0.21948242]]]), array([[[0.1541748 ]],\n",
            "\n",
            "       [[0.10003662]],\n",
            "\n",
            "       [[0.74560547]]]), array([[[0.62890625]],\n",
            "\n",
            "       [[0.02139282]],\n",
            "\n",
            "       [[0.34960938]]]), array([[[0.17944336]],\n",
            "\n",
            "       [[0.05123901]],\n",
            "\n",
            "       [[0.76953125]]]), array([[[0.29589844]],\n",
            "\n",
            "       [[0.05245972]],\n",
            "\n",
            "       [[0.65185547]]]), array([[[0.12719727]],\n",
            "\n",
            "       [[0.03683472]],\n",
            "\n",
            "       [[0.8359375 ]]]), array([[[0.02894592]],\n",
            "\n",
            "       [[0.02375793]],\n",
            "\n",
            "       [[0.94726562]]]), array([[[0.13549805]],\n",
            "\n",
            "       [[0.04333496]],\n",
            "\n",
            "       [[0.82128906]]]), array([[[0.08074951]],\n",
            "\n",
            "       [[0.05877686]],\n",
            "\n",
            "       [[0.86035156]]]), array([[[0.15795898]],\n",
            "\n",
            "       [[0.07885742]],\n",
            "\n",
            "       [[0.76318359]]]), array([[[0.17028809]],\n",
            "\n",
            "       [[0.0670166 ]],\n",
            "\n",
            "       [[0.76269531]]]), array([[[0.02716064]],\n",
            "\n",
            "       [[0.10784912]],\n",
            "\n",
            "       [[0.86474609]]]), array([[[0.12536621]],\n",
            "\n",
            "       [[0.05197144]],\n",
            "\n",
            "       [[0.82275391]]]), array([[[0.0276947 ]],\n",
            "\n",
            "       [[0.11157227]],\n",
            "\n",
            "       [[0.86083984]]]), array([[[0.62158203]],\n",
            "\n",
            "       [[0.03102112]],\n",
            "\n",
            "       [[0.34716797]]]), array([[[0.12017822]],\n",
            "\n",
            "       [[0.04095459]],\n",
            "\n",
            "       [[0.83886719]]]), array([[[0.08966064]],\n",
            "\n",
            "       [[0.08178711]],\n",
            "\n",
            "       [[0.82861328]]]), array([[[0.46801758]],\n",
            "\n",
            "       [[0.05752563]],\n",
            "\n",
            "       [[0.47436523]]]), array([[[0.18481445]],\n",
            "\n",
            "       [[0.06713867]],\n",
            "\n",
            "       [[0.74804688]]]), array([[[0.16003418]],\n",
            "\n",
            "       [[0.11560059]],\n",
            "\n",
            "       [[0.72460938]]]), array([[[0.05157471]],\n",
            "\n",
            "       [[0.03686523]],\n",
            "\n",
            "       [[0.91162109]]]), array([[[0.25878906]],\n",
            "\n",
            "       [[0.03625488]],\n",
            "\n",
            "       [[0.70507812]]]), array([[[0.15246582]],\n",
            "\n",
            "       [[0.12127686]],\n",
            "\n",
            "       [[0.72607422]]]), array([[[0.42553711]],\n",
            "\n",
            "       [[0.06610107]],\n",
            "\n",
            "       [[0.50830078]]]), array([[[0.3203125 ]],\n",
            "\n",
            "       [[0.04879761]],\n",
            "\n",
            "       [[0.63085938]]]), array([[[0.79931641]],\n",
            "\n",
            "       [[0.0177002 ]],\n",
            "\n",
            "       [[0.18322754]]]), array([[[0.12976074]],\n",
            "\n",
            "       [[0.11065674]],\n",
            "\n",
            "       [[0.75976562]]]), array([[[0.17810059]],\n",
            "\n",
            "       [[0.05715942]],\n",
            "\n",
            "       [[0.76464844]]]), array([[[0.28930664]],\n",
            "\n",
            "       [[0.03671265]],\n",
            "\n",
            "       [[0.67382812]]]), array([[[0.24768066]],\n",
            "\n",
            "       [[0.04772949]],\n",
            "\n",
            "       [[0.70458984]]]), array([[[0.03778076]],\n",
            "\n",
            "       [[0.04669189]],\n",
            "\n",
            "       [[0.91552734]]]), array([[[0.00879669]],\n",
            "\n",
            "       [[0.13366699]],\n",
            "\n",
            "       [[0.85742188]]]), array([[[0.20422363]],\n",
            "\n",
            "       [[0.08648682]],\n",
            "\n",
            "       [[0.70947266]]]), array([[[0.15380859]],\n",
            "\n",
            "       [[0.10479736]],\n",
            "\n",
            "       [[0.74121094]]]), array([[[0.1340332 ]],\n",
            "\n",
            "       [[0.03674316]],\n",
            "\n",
            "       [[0.82910156]]]), array([[[0.05526733]],\n",
            "\n",
            "       [[0.03213501]],\n",
            "\n",
            "       [[0.91259766]]]), array([[[0.26635742]],\n",
            "\n",
            "       [[0.11126709]],\n",
            "\n",
            "       [[0.62207031]]]), array([[[0.11450195]],\n",
            "\n",
            "       [[0.04199219]],\n",
            "\n",
            "       [[0.84375   ]]]), array([[[0.09991455]],\n",
            "\n",
            "       [[0.06835938]],\n",
            "\n",
            "       [[0.83154297]]]), array([[[0.16223145]],\n",
            "\n",
            "       [[0.03930664]],\n",
            "\n",
            "       [[0.79833984]]]), array([[[0.37719727]],\n",
            "\n",
            "       [[0.02644348]],\n",
            "\n",
            "       [[0.59667969]]]), array([[[0.16601562]],\n",
            "\n",
            "       [[0.04296875]],\n",
            "\n",
            "       [[0.79101562]]]), array([[[0.13928223]],\n",
            "\n",
            "       [[0.08270264]],\n",
            "\n",
            "       [[0.77783203]]]), array([[[0.2565918 ]],\n",
            "\n",
            "       [[0.05957031]],\n",
            "\n",
            "       [[0.68408203]]]), array([[[0.081604  ]],\n",
            "\n",
            "       [[0.06506348]],\n",
            "\n",
            "       [[0.85351562]]]), array([[[0.14672852]],\n",
            "\n",
            "       [[0.04138184]],\n",
            "\n",
            "       [[0.81201172]]]), array([[[0.06500244]],\n",
            "\n",
            "       [[0.03900146]],\n",
            "\n",
            "       [[0.89599609]]]), array([[[0.01302338]],\n",
            "\n",
            "       [[0.04101562]],\n",
            "\n",
            "       [[0.94580078]]]), array([[[0.10400391]],\n",
            "\n",
            "       [[0.0484314 ]],\n",
            "\n",
            "       [[0.84765625]]]), array([[[0.15917969]],\n",
            "\n",
            "       [[0.08074951]],\n",
            "\n",
            "       [[0.76025391]]]), array([[[0.35546875]],\n",
            "\n",
            "       [[0.06970215]],\n",
            "\n",
            "       [[0.57470703]]]), array([[[0.63818359]],\n",
            "\n",
            "       [[0.03100586]],\n",
            "\n",
            "       [[0.33081055]]]), array([[[0.08526611]],\n",
            "\n",
            "       [[0.10717773]],\n",
            "\n",
            "       [[0.80761719]]]), array([[[0.47094727]],\n",
            "\n",
            "       [[0.08953857]],\n",
            "\n",
            "       [[0.43945312]]]), array([[[0.2902832 ]],\n",
            "\n",
            "       [[0.06976318]],\n",
            "\n",
            "       [[0.63964844]]]), array([[[0.13635254]],\n",
            "\n",
            "       [[0.08465576]],\n",
            "\n",
            "       [[0.77929688]]]), array([[[0.02470398]],\n",
            "\n",
            "       [[0.05108643]],\n",
            "\n",
            "       [[0.92431641]]]), array([[[0.09844971]],\n",
            "\n",
            "       [[0.1027832 ]],\n",
            "\n",
            "       [[0.79882812]]]), array([[[0.08532715]],\n",
            "\n",
            "       [[0.02325439]],\n",
            "\n",
            "       [[0.89160156]]]), array([[[0.49389648]],\n",
            "\n",
            "       [[0.05874634]],\n",
            "\n",
            "       [[0.44726562]]]), array([[[0.15637207]],\n",
            "\n",
            "       [[0.03878784]],\n",
            "\n",
            "       [[0.8046875 ]]]), array([[[0.09393311]],\n",
            "\n",
            "       [[0.06500244]],\n",
            "\n",
            "       [[0.84130859]]]), array([[[0.1505127 ]],\n",
            "\n",
            "       [[0.0378418 ]],\n",
            "\n",
            "       [[0.81152344]]]), array([[[0.41796875]],\n",
            "\n",
            "       [[0.03665161]],\n",
            "\n",
            "       [[0.54541016]]]), array([[[0.67724609]],\n",
            "\n",
            "       [[0.01950073]],\n",
            "\n",
            "       [[0.30322266]]]), array([[[0.1239624 ]],\n",
            "\n",
            "       [[0.02854919]],\n",
            "\n",
            "       [[0.84765625]]]), array([[[0.12634277]],\n",
            "\n",
            "       [[0.09375   ]],\n",
            "\n",
            "       [[0.77978516]]]), array([[[0.04412842]],\n",
            "\n",
            "       [[0.05404663]],\n",
            "\n",
            "       [[0.90185547]]]), array([[[0.09283447]],\n",
            "\n",
            "       [[0.02035522]],\n",
            "\n",
            "       [[0.88671875]]]), array([[[0.06219482]],\n",
            "\n",
            "       [[0.05264282]],\n",
            "\n",
            "       [[0.88525391]]]), array([[[0.28393555]],\n",
            "\n",
            "       [[0.06118774]],\n",
            "\n",
            "       [[0.65478516]]]), array([[[0.02825928]],\n",
            "\n",
            "       [[0.11810303]],\n",
            "\n",
            "       [[0.85351562]]]), array([[[0.49829102]],\n",
            "\n",
            "       [[0.02032471]],\n",
            "\n",
            "       [[0.48144531]]]), array([[[0.02523804]],\n",
            "\n",
            "       [[0.04968262]],\n",
            "\n",
            "       [[0.92529297]]]), array([[[0.0970459 ]],\n",
            "\n",
            "       [[0.09326172]],\n",
            "\n",
            "       [[0.80957031]]]), array([[[0.33129883]],\n",
            "\n",
            "       [[0.0607605 ]],\n",
            "\n",
            "       [[0.60791016]]]), array([[[0.01138306]],\n",
            "\n",
            "       [[0.08953857]],\n",
            "\n",
            "       [[0.89892578]]]), array([[[0.23327637]],\n",
            "\n",
            "       [[0.04467773]],\n",
            "\n",
            "       [[0.72216797]]]), array([[[0.1229248 ]],\n",
            "\n",
            "       [[0.04110718]],\n",
            "\n",
            "       [[0.8359375 ]]]), array([[[0.17102051]],\n",
            "\n",
            "       [[0.06323242]],\n",
            "\n",
            "       [[0.765625  ]]]), array([[[0.07476807]],\n",
            "\n",
            "       [[0.02018738]],\n",
            "\n",
            "       [[0.90478516]]]), array([[[0.4597168 ]],\n",
            "\n",
            "       [[0.02346802]],\n",
            "\n",
            "       [[0.51660156]]]), array([[[0.25317383]],\n",
            "\n",
            "       [[0.034729  ]],\n",
            "\n",
            "       [[0.71191406]]]), array([[[0.11456299]],\n",
            "\n",
            "       [[0.04754639]],\n",
            "\n",
            "       [[0.83789062]]]), array([[[0.18017578]],\n",
            "\n",
            "       [[0.07318115]],\n",
            "\n",
            "       [[0.74658203]]]), array([[[0.54736328]],\n",
            "\n",
            "       [[0.05984497]],\n",
            "\n",
            "       [[0.39257812]]]), array([[[0.13195801]],\n",
            "\n",
            "       [[0.05575562]],\n",
            "\n",
            "       [[0.8125    ]]]), array([[[0.07611084]],\n",
            "\n",
            "       [[0.05105591]],\n",
            "\n",
            "       [[0.87304688]]]), array([[[0.23400879]],\n",
            "\n",
            "       [[0.0475769 ]],\n",
            "\n",
            "       [[0.71826172]]]), array([[[0.22753906]],\n",
            "\n",
            "       [[0.05352783]],\n",
            "\n",
            "       [[0.71875   ]]]), array([[[0.1192627 ]],\n",
            "\n",
            "       [[0.03210449]],\n",
            "\n",
            "       [[0.84863281]]]), array([[[0.07928467]],\n",
            "\n",
            "       [[0.05844116]],\n",
            "\n",
            "       [[0.86230469]]]), array([[[0.07647705]],\n",
            "\n",
            "       [[0.06665039]],\n",
            "\n",
            "       [[0.85693359]]]), array([[[0.79052734]],\n",
            "\n",
            "       [[0.01974487]],\n",
            "\n",
            "       [[0.18969727]]]), array([[[0.94384766]],\n",
            "\n",
            "       [[0.00808716]],\n",
            "\n",
            "       [[0.04785156]]]), array([[[0.13439941]],\n",
            "\n",
            "       [[0.07049561]],\n",
            "\n",
            "       [[0.79492188]]]), array([[[0.54394531]],\n",
            "\n",
            "       [[0.03753662]],\n",
            "\n",
            "       [[0.41845703]]]), array([[[0.13391113]],\n",
            "\n",
            "       [[0.11499023]],\n",
            "\n",
            "       [[0.75097656]]]), array([[[0.00858307]],\n",
            "\n",
            "       [[0.14208984]],\n",
            "\n",
            "       [[0.84960938]]]), array([[[0.46435547]],\n",
            "\n",
            "       [[0.05096436]],\n",
            "\n",
            "       [[0.48461914]]]), array([[[0.10662842]],\n",
            "\n",
            "       [[0.04864502]],\n",
            "\n",
            "       [[0.84472656]]]), array([[[0.45068359]],\n",
            "\n",
            "       [[0.04711914]],\n",
            "\n",
            "       [[0.50244141]]]), array([[[0.41381836]],\n",
            "\n",
            "       [[0.03128052]],\n",
            "\n",
            "       [[0.5546875 ]]]), array([[[0.05615234]],\n",
            "\n",
            "       [[0.04443359]],\n",
            "\n",
            "       [[0.89941406]]]), array([[[0.13464355]],\n",
            "\n",
            "       [[0.03961182]],\n",
            "\n",
            "       [[0.82568359]]]), array([[[0.03448486]],\n",
            "\n",
            "       [[0.17675781]],\n",
            "\n",
            "       [[0.78857422]]]), array([[[0.28344727]],\n",
            "\n",
            "       [[0.05438232]],\n",
            "\n",
            "       [[0.66210938]]]), array([[[0.05554199]],\n",
            "\n",
            "       [[0.05413818]],\n",
            "\n",
            "       [[0.89013672]]]), array([[[0.21960449]],\n",
            "\n",
            "       [[0.06298828]],\n",
            "\n",
            "       [[0.71728516]]]), array([[[0.10217285]],\n",
            "\n",
            "       [[0.03018188]],\n",
            "\n",
            "       [[0.86767578]]]), array([[[0.11590576]],\n",
            "\n",
            "       [[0.02438354]],\n",
            "\n",
            "       [[0.85986328]]]), array([[[0.10388184]],\n",
            "\n",
            "       [[0.04440308]],\n",
            "\n",
            "       [[0.8515625 ]]]), array([[[0.06134033]],\n",
            "\n",
            "       [[0.02296448]],\n",
            "\n",
            "       [[0.91552734]]]), array([[[0.09063721]],\n",
            "\n",
            "       [[0.03283691]],\n",
            "\n",
            "       [[0.87646484]]]), array([[[0.07843018]],\n",
            "\n",
            "       [[0.02127075]],\n",
            "\n",
            "       [[0.90039062]]]), array([[[0.1126709 ]],\n",
            "\n",
            "       [[0.03231812]],\n",
            "\n",
            "       [[0.85498047]]]), array([[[0.0333252 ]],\n",
            "\n",
            "       [[0.05114746]],\n",
            "\n",
            "       [[0.91552734]]]), array([[[0.35742188]],\n",
            "\n",
            "       [[0.08270264]],\n",
            "\n",
            "       [[0.56005859]]]), array([[[0.11090088]],\n",
            "\n",
            "       [[0.04348755]],\n",
            "\n",
            "       [[0.84570312]]]), array([[[0.33251953]],\n",
            "\n",
            "       [[0.04733276]],\n",
            "\n",
            "       [[0.62011719]]]), array([[[0.18395996]],\n",
            "\n",
            "       [[0.06591797]],\n",
            "\n",
            "       [[0.75      ]]]), array([[[0.08740234]],\n",
            "\n",
            "       [[0.06970215]],\n",
            "\n",
            "       [[0.84277344]]]), array([[[0.76269531]],\n",
            "\n",
            "       [[0.01109314]],\n",
            "\n",
            "       [[0.22619629]]]), array([[[0.05651855]],\n",
            "\n",
            "       [[0.09350586]],\n",
            "\n",
            "       [[0.85009766]]]), array([[[0.20007324]],\n",
            "\n",
            "       [[0.03427124]],\n",
            "\n",
            "       [[0.765625  ]]]), array([[[0.06738281]],\n",
            "\n",
            "       [[0.07928467]],\n",
            "\n",
            "       [[0.85351562]]]), array([[[0.3972168 ]],\n",
            "\n",
            "       [[0.04806519]],\n",
            "\n",
            "       [[0.5546875 ]]]), array([[[0.18103027]],\n",
            "\n",
            "       [[0.09332275]],\n",
            "\n",
            "       [[0.72558594]]]), array([[[0.14013672]],\n",
            "\n",
            "       [[0.08752441]],\n",
            "\n",
            "       [[0.77246094]]]), array([[[0.40136719]],\n",
            "\n",
            "       [[0.03256226]],\n",
            "\n",
            "       [[0.56591797]]]), array([[[0.05334473]],\n",
            "\n",
            "       [[0.03561401]],\n",
            "\n",
            "       [[0.91113281]]]), array([[[0.02879333]],\n",
            "\n",
            "       [[0.02804565]],\n",
            "\n",
            "       [[0.94335938]]]), array([[[0.05801392]],\n",
            "\n",
            "       [[0.05990601]],\n",
            "\n",
            "       [[0.88232422]]]), array([[[0.15283203]],\n",
            "\n",
            "       [[0.0736084 ]],\n",
            "\n",
            "       [[0.7734375 ]]]), array([[[0.24121094]],\n",
            "\n",
            "       [[0.09448242]],\n",
            "\n",
            "       [[0.6640625 ]]]), array([[[0.0814209 ]],\n",
            "\n",
            "       [[0.04522705]],\n",
            "\n",
            "       [[0.87353516]]]), array([[[0.17797852]],\n",
            "\n",
            "       [[0.05596924]],\n",
            "\n",
            "       [[0.76611328]]]), array([[[0.0713501 ]],\n",
            "\n",
            "       [[0.10058594]],\n",
            "\n",
            "       [[0.828125  ]]]), array([[[0.11853027]],\n",
            "\n",
            "       [[0.06610107]],\n",
            "\n",
            "       [[0.81542969]]]), array([[[0.50634766]],\n",
            "\n",
            "       [[0.04672241]],\n",
            "\n",
            "       [[0.44702148]]]), array([[[0.10839844]],\n",
            "\n",
            "       [[0.04055786]],\n",
            "\n",
            "       [[0.85107422]]]), array([[[0.0456543 ]],\n",
            "\n",
            "       [[0.13562012]],\n",
            "\n",
            "       [[0.81884766]]]), array([[[0.09991455]],\n",
            "\n",
            "       [[0.06494141]],\n",
            "\n",
            "       [[0.83496094]]]), array([[[0.27758789]],\n",
            "\n",
            "       [[0.04553223]],\n",
            "\n",
            "       [[0.67675781]]]), array([[[0.30810547]],\n",
            "\n",
            "       [[0.06500244]],\n",
            "\n",
            "       [[0.62695312]]]), array([[[0.07751465]],\n",
            "\n",
            "       [[0.09979248]],\n",
            "\n",
            "       [[0.82275391]]]), array([[[0.27978516]],\n",
            "\n",
            "       [[0.05258179]],\n",
            "\n",
            "       [[0.66748047]]]), array([[[0.20092773]],\n",
            "\n",
            "       [[0.05151367]],\n",
            "\n",
            "       [[0.74755859]]]), array([[[0.44946289]],\n",
            "\n",
            "       [[0.03213501]],\n",
            "\n",
            "       [[0.51855469]]]), array([[[0.19787598]],\n",
            "\n",
            "       [[0.05722046]],\n",
            "\n",
            "       [[0.74511719]]]), array([[[0.0892334 ]],\n",
            "\n",
            "       [[0.09686279]],\n",
            "\n",
            "       [[0.81396484]]]), array([[[0.06671143]],\n",
            "\n",
            "       [[0.0423584 ]],\n",
            "\n",
            "       [[0.89111328]]]), array([[[0.56933594]],\n",
            "\n",
            "       [[0.020401  ]],\n",
            "\n",
            "       [[0.41015625]]]), array([[[0.02676392]],\n",
            "\n",
            "       [[0.04727173]],\n",
            "\n",
            "       [[0.92578125]]]), array([[[0.41723633]],\n",
            "\n",
            "       [[0.04031372]],\n",
            "\n",
            "       [[0.54248047]]]), array([[[0.05722046]],\n",
            "\n",
            "       [[0.0447998 ]],\n",
            "\n",
            "       [[0.89794922]]]), array([[[0.01138306]],\n",
            "\n",
            "       [[0.06137085]],\n",
            "\n",
            "       [[0.92724609]]]), array([[[0.11706543]],\n",
            "\n",
            "       [[0.06378174]],\n",
            "\n",
            "       [[0.81933594]]]), array([[[0.16833496]],\n",
            "\n",
            "       [[0.0894165 ]],\n",
            "\n",
            "       [[0.7421875 ]]]), array([[[0.04342651]],\n",
            "\n",
            "       [[0.06787109]],\n",
            "\n",
            "       [[0.88867188]]]), array([[[0.10479736]],\n",
            "\n",
            "       [[0.04830933]],\n",
            "\n",
            "       [[0.84667969]]]), array([[[0.37011719]],\n",
            "\n",
            "       [[0.04116821]],\n",
            "\n",
            "       [[0.58886719]]]), array([[[0.07757568]],\n",
            "\n",
            "       [[0.04101562]],\n",
            "\n",
            "       [[0.88134766]]]), array([[[0.13464355]],\n",
            "\n",
            "       [[0.02575684]],\n",
            "\n",
            "       [[0.83935547]]]), array([[[0.21691895]],\n",
            "\n",
            "       [[0.10778809]],\n",
            "\n",
            "       [[0.67529297]]]), array([[[0.18835449]],\n",
            "\n",
            "       [[0.10241699]],\n",
            "\n",
            "       [[0.70947266]]]), array([[[0.19946289]],\n",
            "\n",
            "       [[0.1081543 ]],\n",
            "\n",
            "       [[0.69238281]]]), array([[[0.36621094]],\n",
            "\n",
            "       [[0.04187012]],\n",
            "\n",
            "       [[0.59179688]]]), array([[[0.16809082]],\n",
            "\n",
            "       [[0.01589966]],\n",
            "\n",
            "       [[0.81591797]]]), array([[[0.04022217]],\n",
            "\n",
            "       [[0.02615356]],\n",
            "\n",
            "       [[0.93359375]]]), array([[[0.16015625]],\n",
            "\n",
            "       [[0.06494141]],\n",
            "\n",
            "       [[0.77490234]]]), array([[[0.21350098]],\n",
            "\n",
            "       [[0.03744507]],\n",
            "\n",
            "       [[0.74902344]]]), array([[[0.06246948]],\n",
            "\n",
            "       [[0.05474854]],\n",
            "\n",
            "       [[0.8828125 ]]]), array([[[0.14135742]],\n",
            "\n",
            "       [[0.06069946]],\n",
            "\n",
            "       [[0.79785156]]]), array([[[0.06695557]],\n",
            "\n",
            "       [[0.0430603 ]],\n",
            "\n",
            "       [[0.89013672]]]), array([[[0.09020996]],\n",
            "\n",
            "       [[0.07763672]],\n",
            "\n",
            "       [[0.83203125]]]), array([[[0.390625  ]],\n",
            "\n",
            "       [[0.07073975]],\n",
            "\n",
            "       [[0.53857422]]]), array([[[0.76708984]],\n",
            "\n",
            "       [[0.01812744]],\n",
            "\n",
            "       [[0.21484375]]]), array([[[0.63720703]],\n",
            "\n",
            "       [[0.04693604]],\n",
            "\n",
            "       [[0.31616211]]]), array([[[0.24633789]],\n",
            "\n",
            "       [[0.06262207]],\n",
            "\n",
            "       [[0.69091797]]]), array([[[0.29248047]],\n",
            "\n",
            "       [[0.02394104]],\n",
            "\n",
            "       [[0.68359375]]]), array([[[0.08929443]],\n",
            "\n",
            "       [[0.08428955]],\n",
            "\n",
            "       [[0.82617188]]]), array([[[0.20581055]],\n",
            "\n",
            "       [[0.06646729]],\n",
            "\n",
            "       [[0.72753906]]]), array([[[0.65576172]],\n",
            "\n",
            "       [[0.03411865]],\n",
            "\n",
            "       [[0.31005859]]]), array([[[0.21728516]],\n",
            "\n",
            "       [[0.05310059]],\n",
            "\n",
            "       [[0.72949219]]]), array([[[0.37597656]],\n",
            "\n",
            "       [[0.06585693]],\n",
            "\n",
            "       [[0.55810547]]]), array([[[0.12353516]],\n",
            "\n",
            "       [[0.08892822]],\n",
            "\n",
            "       [[0.78759766]]]), array([[[0.02462769]],\n",
            "\n",
            "       [[0.07727051]],\n",
            "\n",
            "       [[0.89794922]]]), array([[[0.12866211]],\n",
            "\n",
            "       [[0.05752563]],\n",
            "\n",
            "       [[0.81396484]]]), array([[[0.01117706]],\n",
            "\n",
            "       [[0.23522949]],\n",
            "\n",
            "       [[0.75341797]]]), array([[[0.34155273]],\n",
            "\n",
            "       [[0.03387451]],\n",
            "\n",
            "       [[0.62451172]]]), array([[[0.06097412]],\n",
            "\n",
            "       [[0.05072021]],\n",
            "\n",
            "       [[0.88818359]]]), array([[[0.07531738]],\n",
            "\n",
            "       [[0.02850342]],\n",
            "\n",
            "       [[0.89599609]]]), array([[[0.07073975]],\n",
            "\n",
            "       [[0.0508728 ]],\n",
            "\n",
            "       [[0.87841797]]]), array([[[0.01109314]],\n",
            "\n",
            "       [[0.09881592]],\n",
            "\n",
            "       [[0.89013672]]]), array([[[0.10388184]],\n",
            "\n",
            "       [[0.03588867]],\n",
            "\n",
            "       [[0.86035156]]]), array([[[0.1595459 ]],\n",
            "\n",
            "       [[0.05978394]],\n",
            "\n",
            "       [[0.78076172]]]), array([[[0.06109619]],\n",
            "\n",
            "       [[0.02268982]],\n",
            "\n",
            "       [[0.91601562]]]), array([[[0.57714844]],\n",
            "\n",
            "       [[0.02661133]],\n",
            "\n",
            "       [[0.39624023]]]), array([[[0.67041016]],\n",
            "\n",
            "       [[0.02581787]],\n",
            "\n",
            "       [[0.30371094]]]), array([[[0.31860352]],\n",
            "\n",
            "       [[0.08251953]],\n",
            "\n",
            "       [[0.59863281]]]), array([[[0.28442383]],\n",
            "\n",
            "       [[0.0484314 ]],\n",
            "\n",
            "       [[0.66699219]]]), array([[[0.08673096]],\n",
            "\n",
            "       [[0.090271  ]],\n",
            "\n",
            "       [[0.82324219]]]), array([[[0.14648438]],\n",
            "\n",
            "       [[0.06524658]],\n",
            "\n",
            "       [[0.78808594]]]), array([[[0.12915039]],\n",
            "\n",
            "       [[0.11853027]],\n",
            "\n",
            "       [[0.75244141]]]), array([[[0.31274414]],\n",
            "\n",
            "       [[0.03765869]],\n",
            "\n",
            "       [[0.64990234]]]), array([[[0.23132324]],\n",
            "\n",
            "       [[0.08068848]],\n",
            "\n",
            "       [[0.68798828]]]), array([[[0.36401367]],\n",
            "\n",
            "       [[0.0562439 ]],\n",
            "\n",
            "       [[0.57958984]]]), array([[[0.03146362]],\n",
            "\n",
            "       [[0.0604248 ]],\n",
            "\n",
            "       [[0.90820312]]]), array([[[0.48608398]],\n",
            "\n",
            "       [[0.02755737]],\n",
            "\n",
            "       [[0.48657227]]]), array([[[0.49853516]],\n",
            "\n",
            "       [[0.08557129]],\n",
            "\n",
            "       [[0.41601562]]]), array([[[0.26123047]],\n",
            "\n",
            "       [[0.05709839]],\n",
            "\n",
            "       [[0.68164062]]]), array([[[0.2175293 ]],\n",
            "\n",
            "       [[0.06860352]],\n",
            "\n",
            "       [[0.71386719]]]), array([[[0.38085938]],\n",
            "\n",
            "       [[0.07757568]],\n",
            "\n",
            "       [[0.54150391]]]), array([[[0.58154297]],\n",
            "\n",
            "       [[0.02259827]],\n",
            "\n",
            "       [[0.39575195]]]), array([[[0.14294434]],\n",
            "\n",
            "       [[0.0914917 ]],\n",
            "\n",
            "       [[0.765625  ]]]), array([[[0.08850098]],\n",
            "\n",
            "       [[0.05764771]],\n",
            "\n",
            "       [[0.85400391]]]), array([[[0.0446167 ]],\n",
            "\n",
            "       [[0.03829956]],\n",
            "\n",
            "       [[0.91699219]]]), array([[[0.17565918]],\n",
            "\n",
            "       [[0.04220581]],\n",
            "\n",
            "       [[0.78222656]]]), array([[[0.01805115]],\n",
            "\n",
            "       [[0.09698486]],\n",
            "\n",
            "       [[0.88476562]]]), array([[[0.53662109]],\n",
            "\n",
            "       [[0.05630493]],\n",
            "\n",
            "       [[0.40722656]]]), array([[[0.05215454]],\n",
            "\n",
            "       [[0.02880859]],\n",
            "\n",
            "       [[0.91894531]]]), array([[[0.05947876]],\n",
            "\n",
            "       [[0.04754639]],\n",
            "\n",
            "       [[0.89306641]]]), array([[[0.17126465]],\n",
            "\n",
            "       [[0.0670166 ]],\n",
            "\n",
            "       [[0.76171875]]]), array([[[0.13098145]],\n",
            "\n",
            "       [[0.04803467]],\n",
            "\n",
            "       [[0.82128906]]]), array([[[0.12036133]],\n",
            "\n",
            "       [[0.06732178]],\n",
            "\n",
            "       [[0.8125    ]]]), array([[[0.08361816]],\n",
            "\n",
            "       [[0.0480957 ]],\n",
            "\n",
            "       [[0.86816406]]]), array([[[0.10217285]],\n",
            "\n",
            "       [[0.0574646 ]],\n",
            "\n",
            "       [[0.84033203]]]), array([[[0.36865234]],\n",
            "\n",
            "       [[0.02679443]],\n",
            "\n",
            "       [[0.60449219]]]), array([[[0.49291992]],\n",
            "\n",
            "       [[0.0411377 ]],\n",
            "\n",
            "       [[0.46606445]]]), array([[[0.02618408]],\n",
            "\n",
            "       [[0.01118469]],\n",
            "\n",
            "       [[0.96240234]]]), array([[[0.01209259]],\n",
            "\n",
            "       [[0.1628418 ]],\n",
            "\n",
            "       [[0.82519531]]]), array([[[0.04452515]],\n",
            "\n",
            "       [[0.02420044]],\n",
            "\n",
            "       [[0.93115234]]]), array([[[0.19726562]],\n",
            "\n",
            "       [[0.03942871]],\n",
            "\n",
            "       [[0.76318359]]]), array([[[0.10754395]],\n",
            "\n",
            "       [[0.06530762]],\n",
            "\n",
            "       [[0.82714844]]]), array([[[0.14733887]],\n",
            "\n",
            "       [[0.04782104]],\n",
            "\n",
            "       [[0.8046875 ]]]), array([[[0.05029297]],\n",
            "\n",
            "       [[0.03778076]],\n",
            "\n",
            "       [[0.91210938]]]), array([[[0.17602539]],\n",
            "\n",
            "       [[0.03063965]],\n",
            "\n",
            "       [[0.79345703]]]), array([[[0.52148438]],\n",
            "\n",
            "       [[0.04580688]],\n",
            "\n",
            "       [[0.43286133]]]), array([[[0.34082031]],\n",
            "\n",
            "       [[0.0609436 ]],\n",
            "\n",
            "       [[0.59814453]]]), array([[[0.04592896]],\n",
            "\n",
            "       [[0.13171387]],\n",
            "\n",
            "       [[0.82226562]]]), array([[[0.1529541 ]],\n",
            "\n",
            "       [[0.05447388]],\n",
            "\n",
            "       [[0.79248047]]]), array([[[0.4597168 ]],\n",
            "\n",
            "       [[0.02062988]],\n",
            "\n",
            "       [[0.51953125]]]), array([[[0.09924316]],\n",
            "\n",
            "       [[0.1036377 ]],\n",
            "\n",
            "       [[0.796875  ]]]), array([[[0.77490234]],\n",
            "\n",
            "       [[0.02444458]],\n",
            "\n",
            "       [[0.20092773]]]), array([[[0.27294922]],\n",
            "\n",
            "       [[0.04782104]],\n",
            "\n",
            "       [[0.67919922]]]), array([[[0.41772461]],\n",
            "\n",
            "       [[0.08465576]],\n",
            "\n",
            "       [[0.49755859]]]), array([[[0.11566162]],\n",
            "\n",
            "       [[0.08770752]],\n",
            "\n",
            "       [[0.796875  ]]]), array([[[0.16467285]],\n",
            "\n",
            "       [[0.04797363]],\n",
            "\n",
            "       [[0.78710938]]]), array([[[0.13317871]],\n",
            "\n",
            "       [[0.04089355]],\n",
            "\n",
            "       [[0.82617188]]]), array([[[0.11730957]],\n",
            "\n",
            "       [[0.05807495]],\n",
            "\n",
            "       [[0.82470703]]]), array([[[0.31860352]],\n",
            "\n",
            "       [[0.02561951]],\n",
            "\n",
            "       [[0.65576172]]]), array([[[0.12841797]],\n",
            "\n",
            "       [[0.05285645]],\n",
            "\n",
            "       [[0.81884766]]]), array([[[0.18188477]],\n",
            "\n",
            "       [[0.05456543]],\n",
            "\n",
            "       [[0.76367188]]]), array([[[0.6640625 ]],\n",
            "\n",
            "       [[0.01696777]],\n",
            "\n",
            "       [[0.3190918 ]]]), array([[[0.06420898]],\n",
            "\n",
            "       [[0.04730225]],\n",
            "\n",
            "       [[0.88867188]]]), array([[[0.05056763]],\n",
            "\n",
            "       [[0.04962158]],\n",
            "\n",
            "       [[0.89990234]]]), array([[[0.36621094]],\n",
            "\n",
            "       [[0.05438232]],\n",
            "\n",
            "       [[0.57958984]]]), array([[[0.24255371]],\n",
            "\n",
            "       [[0.06152344]],\n",
            "\n",
            "       [[0.69580078]]]), array([[[0.1940918 ]],\n",
            "\n",
            "       [[0.06069946]],\n",
            "\n",
            "       [[0.74511719]]]), array([[[0.2277832 ]],\n",
            "\n",
            "       [[0.02931213]],\n",
            "\n",
            "       [[0.74267578]]]), array([[[0.08294678]],\n",
            "\n",
            "       [[0.03143311]],\n",
            "\n",
            "       [[0.88574219]]]), array([[[0.27929688]],\n",
            "\n",
            "       [[0.04064941]],\n",
            "\n",
            "       [[0.68017578]]]), array([[[0.37670898]],\n",
            "\n",
            "       [[0.03656006]],\n",
            "\n",
            "       [[0.58691406]]]), array([[[0.23193359]],\n",
            "\n",
            "       [[0.0836792 ]],\n",
            "\n",
            "       [[0.68457031]]]), array([[[0.63720703]],\n",
            "\n",
            "       [[0.05767822]],\n",
            "\n",
            "       [[0.30517578]]]), array([[[0.13574219]],\n",
            "\n",
            "       [[0.05450439]],\n",
            "\n",
            "       [[0.80957031]]]), array([[[0.43505859]],\n",
            "\n",
            "       [[0.0369873 ]],\n",
            "\n",
            "       [[0.52783203]]]), array([[[0.43798828]],\n",
            "\n",
            "       [[0.02963257]],\n",
            "\n",
            "       [[0.53222656]]]), array([[[0.14489746]],\n",
            "\n",
            "       [[0.07958984]],\n",
            "\n",
            "       [[0.77539062]]]), array([[[0.66162109]],\n",
            "\n",
            "       [[0.03985596]],\n",
            "\n",
            "       [[0.29858398]]]), array([[[0.06408691]],\n",
            "\n",
            "       [[0.04263306]],\n",
            "\n",
            "       [[0.89306641]]]), array([[[0.3996582 ]],\n",
            "\n",
            "       [[0.02723694]],\n",
            "\n",
            "       [[0.57324219]]]), array([[[0.19360352]],\n",
            "\n",
            "       [[0.06616211]],\n",
            "\n",
            "       [[0.74023438]]]), array([[[0.17712402]],\n",
            "\n",
            "       [[0.04815674]],\n",
            "\n",
            "       [[0.77490234]]]), array([[[0.28735352]],\n",
            "\n",
            "       [[0.03744507]],\n",
            "\n",
            "       [[0.67529297]]]), array([[[0.09399414]],\n",
            "\n",
            "       [[0.09844971]],\n",
            "\n",
            "       [[0.80761719]]]), array([[[0.75097656]],\n",
            "\n",
            "       [[0.02793884]],\n",
            "\n",
            "       [[0.22131348]]]), array([[[0.19970703]],\n",
            "\n",
            "       [[0.06970215]],\n",
            "\n",
            "       [[0.73046875]]]), array([[[0.09680176]],\n",
            "\n",
            "       [[0.04623413]],\n",
            "\n",
            "       [[0.85693359]]]), array([[[0.0168457 ]],\n",
            "\n",
            "       [[0.04745483]],\n",
            "\n",
            "       [[0.93554688]]]), array([[[0.12890625]],\n",
            "\n",
            "       [[0.04559326]],\n",
            "\n",
            "       [[0.82568359]]]), array([[[0.02290344]],\n",
            "\n",
            "       [[0.09881592]],\n",
            "\n",
            "       [[0.87841797]]]), array([[[0.1427002 ]],\n",
            "\n",
            "       [[0.16052246]],\n",
            "\n",
            "       [[0.69677734]]]), array([[[0.04870605]],\n",
            "\n",
            "       [[0.12084961]],\n",
            "\n",
            "       [[0.83056641]]]), array([[[0.04794312]],\n",
            "\n",
            "       [[0.05596924]],\n",
            "\n",
            "       [[0.89599609]]]), array([[[0.22375488]],\n",
            "\n",
            "       [[0.06524658]],\n",
            "\n",
            "       [[0.7109375 ]]]), array([[[0.56005859]],\n",
            "\n",
            "       [[0.04644775]],\n",
            "\n",
            "       [[0.39355469]]]), array([[[0.17016602]],\n",
            "\n",
            "       [[0.06222534]],\n",
            "\n",
            "       [[0.76757812]]]), array([[[0.11694336]],\n",
            "\n",
            "       [[0.06402588]],\n",
            "\n",
            "       [[0.81884766]]]), array([[[0.08972168]],\n",
            "\n",
            "       [[0.11260986]],\n",
            "\n",
            "       [[0.79785156]]]), array([[[0.36376953]],\n",
            "\n",
            "       [[0.05770874]],\n",
            "\n",
            "       [[0.57861328]]]), array([[[0.13134766]],\n",
            "\n",
            "       [[0.07275391]],\n",
            "\n",
            "       [[0.79589844]]]), array([[[0.34814453]],\n",
            "\n",
            "       [[0.05114746]],\n",
            "\n",
            "       [[0.60058594]]]), array([[[0.43994141]],\n",
            "\n",
            "       [[0.05194092]],\n",
            "\n",
            "       [[0.50830078]]]), array([[[0.25634766]],\n",
            "\n",
            "       [[0.03622437]],\n",
            "\n",
            "       [[0.70751953]]]), array([[[0.19213867]],\n",
            "\n",
            "       [[0.05108643]],\n",
            "\n",
            "       [[0.75683594]]]), array([[[0.32006836]],\n",
            "\n",
            "       [[0.0609436 ]],\n",
            "\n",
            "       [[0.61914062]]]), array([[[0.01446533]],\n",
            "\n",
            "       [[0.0402832 ]],\n",
            "\n",
            "       [[0.9453125 ]]]), array([[[0.78710938]],\n",
            "\n",
            "       [[0.0196991 ]],\n",
            "\n",
            "       [[0.19311523]]]), array([[[0.03768921]],\n",
            "\n",
            "       [[0.07641602]],\n",
            "\n",
            "       [[0.88574219]]]), array([[[0.26049805]],\n",
            "\n",
            "       [[0.08862305]],\n",
            "\n",
            "       [[0.65087891]]]), array([[[0.10577393]],\n",
            "\n",
            "       [[0.03778076]],\n",
            "\n",
            "       [[0.85644531]]]), array([[[0.33862305]],\n",
            "\n",
            "       [[0.07629395]],\n",
            "\n",
            "       [[0.58496094]]]), array([[[0.07122803]],\n",
            "\n",
            "       [[0.06958008]],\n",
            "\n",
            "       [[0.859375  ]]]), array([[[0.10266113]],\n",
            "\n",
            "       [[0.05828857]],\n",
            "\n",
            "       [[0.83886719]]]), array([[[0.41430664]],\n",
            "\n",
            "       [[0.05752563]],\n",
            "\n",
            "       [[0.52832031]]]), array([[[0.1315918 ]],\n",
            "\n",
            "       [[0.05383301]],\n",
            "\n",
            "       [[0.81445312]]]), array([[[0.28930664]],\n",
            "\n",
            "       [[0.04040527]],\n",
            "\n",
            "       [[0.67041016]]]), array([[[0.04266357]],\n",
            "\n",
            "       [[0.1418457 ]],\n",
            "\n",
            "       [[0.81542969]]]), array([[[0.14404297]],\n",
            "\n",
            "       [[0.05505371]],\n",
            "\n",
            "       [[0.80078125]]]), array([[[0.18688965]],\n",
            "\n",
            "       [[0.05728149]],\n",
            "\n",
            "       [[0.75585938]]]), array([[[0.0927124 ]],\n",
            "\n",
            "       [[0.07489014]],\n",
            "\n",
            "       [[0.83251953]]]), array([[[0.02980042]],\n",
            "\n",
            "       [[0.08654785]],\n",
            "\n",
            "       [[0.88378906]]]), array([[[0.17443848]],\n",
            "\n",
            "       [[0.05471802]],\n",
            "\n",
            "       [[0.77099609]]]), array([[[0.0375061 ]],\n",
            "\n",
            "       [[0.09844971]],\n",
            "\n",
            "       [[0.86425781]]]), array([[[0.04620361]],\n",
            "\n",
            "       [[0.04971313]],\n",
            "\n",
            "       [[0.90429688]]]), array([[[0.12426758]],\n",
            "\n",
            "       [[0.08789062]],\n",
            "\n",
            "       [[0.78759766]]]), array([[[0.112854  ]],\n",
            "\n",
            "       [[0.08062744]],\n",
            "\n",
            "       [[0.80664062]]]), array([[[0.0869751 ]],\n",
            "\n",
            "       [[0.04852295]],\n",
            "\n",
            "       [[0.86474609]]]), array([[[0.27319336]],\n",
            "\n",
            "       [[0.09320068]],\n",
            "\n",
            "       [[0.63378906]]]), array([[[0.20080566]],\n",
            "\n",
            "       [[0.0725708 ]],\n",
            "\n",
            "       [[0.7265625 ]]]), array([[[0.17712402]],\n",
            "\n",
            "       [[0.04397583]],\n",
            "\n",
            "       [[0.77880859]]]), array([[[0.0657959 ]],\n",
            "\n",
            "       [[0.04333496]],\n",
            "\n",
            "       [[0.89111328]]]), array([[[0.42822266]],\n",
            "\n",
            "       [[0.03738403]],\n",
            "\n",
            "       [[0.53466797]]]), array([[[0.37719727]],\n",
            "\n",
            "       [[0.04055786]],\n",
            "\n",
            "       [[0.58203125]]]), array([[[0.07366943]],\n",
            "\n",
            "       [[0.07110596]],\n",
            "\n",
            "       [[0.85498047]]]), array([[[0.10351562]],\n",
            "\n",
            "       [[0.05236816]],\n",
            "\n",
            "       [[0.84423828]]]), array([[[0.59960938]],\n",
            "\n",
            "       [[0.01753235]],\n",
            "\n",
            "       [[0.38305664]]]), array([[[0.14794922]],\n",
            "\n",
            "       [[0.05569458]],\n",
            "\n",
            "       [[0.79638672]]]), array([[[0.24462891]],\n",
            "\n",
            "       [[0.05065918]],\n",
            "\n",
            "       [[0.70458984]]]), array([[[0.0501709 ]],\n",
            "\n",
            "       [[0.04821777]],\n",
            "\n",
            "       [[0.90185547]]]), array([[[0.2565918 ]],\n",
            "\n",
            "       [[0.05429077]],\n",
            "\n",
            "       [[0.68896484]]]), array([[[0.1776123 ]],\n",
            "\n",
            "       [[0.06256104]],\n",
            "\n",
            "       [[0.75976562]]]), array([[[0.27832031]],\n",
            "\n",
            "       [[0.04205322]],\n",
            "\n",
            "       [[0.6796875 ]]]), array([[[0.21496582]],\n",
            "\n",
            "       [[0.04748535]],\n",
            "\n",
            "       [[0.73779297]]]), array([[[0.24121094]],\n",
            "\n",
            "       [[0.05401611]],\n",
            "\n",
            "       [[0.70458984]]]), array([[[0.71337891]],\n",
            "\n",
            "       [[0.01722717]],\n",
            "\n",
            "       [[0.26953125]]]), array([[[0.22766113]],\n",
            "\n",
            "       [[0.07470703]],\n",
            "\n",
            "       [[0.69775391]]]), array([[[0.10339355]],\n",
            "\n",
            "       [[0.07556152]],\n",
            "\n",
            "       [[0.82128906]]]), array([[[0.36474609]],\n",
            "\n",
            "       [[0.06274414]],\n",
            "\n",
            "       [[0.57226562]]]), array([[[0.13183594]],\n",
            "\n",
            "       [[0.1385498 ]],\n",
            "\n",
            "       [[0.72949219]]]), array([[[0.07556152]],\n",
            "\n",
            "       [[0.04171753]],\n",
            "\n",
            "       [[0.8828125 ]]]), array([[[0.05682373]],\n",
            "\n",
            "       [[0.08984375]],\n",
            "\n",
            "       [[0.85351562]]]), array([[[0.73779297]],\n",
            "\n",
            "       [[0.01991272]],\n",
            "\n",
            "       [[0.24230957]]]), array([[[0.21960449]],\n",
            "\n",
            "       [[0.04458618]],\n",
            "\n",
            "       [[0.73583984]]]), array([[[0.11621094]],\n",
            "\n",
            "       [[0.10101318]],\n",
            "\n",
            "       [[0.78271484]]]), array([[[0.03167725]],\n",
            "\n",
            "       [[0.06744385]],\n",
            "\n",
            "       [[0.90087891]]]), array([[[0.28515625]],\n",
            "\n",
            "       [[0.06213379]],\n",
            "\n",
            "       [[0.65283203]]]), array([[[0.29589844]],\n",
            "\n",
            "       [[0.0569458 ]],\n",
            "\n",
            "       [[0.64697266]]]), array([[[0.081604  ]],\n",
            "\n",
            "       [[0.02801514]],\n",
            "\n",
            "       [[0.89013672]]]), array([[[0.46728516]],\n",
            "\n",
            "       [[0.02142334]],\n",
            "\n",
            "       [[0.51123047]]]), array([[[0.24975586]],\n",
            "\n",
            "       [[0.06646729]],\n",
            "\n",
            "       [[0.68359375]]]), array([[[0.0333252 ]],\n",
            "\n",
            "       [[0.04995728]],\n",
            "\n",
            "       [[0.91650391]]]), array([[[0.75292969]],\n",
            "\n",
            "       [[0.00913239]],\n",
            "\n",
            "       [[0.2376709 ]]]), array([[[0.20910645]],\n",
            "\n",
            "       [[0.05081177]],\n",
            "\n",
            "       [[0.74023438]]]), array([[[0.04663086]],\n",
            "\n",
            "       [[0.0519104 ]],\n",
            "\n",
            "       [[0.90136719]]]), array([[[0.22045898]],\n",
            "\n",
            "       [[0.07751465]],\n",
            "\n",
            "       [[0.70214844]]]), array([[[0.30175781]],\n",
            "\n",
            "       [[0.05255127]],\n",
            "\n",
            "       [[0.64550781]]]), array([[[0.65869141]],\n",
            "\n",
            "       [[0.02427673]],\n",
            "\n",
            "       [[0.31713867]]]), array([[[0.06210327]],\n",
            "\n",
            "       [[0.02890015]],\n",
            "\n",
            "       [[0.90917969]]]), array([[[0.21862793]],\n",
            "\n",
            "       [[0.04666138]],\n",
            "\n",
            "       [[0.73486328]]]), array([[[0.12243652]],\n",
            "\n",
            "       [[0.05111694]],\n",
            "\n",
            "       [[0.82666016]]]), array([[[0.42822266]],\n",
            "\n",
            "       [[0.06634521]],\n",
            "\n",
            "       [[0.50537109]]]), array([[[0.49414062]],\n",
            "\n",
            "       [[0.03570557]],\n",
            "\n",
            "       [[0.47021484]]]), array([[[0.10632324]],\n",
            "\n",
            "       [[0.05114746]],\n",
            "\n",
            "       [[0.84228516]]]), array([[[0.20446777]],\n",
            "\n",
            "       [[0.05877686]],\n",
            "\n",
            "       [[0.73681641]]]), array([[[0.05963135]],\n",
            "\n",
            "       [[0.03114319]],\n",
            "\n",
            "       [[0.90917969]]]), array([[[0.10021973]],\n",
            "\n",
            "       [[0.11517334]],\n",
            "\n",
            "       [[0.78466797]]]), array([[[0.11517334]],\n",
            "\n",
            "       [[0.04516602]],\n",
            "\n",
            "       [[0.83984375]]]), array([[[0.01823425]],\n",
            "\n",
            "       [[0.02941895]],\n",
            "\n",
            "       [[0.95214844]]]), array([[[0.17578125]],\n",
            "\n",
            "       [[0.03274536]],\n",
            "\n",
            "       [[0.79150391]]]), array([[[0.11108398]],\n",
            "\n",
            "       [[0.0519104 ]],\n",
            "\n",
            "       [[0.83691406]]]), array([[[0.04504395]],\n",
            "\n",
            "       [[0.04071045]],\n",
            "\n",
            "       [[0.9140625 ]]]), array([[[0.06768799]],\n",
            "\n",
            "       [[0.0881958 ]],\n",
            "\n",
            "       [[0.84423828]]]), array([[[0.23327637]],\n",
            "\n",
            "       [[0.08660889]],\n",
            "\n",
            "       [[0.68017578]]]), array([[[0.38378906]],\n",
            "\n",
            "       [[0.0574646 ]],\n",
            "\n",
            "       [[0.55859375]]]), array([[[0.02987671]],\n",
            "\n",
            "       [[0.10888672]],\n",
            "\n",
            "       [[0.86132812]]]), array([[[0.01733398]],\n",
            "\n",
            "       [[0.02038574]],\n",
            "\n",
            "       [[0.96240234]]]), array([[[0.09289551]],\n",
            "\n",
            "       [[0.09350586]],\n",
            "\n",
            "       [[0.81347656]]]), array([[[0.10406494]],\n",
            "\n",
            "       [[0.10760498]],\n",
            "\n",
            "       [[0.78808594]]]), array([[[0.13867188]],\n",
            "\n",
            "       [[0.08508301]],\n",
            "\n",
            "       [[0.77636719]]]), array([[[0.0239563 ]],\n",
            "\n",
            "       [[0.06488037]],\n",
            "\n",
            "       [[0.91113281]]]), array([[[0.05502319]],\n",
            "\n",
            "       [[0.05264282]],\n",
            "\n",
            "       [[0.89208984]]]), array([[[0.20983887]],\n",
            "\n",
            "       [[0.05038452]],\n",
            "\n",
            "       [[0.73974609]]]), array([[[0.16687012]],\n",
            "\n",
            "       [[0.04220581]],\n",
            "\n",
            "       [[0.79101562]]]), array([[[0.17883301]],\n",
            "\n",
            "       [[0.06228638]],\n",
            "\n",
            "       [[0.75878906]]]), array([[[0.02571106]],\n",
            "\n",
            "       [[0.0960083 ]],\n",
            "\n",
            "       [[0.87841797]]]), array([[[0.66503906]],\n",
            "\n",
            "       [[0.01928711]],\n",
            "\n",
            "       [[0.31567383]]]), array([[[0.91943359]],\n",
            "\n",
            "       [[0.00798798]],\n",
            "\n",
            "       [[0.0725708 ]]]), array([[[0.1015625 ]],\n",
            "\n",
            "       [[0.09301758]],\n",
            "\n",
            "       [[0.80566406]]]), array([[[0.61132812]],\n",
            "\n",
            "       [[0.04199219]],\n",
            "\n",
            "       [[0.34643555]]]), array([[[0.45776367]],\n",
            "\n",
            "       [[0.03497314]],\n",
            "\n",
            "       [[0.50732422]]]), array([[[0.09729004]],\n",
            "\n",
            "       [[0.01311493]],\n",
            "\n",
            "       [[0.88964844]]]), array([[[0.08721924]],\n",
            "\n",
            "       [[0.03851318]],\n",
            "\n",
            "       [[0.87402344]]]), array([[[0.27880859]],\n",
            "\n",
            "       [[0.0458374 ]],\n",
            "\n",
            "       [[0.67529297]]]), array([[[0.12060547]],\n",
            "\n",
            "       [[0.05767822]],\n",
            "\n",
            "       [[0.82177734]]]), array([[[0.17321777]],\n",
            "\n",
            "       [[0.06317139]],\n",
            "\n",
            "       [[0.76367188]]]), array([[[0.54833984]],\n",
            "\n",
            "       [[0.01950073]],\n",
            "\n",
            "       [[0.43212891]]]), array([[[0.11938477]],\n",
            "\n",
            "       [[0.08563232]],\n",
            "\n",
            "       [[0.79492188]]]), array([[[0.18457031]],\n",
            "\n",
            "       [[0.02563477]],\n",
            "\n",
            "       [[0.78955078]]]), array([[[0.32080078]],\n",
            "\n",
            "       [[0.0411377 ]],\n",
            "\n",
            "       [[0.63818359]]]), array([[[0.14953613]],\n",
            "\n",
            "       [[0.0435791 ]],\n",
            "\n",
            "       [[0.80712891]]]), array([[[0.12261963]],\n",
            "\n",
            "       [[0.08551025]],\n",
            "\n",
            "       [[0.79199219]]]), array([[[0.29174805]],\n",
            "\n",
            "       [[0.04779053]],\n",
            "\n",
            "       [[0.66015625]]]), array([[[0.14465332]],\n",
            "\n",
            "       [[0.06057739]],\n",
            "\n",
            "       [[0.79492188]]]), array([[[0.62988281]],\n",
            "\n",
            "       [[0.03387451]],\n",
            "\n",
            "       [[0.33642578]]]), array([[[0.14038086]],\n",
            "\n",
            "       [[0.02972412]],\n",
            "\n",
            "       [[0.83007812]]]), array([[[0.23742676]],\n",
            "\n",
            "       [[0.02485657]],\n",
            "\n",
            "       [[0.73779297]]]), array([[[0.28979492]],\n",
            "\n",
            "       [[0.03475952]],\n",
            "\n",
            "       [[0.67529297]]]), array([[[0.22106934]],\n",
            "\n",
            "       [[0.01568604]],\n",
            "\n",
            "       [[0.76318359]]]), array([[[0.11193848]],\n",
            "\n",
            "       [[0.05981445]],\n",
            "\n",
            "       [[0.828125  ]]]), array([[[0.74121094]],\n",
            "\n",
            "       [[0.04086304]],\n",
            "\n",
            "       [[0.21813965]]]), array([[[0.06359863]],\n",
            "\n",
            "       [[0.03964233]],\n",
            "\n",
            "       [[0.89697266]]]), array([[[0.19775391]],\n",
            "\n",
            "       [[0.03985596]],\n",
            "\n",
            "       [[0.76220703]]]), array([[[0.58105469]],\n",
            "\n",
            "       [[0.03384399]],\n",
            "\n",
            "       [[0.38525391]]]), array([[[0.2364502 ]],\n",
            "\n",
            "       [[0.05480957]],\n",
            "\n",
            "       [[0.70849609]]]), array([[[0.82421875]],\n",
            "\n",
            "       [[0.02076721]],\n",
            "\n",
            "       [[0.15515137]]]), array([[[0.01582336]],\n",
            "\n",
            "       [[0.02630615]],\n",
            "\n",
            "       [[0.95800781]]]), array([[[0.75927734]],\n",
            "\n",
            "       [[0.02575684]],\n",
            "\n",
            "       [[0.21484375]]]), array([[[0.06713867]],\n",
            "\n",
            "       [[0.0308075 ]],\n",
            "\n",
            "       [[0.90185547]]]), array([[[0.05392456]],\n",
            "\n",
            "       [[0.02812195]],\n",
            "\n",
            "       [[0.91796875]]]), array([[[0.56005859]],\n",
            "\n",
            "       [[0.03308105]],\n",
            "\n",
            "       [[0.40698242]]]), array([[[0.18359375]],\n",
            "\n",
            "       [[0.06915283]],\n",
            "\n",
            "       [[0.74707031]]]), array([[[0.37963867]],\n",
            "\n",
            "       [[0.03170776]],\n",
            "\n",
            "       [[0.58837891]]]), array([[[0.09588623]],\n",
            "\n",
            "       [[0.04833984]],\n",
            "\n",
            "       [[0.85595703]]]), array([[[0.03108215]],\n",
            "\n",
            "       [[0.01678467]],\n",
            "\n",
            "       [[0.95214844]]]), array([[[0.32568359]],\n",
            "\n",
            "       [[0.02589417]],\n",
            "\n",
            "       [[0.6484375 ]]]), array([[[0.53369141]],\n",
            "\n",
            "       [[0.02471924]],\n",
            "\n",
            "       [[0.44165039]]]), array([[[0.02624512]],\n",
            "\n",
            "       [[0.03683472]],\n",
            "\n",
            "       [[0.93701172]]]), array([[[0.28320312]],\n",
            "\n",
            "       [[0.04534912]],\n",
            "\n",
            "       [[0.67138672]]]), array([[[0.20605469]],\n",
            "\n",
            "       [[0.04248047]],\n",
            "\n",
            "       [[0.75146484]]]), array([[[0.36328125]],\n",
            "\n",
            "       [[0.03527832]],\n",
            "\n",
            "       [[0.6015625 ]]]), array([[[0.32617188]],\n",
            "\n",
            "       [[0.08270264]],\n",
            "\n",
            "       [[0.59130859]]]), array([[[0.10253906]],\n",
            "\n",
            "       [[0.04846191]],\n",
            "\n",
            "       [[0.84912109]]]), array([[[0.10748291]],\n",
            "\n",
            "       [[0.08056641]],\n",
            "\n",
            "       [[0.81201172]]]), array([[[0.38427734]],\n",
            "\n",
            "       [[0.13781738]],\n",
            "\n",
            "       [[0.4777832 ]]]), array([[[0.06549072]],\n",
            "\n",
            "       [[0.03677368]],\n",
            "\n",
            "       [[0.89794922]]]), array([[[0.53662109]],\n",
            "\n",
            "       [[0.03985596]],\n",
            "\n",
            "       [[0.42358398]]]), array([[[0.16772461]],\n",
            "\n",
            "       [[0.05340576]],\n",
            "\n",
            "       [[0.77880859]]]), array([[[0.11590576]],\n",
            "\n",
            "       [[0.06665039]],\n",
            "\n",
            "       [[0.81738281]]]), array([[[0.14233398]],\n",
            "\n",
            "       [[0.08929443]],\n",
            "\n",
            "       [[0.76855469]]]), array([[[0.25219727]],\n",
            "\n",
            "       [[0.02989197]],\n",
            "\n",
            "       [[0.71826172]]]), array([[[0.36645508]],\n",
            "\n",
            "       [[0.04394531]],\n",
            "\n",
            "       [[0.58935547]]]), array([[[0.30493164]],\n",
            "\n",
            "       [[0.03869629]],\n",
            "\n",
            "       [[0.65625   ]]]), array([[[0.54492188]],\n",
            "\n",
            "       [[0.03518677]],\n",
            "\n",
            "       [[0.41992188]]]), array([[[0.54882812]],\n",
            "\n",
            "       [[0.03930664]],\n",
            "\n",
            "       [[0.41186523]]]), array([[[0.09545898]],\n",
            "\n",
            "       [[0.06671143]],\n",
            "\n",
            "       [[0.83789062]]]), array([[[0.20898438]],\n",
            "\n",
            "       [[0.05319214]],\n",
            "\n",
            "       [[0.73779297]]]), array([[[0.22644043]],\n",
            "\n",
            "       [[0.09655762]],\n",
            "\n",
            "       [[0.67675781]]]), array([[[0.08709717]],\n",
            "\n",
            "       [[0.02880859]],\n",
            "\n",
            "       [[0.88427734]]]), array([[[0.38305664]],\n",
            "\n",
            "       [[0.12487793]],\n",
            "\n",
            "       [[0.49194336]]]), array([[[0.63916016]],\n",
            "\n",
            "       [[0.02015686]],\n",
            "\n",
            "       [[0.34057617]]]), array([[[0.61083984]],\n",
            "\n",
            "       [[0.02172852]],\n",
            "\n",
            "       [[0.3671875 ]]]), array([[[0.52197266]],\n",
            "\n",
            "       [[0.03994751]],\n",
            "\n",
            "       [[0.43798828]]]), array([[[0.29394531]],\n",
            "\n",
            "       [[0.04959106]],\n",
            "\n",
            "       [[0.65625   ]]]), array([[[0.66650391]],\n",
            "\n",
            "       [[0.02111816]],\n",
            "\n",
            "       [[0.31225586]]]), array([[[0.34277344]],\n",
            "\n",
            "       [[0.05197144]],\n",
            "\n",
            "       [[0.60498047]]]), array([[[0.87402344]],\n",
            "\n",
            "       [[0.01609802]],\n",
            "\n",
            "       [[0.10968018]]]), array([[[0.17907715]],\n",
            "\n",
            "       [[0.03152466]],\n",
            "\n",
            "       [[0.78955078]]]), array([[[0.21496582]],\n",
            "\n",
            "       [[0.0703125 ]],\n",
            "\n",
            "       [[0.71484375]]]), array([[[0.3762207 ]],\n",
            "\n",
            "       [[0.0296936 ]],\n",
            "\n",
            "       [[0.59423828]]]), array([[[0.64160156]],\n",
            "\n",
            "       [[0.03659058]],\n",
            "\n",
            "       [[0.32177734]]]), array([[[0.12243652]],\n",
            "\n",
            "       [[0.03738403]],\n",
            "\n",
            "       [[0.84033203]]]), array([[[0.16662598]],\n",
            "\n",
            "       [[0.10699463]],\n",
            "\n",
            "       [[0.7265625 ]]]), array([[[0.76367188]],\n",
            "\n",
            "       [[0.02105713]],\n",
            "\n",
            "       [[0.2154541 ]]]), array([[[0.45849609]],\n",
            "\n",
            "       [[0.04974365]],\n",
            "\n",
            "       [[0.49169922]]]), array([[[0.1472168 ]],\n",
            "\n",
            "       [[0.04721069]],\n",
            "\n",
            "       [[0.80566406]]]), array([[[0.20056152]],\n",
            "\n",
            "       [[0.06396484]],\n",
            "\n",
            "       [[0.73535156]]]), array([[[0.42236328]],\n",
            "\n",
            "       [[0.03793335]],\n",
            "\n",
            "       [[0.53955078]]]), array([[[0.27734375]],\n",
            "\n",
            "       [[0.05627441]],\n",
            "\n",
            "       [[0.66650391]]]), array([[[0.06051636]],\n",
            "\n",
            "       [[0.06158447]],\n",
            "\n",
            "       [[0.87792969]]]), array([[[0.45629883]],\n",
            "\n",
            "       [[0.03701782]],\n",
            "\n",
            "       [[0.50683594]]]), array([[[0.38916016]],\n",
            "\n",
            "       [[0.0418396 ]],\n",
            "\n",
            "       [[0.56933594]]]), array([[[0.10205078]],\n",
            "\n",
            "       [[0.03091431]],\n",
            "\n",
            "       [[0.8671875 ]]]), array([[[0.82128906]],\n",
            "\n",
            "       [[0.00888824]],\n",
            "\n",
            "       [[0.16967773]]]), array([[[0.11474609]],\n",
            "\n",
            "       [[0.03665161]],\n",
            "\n",
            "       [[0.84863281]]]), array([[[0.56933594]],\n",
            "\n",
            "       [[0.01802063]],\n",
            "\n",
            "       [[0.41259766]]]), array([[[0.09686279]],\n",
            "\n",
            "       [[0.08349609]],\n",
            "\n",
            "       [[0.81982422]]]), array([[[0.38085938]],\n",
            "\n",
            "       [[0.05160522]],\n",
            "\n",
            "       [[0.56738281]]]), array([[[0.19921875]],\n",
            "\n",
            "       [[0.05032349]],\n",
            "\n",
            "       [[0.75048828]]]), array([[[0.30615234]],\n",
            "\n",
            "       [[0.03604126]],\n",
            "\n",
            "       [[0.65771484]]]), array([[[0.35864258]],\n",
            "\n",
            "       [[0.03579712]],\n",
            "\n",
            "       [[0.60546875]]]), array([[[0.17883301]],\n",
            "\n",
            "       [[0.06262207]],\n",
            "\n",
            "       [[0.75830078]]]), array([[[0.21447754]],\n",
            "\n",
            "       [[0.04458618]],\n",
            "\n",
            "       [[0.74121094]]]), array([[[0.57421875]],\n",
            "\n",
            "       [[0.03030396]],\n",
            "\n",
            "       [[0.39575195]]]), array([[[0.00787354]],\n",
            "\n",
            "       [[0.04394531]],\n",
            "\n",
            "       [[0.94824219]]]), array([[[0.03088379]],\n",
            "\n",
            "       [[0.01570129]],\n",
            "\n",
            "       [[0.95361328]]]), array([[[0.50634766]],\n",
            "\n",
            "       [[0.03353882]],\n",
            "\n",
            "       [[0.46020508]]]), array([[[0.09490967]],\n",
            "\n",
            "       [[0.02650452]],\n",
            "\n",
            "       [[0.87841797]]]), array([[[0.49804688]],\n",
            "\n",
            "       [[0.01863098]],\n",
            "\n",
            "       [[0.48339844]]]), array([[[0.71679688]],\n",
            "\n",
            "       [[0.03170776]],\n",
            "\n",
            "       [[0.25146484]]]), array([[[0.76074219]],\n",
            "\n",
            "       [[0.02418518]],\n",
            "\n",
            "       [[0.21520996]]]), array([[[0.23120117]],\n",
            "\n",
            "       [[0.05435181]],\n",
            "\n",
            "       [[0.71435547]]]), array([[[0.21850586]],\n",
            "\n",
            "       [[0.04605103]],\n",
            "\n",
            "       [[0.73535156]]]), array([[[0.43066406]],\n",
            "\n",
            "       [[0.0319519 ]],\n",
            "\n",
            "       [[0.53759766]]]), array([[[0.35327148]],\n",
            "\n",
            "       [[0.05337524]],\n",
            "\n",
            "       [[0.59326172]]]), array([[[0.68554688]],\n",
            "\n",
            "       [[0.02922058]],\n",
            "\n",
            "       [[0.28515625]]]), array([[[0.44677734]],\n",
            "\n",
            "       [[0.10772705]],\n",
            "\n",
            "       [[0.4453125 ]]]), array([[[0.73828125]],\n",
            "\n",
            "       [[0.02113342]],\n",
            "\n",
            "       [[0.24047852]]]), array([[[0.58935547]],\n",
            "\n",
            "       [[0.02470398]],\n",
            "\n",
            "       [[0.38598633]]]), array([[[0.49365234]],\n",
            "\n",
            "       [[0.05853271]],\n",
            "\n",
            "       [[0.44775391]]]), array([[[0.67529297]],\n",
            "\n",
            "       [[0.02914429]],\n",
            "\n",
            "       [[0.2956543 ]]]), array([[[0.20092773]],\n",
            "\n",
            "       [[0.05203247]],\n",
            "\n",
            "       [[0.74707031]]]), array([[[0.77978516]],\n",
            "\n",
            "       [[0.01676941]],\n",
            "\n",
            "       [[0.20324707]]]), array([[[0.49682617]],\n",
            "\n",
            "       [[0.01856995]],\n",
            "\n",
            "       [[0.48461914]]]), array([[[0.0206604 ]],\n",
            "\n",
            "       [[0.02374268]],\n",
            "\n",
            "       [[0.95556641]]]), array([[[0.07312012]],\n",
            "\n",
            "       [[0.03445435]],\n",
            "\n",
            "       [[0.89257812]]]), array([[[0.14111328]],\n",
            "\n",
            "       [[0.16101074]],\n",
            "\n",
            "       [[0.69775391]]]), array([[[0.75097656]],\n",
            "\n",
            "       [[0.02223206]],\n",
            "\n",
            "       [[0.22692871]]]), array([[[0.89550781]],\n",
            "\n",
            "       [[0.01177216]],\n",
            "\n",
            "       [[0.09259033]]]), array([[[0.04940796]],\n",
            "\n",
            "       [[0.04165649]],\n",
            "\n",
            "       [[0.90917969]]]), array([[[0.3659668 ]],\n",
            "\n",
            "       [[0.03271484]],\n",
            "\n",
            "       [[0.6015625 ]]]), array([[[0.66259766]],\n",
            "\n",
            "       [[0.03466797]],\n",
            "\n",
            "       [[0.30297852]]]), array([[[0.17456055]],\n",
            "\n",
            "       [[0.17163086]],\n",
            "\n",
            "       [[0.65380859]]]), array([[[0.26904297]],\n",
            "\n",
            "       [[0.07501221]],\n",
            "\n",
            "       [[0.65576172]]]), array([[[0.33544922]],\n",
            "\n",
            "       [[0.03555298]],\n",
            "\n",
            "       [[0.62890625]]]), array([[[0.4128418 ]],\n",
            "\n",
            "       [[0.02636719]],\n",
            "\n",
            "       [[0.56054688]]]), array([[[0.20275879]],\n",
            "\n",
            "       [[0.05807495]],\n",
            "\n",
            "       [[0.73925781]]]), array([[[0.60546875]],\n",
            "\n",
            "       [[0.03219604]],\n",
            "\n",
            "       [[0.36206055]]]), array([[[0.24829102]],\n",
            "\n",
            "       [[0.04434204]],\n",
            "\n",
            "       [[0.70751953]]]), array([[[0.74951172]],\n",
            "\n",
            "       [[0.01617432]],\n",
            "\n",
            "       [[0.23413086]]]), array([[[0.64746094]],\n",
            "\n",
            "       [[0.0276947 ]],\n",
            "\n",
            "       [[0.32495117]]]), array([[[0.45922852]],\n",
            "\n",
            "       [[0.04229736]],\n",
            "\n",
            "       [[0.49853516]]]), array([[[0.3293457 ]],\n",
            "\n",
            "       [[0.03305054]],\n",
            "\n",
            "       [[0.63769531]]]), array([[[0.22436523]],\n",
            "\n",
            "       [[0.04968262]],\n",
            "\n",
            "       [[0.72607422]]]), array([[[0.47070312]],\n",
            "\n",
            "       [[0.05084229]],\n",
            "\n",
            "       [[0.47851562]]]), array([[[0.52001953]],\n",
            "\n",
            "       [[0.03259277]],\n",
            "\n",
            "       [[0.44750977]]]), array([[[0.19042969]],\n",
            "\n",
            "       [[0.05331421]],\n",
            "\n",
            "       [[0.75634766]]]), array([[[0.03460693]],\n",
            "\n",
            "       [[0.02848816]],\n",
            "\n",
            "       [[0.93701172]]]), array([[[0.17956543]],\n",
            "\n",
            "       [[0.04772949]],\n",
            "\n",
            "       [[0.77246094]]]), array([[[0.01620483]],\n",
            "\n",
            "       [[0.09002686]],\n",
            "\n",
            "       [[0.89355469]]]), array([[[0.22680664]],\n",
            "\n",
            "       [[0.05944824]],\n",
            "\n",
            "       [[0.71386719]]]), array([[[0.21459961]],\n",
            "\n",
            "       [[0.01585388]],\n",
            "\n",
            "       [[0.76953125]]]), array([[[0.40307617]],\n",
            "\n",
            "       [[0.03359985]],\n",
            "\n",
            "       [[0.56347656]]]), array([[[0.32739258]],\n",
            "\n",
            "       [[0.0625    ]],\n",
            "\n",
            "       [[0.60986328]]]), array([[[0.01410675]],\n",
            "\n",
            "       [[0.02088928]],\n",
            "\n",
            "       [[0.96484375]]]), array([[[0.20349121]],\n",
            "\n",
            "       [[0.04067993]],\n",
            "\n",
            "       [[0.75585938]]]), array([[[0.14465332]],\n",
            "\n",
            "       [[0.08752441]],\n",
            "\n",
            "       [[0.76757812]]]), array([[[0.07354736]],\n",
            "\n",
            "       [[0.06365967]],\n",
            "\n",
            "       [[0.86279297]]]), array([[[0.51757812]],\n",
            "\n",
            "       [[0.0357666 ]],\n",
            "\n",
            "       [[0.4465332 ]]]), array([[[0.07330322]],\n",
            "\n",
            "       [[0.09375   ]],\n",
            "\n",
            "       [[0.83300781]]]), array([[[0.20251465]],\n",
            "\n",
            "       [[0.07745361]],\n",
            "\n",
            "       [[0.72021484]]]), array([[[0.83349609]],\n",
            "\n",
            "       [[0.01109314]],\n",
            "\n",
            "       [[0.15515137]]]), array([[[0.04016113]],\n",
            "\n",
            "       [[0.07110596]],\n",
            "\n",
            "       [[0.88867188]]]), array([[[0.20471191]],\n",
            "\n",
            "       [[0.01686096]],\n",
            "\n",
            "       [[0.77832031]]]), array([[[0.30932617]],\n",
            "\n",
            "       [[0.04870605]],\n",
            "\n",
            "       [[0.64208984]]]), array([[[0.06170654]],\n",
            "\n",
            "       [[0.03945923]],\n",
            "\n",
            "       [[0.89892578]]]), array([[[0.2980957 ]],\n",
            "\n",
            "       [[0.14453125]],\n",
            "\n",
            "       [[0.55712891]]]), array([[[0.36914062]],\n",
            "\n",
            "       [[0.06112671]],\n",
            "\n",
            "       [[0.56982422]]]), array([[[0.22558594]],\n",
            "\n",
            "       [[0.03723145]],\n",
            "\n",
            "       [[0.73730469]]]), array([[[0.81396484]],\n",
            "\n",
            "       [[0.02778625]],\n",
            "\n",
            "       [[0.15844727]]]), array([[[0.5234375 ]],\n",
            "\n",
            "       [[0.03192139]],\n",
            "\n",
            "       [[0.44482422]]]), array([[[0.45336914]],\n",
            "\n",
            "       [[0.02110291]],\n",
            "\n",
            "       [[0.52539062]]]), array([[[0.25634766]],\n",
            "\n",
            "       [[0.05950928]],\n",
            "\n",
            "       [[0.68408203]]]), array([[[0.31982422]],\n",
            "\n",
            "       [[0.04879761]],\n",
            "\n",
            "       [[0.63134766]]]), array([[[0.20715332]],\n",
            "\n",
            "       [[0.07446289]],\n",
            "\n",
            "       [[0.71826172]]]), array([[[0.04727173]],\n",
            "\n",
            "       [[0.06341553]],\n",
            "\n",
            "       [[0.88916016]]]), array([[[0.12475586]],\n",
            "\n",
            "       [[0.05187988]],\n",
            "\n",
            "       [[0.82324219]]]), array([[[0.22717285]],\n",
            "\n",
            "       [[0.05941772]],\n",
            "\n",
            "       [[0.71337891]]]), array([[[0.79101562]],\n",
            "\n",
            "       [[0.01275635]],\n",
            "\n",
            "       [[0.19641113]]]), array([[[0.20739746]],\n",
            "\n",
            "       [[0.04156494]],\n",
            "\n",
            "       [[0.75097656]]]), array([[[0.0395813 ]],\n",
            "\n",
            "       [[0.03915405]],\n",
            "\n",
            "       [[0.92138672]]]), array([[[0.29833984]],\n",
            "\n",
            "       [[0.06069946]],\n",
            "\n",
            "       [[0.64111328]]]), array([[[0.43041992]],\n",
            "\n",
            "       [[0.06057739]],\n",
            "\n",
            "       [[0.50878906]]]), array([[[0.17663574]],\n",
            "\n",
            "       [[0.03945923]],\n",
            "\n",
            "       [[0.78417969]]]), array([[[0.03689575]],\n",
            "\n",
            "       [[0.05697632]],\n",
            "\n",
            "       [[0.90625   ]]]), array([[[0.06549072]],\n",
            "\n",
            "       [[0.01982117]],\n",
            "\n",
            "       [[0.91455078]]]), array([[[0.17224121]],\n",
            "\n",
            "       [[0.07263184]],\n",
            "\n",
            "       [[0.75488281]]]), array([[[0.18908691]],\n",
            "\n",
            "       [[0.05258179]],\n",
            "\n",
            "       [[0.75830078]]]), array([[[0.0461731 ]],\n",
            "\n",
            "       [[0.03543091]],\n",
            "\n",
            "       [[0.91845703]]]), array([[[0.4777832 ]],\n",
            "\n",
            "       [[0.05566406]],\n",
            "\n",
            "       [[0.46655273]]]), array([[[0.59082031]],\n",
            "\n",
            "       [[0.02799988]],\n",
            "\n",
            "       [[0.38134766]]]), array([[[0.03320312]],\n",
            "\n",
            "       [[0.03994751]],\n",
            "\n",
            "       [[0.92675781]]]), array([[[0.25634766]],\n",
            "\n",
            "       [[0.05395508]],\n",
            "\n",
            "       [[0.68994141]]]), array([[[0.33081055]],\n",
            "\n",
            "       [[0.03939819]],\n",
            "\n",
            "       [[0.62988281]]]), array([[[0.8515625 ]],\n",
            "\n",
            "       [[0.01461029]],\n",
            "\n",
            "       [[0.13366699]]]), array([[[0.16308594]],\n",
            "\n",
            "       [[0.03875732]],\n",
            "\n",
            "       [[0.79833984]]]), array([[[0.14782715]],\n",
            "\n",
            "       [[0.02067566]],\n",
            "\n",
            "       [[0.83154297]]]), array([[[0.08551025]],\n",
            "\n",
            "       [[0.06933594]],\n",
            "\n",
            "       [[0.84521484]]]), array([[[0.19274902]],\n",
            "\n",
            "       [[0.06573486]],\n",
            "\n",
            "       [[0.74169922]]]), array([[[0.52880859]],\n",
            "\n",
            "       [[0.06158447]],\n",
            "\n",
            "       [[0.40966797]]]), array([[[0.05148315]],\n",
            "\n",
            "       [[0.03120422]],\n",
            "\n",
            "       [[0.91748047]]]), array([[[0.05453491]],\n",
            "\n",
            "       [[0.0451355 ]],\n",
            "\n",
            "       [[0.90039062]]]), array([[[0.23303223]],\n",
            "\n",
            "       [[0.02490234]],\n",
            "\n",
            "       [[0.7421875 ]]]), array([[[0.42993164]],\n",
            "\n",
            "       [[0.03086853]],\n",
            "\n",
            "       [[0.5390625 ]]]), array([[[0.40405273]],\n",
            "\n",
            "       [[0.05978394]],\n",
            "\n",
            "       [[0.53613281]]]), array([[[0.15124512]],\n",
            "\n",
            "       [[0.04290771]],\n",
            "\n",
            "       [[0.80566406]]]), array([[[0.01010132]],\n",
            "\n",
            "       [[0.13793945]],\n",
            "\n",
            "       [[0.85205078]]]), array([[[0.09588623]],\n",
            "\n",
            "       [[0.03833008]],\n",
            "\n",
            "       [[0.86572266]]]), array([[[0.30566406]],\n",
            "\n",
            "       [[0.037323  ]],\n",
            "\n",
            "       [[0.65673828]]]), array([[[0.23327637]],\n",
            "\n",
            "       [[0.06793213]],\n",
            "\n",
            "       [[0.69873047]]]), array([[[0.64257812]],\n",
            "\n",
            "       [[0.02262878]],\n",
            "\n",
            "       [[0.3347168 ]]]), array([[[0.65625   ]],\n",
            "\n",
            "       [[0.02532959]],\n",
            "\n",
            "       [[0.31835938]]]), array([[[0.06329346]],\n",
            "\n",
            "       [[0.04785156]],\n",
            "\n",
            "       [[0.88867188]]]), array([[[0.84277344]],\n",
            "\n",
            "       [[0.01091003]],\n",
            "\n",
            "       [[0.14648438]]]), array([[[0.26733398]],\n",
            "\n",
            "       [[0.05236816]],\n",
            "\n",
            "       [[0.68017578]]]), array([[[0.16015625]],\n",
            "\n",
            "       [[0.04589844]],\n",
            "\n",
            "       [[0.79394531]]]), array([[[0.34545898]],\n",
            "\n",
            "       [[0.03253174]],\n",
            "\n",
            "       [[0.62207031]]]), array([[[0.05664062]],\n",
            "\n",
            "       [[0.05136108]],\n",
            "\n",
            "       [[0.89208984]]]), array([[[0.16149902]],\n",
            "\n",
            "       [[0.05700684]],\n",
            "\n",
            "       [[0.78125   ]]]), array([[[0.10705566]],\n",
            "\n",
            "       [[0.03463745]],\n",
            "\n",
            "       [[0.85839844]]]), array([[[0.37255859]],\n",
            "\n",
            "       [[0.04144287]],\n",
            "\n",
            "       [[0.5859375 ]]]), array([[[0.36938477]],\n",
            "\n",
            "       [[0.04083252]],\n",
            "\n",
            "       [[0.58984375]]]), array([[[0.25512695]],\n",
            "\n",
            "       [[0.06262207]],\n",
            "\n",
            "       [[0.68212891]]]), array([[[0.18774414]],\n",
            "\n",
            "       [[0.07055664]],\n",
            "\n",
            "       [[0.74169922]]]), array([[[0.28027344]],\n",
            "\n",
            "       [[0.07818604]],\n",
            "\n",
            "       [[0.64160156]]]), array([[[0.39428711]],\n",
            "\n",
            "       [[0.05184937]],\n",
            "\n",
            "       [[0.55371094]]]), array([[[0.46264648]],\n",
            "\n",
            "       [[0.06341553]],\n",
            "\n",
            "       [[0.47387695]]]), array([[[0.13415527]],\n",
            "\n",
            "       [[0.09301758]],\n",
            "\n",
            "       [[0.77294922]]]), array([[[0.31176758]],\n",
            "\n",
            "       [[0.03955078]],\n",
            "\n",
            "       [[0.6484375 ]]]), array([[[0.23486328]],\n",
            "\n",
            "       [[0.0309906 ]],\n",
            "\n",
            "       [[0.734375  ]]]), array([[[0.40136719]],\n",
            "\n",
            "       [[0.05477905]],\n",
            "\n",
            "       [[0.54394531]]]), array([[[0.08197021]],\n",
            "\n",
            "       [[0.02766418]],\n",
            "\n",
            "       [[0.89013672]]]), array([[[0.01531219]],\n",
            "\n",
            "       [[0.04006958]],\n",
            "\n",
            "       [[0.94482422]]]), array([[[0.26293945]],\n",
            "\n",
            "       [[0.08776855]],\n",
            "\n",
            "       [[0.64941406]]]), array([[[0.47216797]],\n",
            "\n",
            "       [[0.03689575]],\n",
            "\n",
            "       [[0.49072266]]]), array([[[0.19262695]],\n",
            "\n",
            "       [[0.07373047]],\n",
            "\n",
            "       [[0.73339844]]]), array([[[0.06811523]],\n",
            "\n",
            "       [[0.02178955]],\n",
            "\n",
            "       [[0.91015625]]]), array([[[0.17565918]],\n",
            "\n",
            "       [[0.04318237]],\n",
            "\n",
            "       [[0.78125   ]]]), array([[[0.05551147]],\n",
            "\n",
            "       [[0.01974487]],\n",
            "\n",
            "       [[0.92480469]]]), array([[[0.02752686]],\n",
            "\n",
            "       [[0.09851074]],\n",
            "\n",
            "       [[0.87402344]]]), array([[[0.03555298]],\n",
            "\n",
            "       [[0.05740356]],\n",
            "\n",
            "       [[0.90722656]]]), array([[[0.2286377 ]],\n",
            "\n",
            "       [[0.03369141]],\n",
            "\n",
            "       [[0.73779297]]]), array([[[0.10968018]],\n",
            "\n",
            "       [[0.03494263]],\n",
            "\n",
            "       [[0.85546875]]]), array([[[0.16967773]],\n",
            "\n",
            "       [[0.13549805]],\n",
            "\n",
            "       [[0.69482422]]]), array([[[0.60253906]],\n",
            "\n",
            "       [[0.02471924]],\n",
            "\n",
            "       [[0.37280273]]]), array([[[0.57763672]],\n",
            "\n",
            "       [[0.03234863]],\n",
            "\n",
            "       [[0.39013672]]]), array([[[0.63525391]],\n",
            "\n",
            "       [[0.02836609]],\n",
            "\n",
            "       [[0.33666992]]]), array([[[0.69580078]],\n",
            "\n",
            "       [[0.03344727]],\n",
            "\n",
            "       [[0.27075195]]]), array([[[0.0670166 ]],\n",
            "\n",
            "       [[0.04034424]],\n",
            "\n",
            "       [[0.89257812]]]), array([[[0.03268433]],\n",
            "\n",
            "       [[0.06445312]],\n",
            "\n",
            "       [[0.90283203]]]), array([[[0.43505859]],\n",
            "\n",
            "       [[0.0491333 ]],\n",
            "\n",
            "       [[0.515625  ]]]), array([[[0.07745361]],\n",
            "\n",
            "       [[0.052948  ]],\n",
            "\n",
            "       [[0.86962891]]]), array([[[0.26220703]],\n",
            "\n",
            "       [[0.06347656]],\n",
            "\n",
            "       [[0.67431641]]]), array([[[0.38720703]],\n",
            "\n",
            "       [[0.01568604]],\n",
            "\n",
            "       [[0.59716797]]]), array([[[0.69677734]],\n",
            "\n",
            "       [[0.02456665]],\n",
            "\n",
            "       [[0.27880859]]]), array([[[0.02818298]],\n",
            "\n",
            "       [[0.05044556]],\n",
            "\n",
            "       [[0.92138672]]]), array([[[0.04403687]],\n",
            "\n",
            "       [[0.01312256]],\n",
            "\n",
            "       [[0.94287109]]]), array([[[0.17443848]],\n",
            "\n",
            "       [[0.03552246]],\n",
            "\n",
            "       [[0.79003906]]]), array([[[0.32006836]],\n",
            "\n",
            "       [[0.03817749]],\n",
            "\n",
            "       [[0.64160156]]]), array([[[0.12194824]],\n",
            "\n",
            "       [[0.02003479]],\n",
            "\n",
            "       [[0.85791016]]]), array([[[0.07141113]],\n",
            "\n",
            "       [[0.07324219]],\n",
            "\n",
            "       [[0.85546875]]]), array([[[0.53662109]],\n",
            "\n",
            "       [[0.04074097]],\n",
            "\n",
            "       [[0.42260742]]]), array([[[0.16040039]],\n",
            "\n",
            "       [[0.04718018]],\n",
            "\n",
            "       [[0.79248047]]]), array([[[0.09136963]],\n",
            "\n",
            "       [[0.06021118]],\n",
            "\n",
            "       [[0.84863281]]]), array([[[0.28393555]],\n",
            "\n",
            "       [[0.06100464]],\n",
            "\n",
            "       [[0.65478516]]]), array([[[0.11627197]],\n",
            "\n",
            "       [[0.04873657]],\n",
            "\n",
            "       [[0.83496094]]]), array([[[0.20532227]],\n",
            "\n",
            "       [[0.03497314]],\n",
            "\n",
            "       [[0.75976562]]]), array([[[0.12792969]],\n",
            "\n",
            "       [[0.01870728]],\n",
            "\n",
            "       [[0.85351562]]]), array([[[0.18835449]],\n",
            "\n",
            "       [[0.04815674]],\n",
            "\n",
            "       [[0.76367188]]]), array([[[0.51660156]],\n",
            "\n",
            "       [[0.03686523]],\n",
            "\n",
            "       [[0.4465332 ]]]), array([[[0.1730957 ]],\n",
            "\n",
            "       [[0.09082031]],\n",
            "\n",
            "       [[0.73632812]]]), array([[[0.43066406]],\n",
            "\n",
            "       [[0.02049255]],\n",
            "\n",
            "       [[0.54882812]]]), array([[[0.10137939]],\n",
            "\n",
            "       [[0.09002686]],\n",
            "\n",
            "       [[0.80859375]]]), array([[[0.39672852]],\n",
            "\n",
            "       [[0.05523682]],\n",
            "\n",
            "       [[0.54833984]]]), array([[[0.57373047]],\n",
            "\n",
            "       [[0.16638184]],\n",
            "\n",
            "       [[0.25976562]]]), array([[[0.63574219]],\n",
            "\n",
            "       [[0.03616333]],\n",
            "\n",
            "       [[0.32788086]]]), array([[[0.66845703]],\n",
            "\n",
            "       [[0.03442383]],\n",
            "\n",
            "       [[0.29711914]]]), array([[[0.54785156]],\n",
            "\n",
            "       [[0.02339172]],\n",
            "\n",
            "       [[0.42895508]]]), array([[[0.29833984]],\n",
            "\n",
            "       [[0.04345703]],\n",
            "\n",
            "       [[0.65820312]]]), array([[[0.04321289]],\n",
            "\n",
            "       [[0.02438354]],\n",
            "\n",
            "       [[0.93261719]]]), array([[[0.01098633]],\n",
            "\n",
            "       [[0.02694702]],\n",
            "\n",
            "       [[0.96191406]]]), array([[[0.86083984]],\n",
            "\n",
            "       [[0.00953674]],\n",
            "\n",
            "       [[0.12976074]]]), array([[[0.12634277]],\n",
            "\n",
            "       [[0.02357483]],\n",
            "\n",
            "       [[0.85009766]]]), array([[[0.73681641]],\n",
            "\n",
            "       [[0.0249176 ]],\n",
            "\n",
            "       [[0.23815918]]]), array([[[0.19262695]],\n",
            "\n",
            "       [[0.05593872]],\n",
            "\n",
            "       [[0.75146484]]]), array([[[0.10009766]],\n",
            "\n",
            "       [[0.02055359]],\n",
            "\n",
            "       [[0.87939453]]]), array([[[0.17150879]],\n",
            "\n",
            "       [[0.03018188]],\n",
            "\n",
            "       [[0.79833984]]]), array([[[0.0552063 ]],\n",
            "\n",
            "       [[0.0970459 ]],\n",
            "\n",
            "       [[0.84765625]]]), array([[[0.08569336]],\n",
            "\n",
            "       [[0.05444336]],\n",
            "\n",
            "       [[0.85986328]]]), array([[[0.19445801]],\n",
            "\n",
            "       [[0.04681396]],\n",
            "\n",
            "       [[0.75878906]]]), array([[[0.03192139]],\n",
            "\n",
            "       [[0.03057861]],\n",
            "\n",
            "       [[0.9375    ]]]), array([[[0.50537109]],\n",
            "\n",
            "       [[0.052948  ]],\n",
            "\n",
            "       [[0.44189453]]]), array([[[0.34863281]],\n",
            "\n",
            "       [[0.06713867]],\n",
            "\n",
            "       [[0.58398438]]]), array([[[0.10449219]],\n",
            "\n",
            "       [[0.08874512]],\n",
            "\n",
            "       [[0.80664062]]]), array([[[0.42089844]],\n",
            "\n",
            "       [[0.04510498]],\n",
            "\n",
            "       [[0.53417969]]]), array([[[0.52929688]],\n",
            "\n",
            "       [[0.06665039]],\n",
            "\n",
            "       [[0.40380859]]]), array([[[0.15856934]],\n",
            "\n",
            "       [[0.05401611]],\n",
            "\n",
            "       [[0.78759766]]]), array([[[0.5234375 ]],\n",
            "\n",
            "       [[0.03335571]],\n",
            "\n",
            "       [[0.44335938]]]), array([[[0.00758743]],\n",
            "\n",
            "       [[0.0271759 ]],\n",
            "\n",
            "       [[0.96533203]]]), array([[[0.41357422]],\n",
            "\n",
            "       [[0.04998779]],\n",
            "\n",
            "       [[0.53662109]]]), array([[[0.31542969]],\n",
            "\n",
            "       [[0.06201172]],\n",
            "\n",
            "       [[0.62255859]]]), array([[[0.06774902]],\n",
            "\n",
            "       [[0.07904053]],\n",
            "\n",
            "       [[0.85302734]]]), array([[[0.10870361]],\n",
            "\n",
            "       [[0.01803589]],\n",
            "\n",
            "       [[0.87304688]]]), array([[[0.04064941]],\n",
            "\n",
            "       [[0.03265381]],\n",
            "\n",
            "       [[0.92675781]]]), array([[[0.26635742]],\n",
            "\n",
            "       [[0.03424072]],\n",
            "\n",
            "       [[0.69970703]]]), array([[[0.67382812]],\n",
            "\n",
            "       [[0.03799438]],\n",
            "\n",
            "       [[0.28833008]]]), array([[[0.09405518]],\n",
            "\n",
            "       [[0.05166626]],\n",
            "\n",
            "       [[0.85449219]]]), array([[[0.13049316]],\n",
            "\n",
            "       [[0.14831543]],\n",
            "\n",
            "       [[0.72119141]]]), array([[[0.76416016]],\n",
            "\n",
            "       [[0.02627563]],\n",
            "\n",
            "       [[0.20959473]]]), array([[[0.22216797]],\n",
            "\n",
            "       [[0.02253723]],\n",
            "\n",
            "       [[0.75537109]]]), array([[[0.09674072]],\n",
            "\n",
            "       [[0.07946777]],\n",
            "\n",
            "       [[0.82373047]]]), array([[[0.50732422]],\n",
            "\n",
            "       [[0.02862549]],\n",
            "\n",
            "       [[0.46435547]]]), array([[[0.15539551]],\n",
            "\n",
            "       [[0.02618408]],\n",
            "\n",
            "       [[0.81835938]]]), array([[[0.44555664]],\n",
            "\n",
            "       [[0.02879333]],\n",
            "\n",
            "       [[0.52587891]]]), array([[[0.13806152]],\n",
            "\n",
            "       [[0.05807495]],\n",
            "\n",
            "       [[0.80371094]]]), array([[[0.05249023]],\n",
            "\n",
            "       [[0.08544922]],\n",
            "\n",
            "       [[0.86230469]]]), array([[[0.08209229]],\n",
            "\n",
            "       [[0.01989746]],\n",
            "\n",
            "       [[0.89794922]]]), array([[[0.72509766]],\n",
            "\n",
            "       [[0.02139282]],\n",
            "\n",
            "       [[0.25366211]]]), array([[[0.25732422]],\n",
            "\n",
            "       [[0.04226685]],\n",
            "\n",
            "       [[0.70019531]]]), array([[[0.1673584 ]],\n",
            "\n",
            "       [[0.05114746]],\n",
            "\n",
            "       [[0.78125   ]]]), array([[[0.24621582]],\n",
            "\n",
            "       [[0.05151367]],\n",
            "\n",
            "       [[0.70214844]]]), array([[[0.36987305]],\n",
            "\n",
            "       [[0.05090332]],\n",
            "\n",
            "       [[0.57910156]]]), array([[[0.02894592]],\n",
            "\n",
            "       [[0.14245605]],\n",
            "\n",
            "       [[0.82861328]]]), array([[[0.21081543]],\n",
            "\n",
            "       [[0.05340576]],\n",
            "\n",
            "       [[0.73583984]]]), array([[[0.27368164]],\n",
            "\n",
            "       [[0.04595947]],\n",
            "\n",
            "       [[0.68066406]]]), array([[[0.09460449]],\n",
            "\n",
            "       [[0.03933716]],\n",
            "\n",
            "       [[0.86621094]]]), array([[[0.10894775]],\n",
            "\n",
            "       [[0.03564453]],\n",
            "\n",
            "       [[0.85546875]]]), array([[[0.43359375]],\n",
            "\n",
            "       [[0.03189087]],\n",
            "\n",
            "       [[0.53466797]]]), array([[[0.09991455]],\n",
            "\n",
            "       [[0.07354736]],\n",
            "\n",
            "       [[0.82666016]]]), array([[[0.5078125 ]],\n",
            "\n",
            "       [[0.0435791 ]],\n",
            "\n",
            "       [[0.44848633]]]), array([[[0.20019531]],\n",
            "\n",
            "       [[0.03689575]],\n",
            "\n",
            "       [[0.76318359]]]), array([[[0.61425781]],\n",
            "\n",
            "       [[0.03158569]],\n",
            "\n",
            "       [[0.35424805]]]), array([[[0.57421875]],\n",
            "\n",
            "       [[0.02537537]],\n",
            "\n",
            "       [[0.40039062]]]), array([[[0.15283203]],\n",
            "\n",
            "       [[0.05422974]],\n",
            "\n",
            "       [[0.79296875]]]), array([[[0.47290039]],\n",
            "\n",
            "       [[0.02035522]],\n",
            "\n",
            "       [[0.50683594]]]), array([[[0.72851562]],\n",
            "\n",
            "       [[0.02127075]],\n",
            "\n",
            "       [[0.25      ]]]), array([[[0.09033203]],\n",
            "\n",
            "       [[0.03170776]],\n",
            "\n",
            "       [[0.87792969]]]), array([[[0.09063721]],\n",
            "\n",
            "       [[0.07354736]],\n",
            "\n",
            "       [[0.8359375 ]]]), array([[[0.06958008]],\n",
            "\n",
            "       [[0.04525757]],\n",
            "\n",
            "       [[0.88525391]]]), array([[[0.70947266]],\n",
            "\n",
            "       [[0.01722717]],\n",
            "\n",
            "       [[0.2734375 ]]]), array([[[0.17651367]],\n",
            "\n",
            "       [[0.07055664]],\n",
            "\n",
            "       [[0.75292969]]]), array([[[0.52197266]],\n",
            "\n",
            "       [[0.03860474]],\n",
            "\n",
            "       [[0.43969727]]]), array([[[0.31884766]],\n",
            "\n",
            "       [[0.02050781]],\n",
            "\n",
            "       [[0.66064453]]]), array([[[0.08746338]],\n",
            "\n",
            "       [[0.02819824]],\n",
            "\n",
            "       [[0.88427734]]]), array([[[0.2286377 ]],\n",
            "\n",
            "       [[0.04058838]],\n",
            "\n",
            "       [[0.73095703]]]), array([[[0.14257812]],\n",
            "\n",
            "       [[0.04800415]],\n",
            "\n",
            "       [[0.80957031]]]), array([[[0.3359375 ]],\n",
            "\n",
            "       [[0.03930664]],\n",
            "\n",
            "       [[0.625     ]]]), array([[[0.41674805]],\n",
            "\n",
            "       [[0.07159424]],\n",
            "\n",
            "       [[0.51171875]]]), array([[[0.44555664]],\n",
            "\n",
            "       [[0.03884888]],\n",
            "\n",
            "       [[0.515625  ]]]), array([[[0.05136108]],\n",
            "\n",
            "       [[0.04626465]],\n",
            "\n",
            "       [[0.90234375]]]), array([[[0.02278137]],\n",
            "\n",
            "       [[0.02423096]],\n",
            "\n",
            "       [[0.953125  ]]]), array([[[0.04476929]],\n",
            "\n",
            "       [[0.05273438]],\n",
            "\n",
            "       [[0.90234375]]]), array([[[0.07830811]],\n",
            "\n",
            "       [[0.05548096]],\n",
            "\n",
            "       [[0.86621094]]]), array([[[0.22937012]],\n",
            "\n",
            "       [[0.08343506]],\n",
            "\n",
            "       [[0.68701172]]]), array([[[0.13745117]],\n",
            "\n",
            "       [[0.04324341]],\n",
            "\n",
            "       [[0.81933594]]]), array([[[0.609375  ]],\n",
            "\n",
            "       [[0.02038574]],\n",
            "\n",
            "       [[0.37036133]]]), array([[[0.50292969]],\n",
            "\n",
            "       [[0.07513428]],\n",
            "\n",
            "       [[0.421875  ]]]), array([[[0.10797119]],\n",
            "\n",
            "       [[0.04168701]],\n",
            "\n",
            "       [[0.85058594]]]), array([[[0.31274414]],\n",
            "\n",
            "       [[0.05892944]],\n",
            "\n",
            "       [[0.62841797]]]), array([[[0.27319336]],\n",
            "\n",
            "       [[0.02626038]],\n",
            "\n",
            "       [[0.70068359]]]), array([[[0.01647949]],\n",
            "\n",
            "       [[0.0690918 ]],\n",
            "\n",
            "       [[0.91455078]]]), array([[[0.20666504]],\n",
            "\n",
            "       [[0.04702759]],\n",
            "\n",
            "       [[0.74609375]]]), array([[[0.37158203]],\n",
            "\n",
            "       [[0.05093384]],\n",
            "\n",
            "       [[0.57763672]]]), array([[[0.50927734]],\n",
            "\n",
            "       [[0.03213501]],\n",
            "\n",
            "       [[0.45849609]]]), array([[[0.08288574]],\n",
            "\n",
            "       [[0.0302887 ]],\n",
            "\n",
            "       [[0.88671875]]]), array([[[0.80517578]],\n",
            "\n",
            "       [[0.01977539]],\n",
            "\n",
            "       [[0.17504883]]]), array([[[0.07672119]],\n",
            "\n",
            "       [[0.05569458]],\n",
            "\n",
            "       [[0.86767578]]]), array([[[0.24890137]],\n",
            "\n",
            "       [[0.04483032]],\n",
            "\n",
            "       [[0.70605469]]]), array([[[0.0993042 ]],\n",
            "\n",
            "       [[0.1496582 ]],\n",
            "\n",
            "       [[0.75097656]]]), array([[[0.10705566]],\n",
            "\n",
            "       [[0.11132812]],\n",
            "\n",
            "       [[0.78173828]]]), array([[[0.78564453]],\n",
            "\n",
            "       [[0.02709961]],\n",
            "\n",
            "       [[0.18725586]]]), array([[[0.16589355]],\n",
            "\n",
            "       [[0.04525757]],\n",
            "\n",
            "       [[0.7890625 ]]]), array([[[0.02130127]],\n",
            "\n",
            "       [[0.0475769 ]],\n",
            "\n",
            "       [[0.93115234]]]), array([[[0.87695312]],\n",
            "\n",
            "       [[0.01160431]],\n",
            "\n",
            "       [[0.11157227]]]), array([[[0.10437012]],\n",
            "\n",
            "       [[0.05502319]],\n",
            "\n",
            "       [[0.84082031]]]), array([[[0.09729004]],\n",
            "\n",
            "       [[0.05831909]],\n",
            "\n",
            "       [[0.84423828]]]), array([[[0.31787109]],\n",
            "\n",
            "       [[0.03189087]],\n",
            "\n",
            "       [[0.65039062]]]), array([[[0.27514648]],\n",
            "\n",
            "       [[0.04605103]],\n",
            "\n",
            "       [[0.67871094]]]), array([[[0.09338379]],\n",
            "\n",
            "       [[0.03109741]],\n",
            "\n",
            "       [[0.87548828]]]), array([[[0.83544922]],\n",
            "\n",
            "       [[0.01541901]],\n",
            "\n",
            "       [[0.14916992]]]), array([[[0.47094727]],\n",
            "\n",
            "       [[0.02351379]],\n",
            "\n",
            "       [[0.50537109]]]), array([[[0.55957031]],\n",
            "\n",
            "       [[0.02529907]],\n",
            "\n",
            "       [[0.41503906]]]), array([[[0.10717773]],\n",
            "\n",
            "       [[0.04190063]],\n",
            "\n",
            "       [[0.85107422]]]), array([[[0.07110596]],\n",
            "\n",
            "       [[0.0534668 ]],\n",
            "\n",
            "       [[0.87548828]]]), array([[[0.56738281]],\n",
            "\n",
            "       [[0.02682495]],\n",
            "\n",
            "       [[0.40576172]]]), array([[[0.20056152]],\n",
            "\n",
            "       [[0.23242188]],\n",
            "\n",
            "       [[0.56689453]]]), array([[[0.64892578]],\n",
            "\n",
            "       [[0.01351929]],\n",
            "\n",
            "       [[0.33764648]]]), array([[[0.49194336]],\n",
            "\n",
            "       [[0.04867554]],\n",
            "\n",
            "       [[0.45922852]]]), array([[[0.05148315]],\n",
            "\n",
            "       [[0.03851318]],\n",
            "\n",
            "       [[0.91015625]]]), array([[[0.02119446]],\n",
            "\n",
            "       [[0.04190063]],\n",
            "\n",
            "       [[0.93701172]]]), array([[[0.25878906]],\n",
            "\n",
            "       [[0.03640747]],\n",
            "\n",
            "       [[0.70458984]]]), array([[[0.18823242]],\n",
            "\n",
            "       [[0.07019043]],\n",
            "\n",
            "       [[0.74169922]]]), array([[[0.14916992]],\n",
            "\n",
            "       [[0.04733276]],\n",
            "\n",
            "       [[0.80371094]]]), array([[[0.26855469]],\n",
            "\n",
            "       [[0.04031372]],\n",
            "\n",
            "       [[0.69091797]]]), array([[[0.07641602]],\n",
            "\n",
            "       [[0.13220215]],\n",
            "\n",
            "       [[0.79150391]]]), array([[[0.35571289]],\n",
            "\n",
            "       [[0.05380249]],\n",
            "\n",
            "       [[0.59033203]]]), array([[[0.01335907]],\n",
            "\n",
            "       [[0.06451416]],\n",
            "\n",
            "       [[0.921875  ]]]), array([[[0.67041016]],\n",
            "\n",
            "       [[0.02642822]],\n",
            "\n",
            "       [[0.30322266]]]), array([[[0.41601562]],\n",
            "\n",
            "       [[0.06115723]],\n",
            "\n",
            "       [[0.52294922]]]), array([[[0.4621582 ]],\n",
            "\n",
            "       [[0.02868652]],\n",
            "\n",
            "       [[0.50927734]]]), array([[[0.13964844]],\n",
            "\n",
            "       [[0.04443359]],\n",
            "\n",
            "       [[0.81591797]]]), array([[[0.14575195]],\n",
            "\n",
            "       [[0.04220581]],\n",
            "\n",
            "       [[0.81201172]]]), array([[[0.03033447]],\n",
            "\n",
            "       [[0.02856445]],\n",
            "\n",
            "       [[0.94091797]]]), array([[[0.2668457 ]],\n",
            "\n",
            "       [[0.05230713]],\n",
            "\n",
            "       [[0.68066406]]]), array([[[0.5390625 ]],\n",
            "\n",
            "       [[0.03689575]],\n",
            "\n",
            "       [[0.42431641]]]), array([[[0.51660156]],\n",
            "\n",
            "       [[0.03479004]],\n",
            "\n",
            "       [[0.44873047]]]), array([[[0.11016846]],\n",
            "\n",
            "       [[0.04400635]],\n",
            "\n",
            "       [[0.84570312]]]), array([[[0.11395264]],\n",
            "\n",
            "       [[0.05276489]],\n",
            "\n",
            "       [[0.83349609]]]), array([[[0.66894531]],\n",
            "\n",
            "       [[0.0317688 ]],\n",
            "\n",
            "       [[0.29956055]]]), array([[[0.1217041 ]],\n",
            "\n",
            "       [[0.05755615]],\n",
            "\n",
            "       [[0.82080078]]]), array([[[0.84765625]],\n",
            "\n",
            "       [[0.01069641]],\n",
            "\n",
            "       [[0.1418457 ]]]), array([[[0.16723633]],\n",
            "\n",
            "       [[0.06088257]],\n",
            "\n",
            "       [[0.77197266]]]), array([[[0.28466797]],\n",
            "\n",
            "       [[0.04382324]],\n",
            "\n",
            "       [[0.67138672]]]), array([[[0.02836609]],\n",
            "\n",
            "       [[0.07714844]],\n",
            "\n",
            "       [[0.89453125]]]), array([[[0.2166748 ]],\n",
            "\n",
            "       [[0.04498291]],\n",
            "\n",
            "       [[0.73828125]]]), array([[[0.34545898]],\n",
            "\n",
            "       [[0.03164673]],\n",
            "\n",
            "       [[0.62304688]]]), array([[[0.09661865]],\n",
            "\n",
            "       [[0.11828613]],\n",
            "\n",
            "       [[0.78515625]]]), array([[[0.15844727]],\n",
            "\n",
            "       [[0.03860474]],\n",
            "\n",
            "       [[0.80273438]]]), array([[[0.63916016]],\n",
            "\n",
            "       [[0.04550171]],\n",
            "\n",
            "       [[0.31518555]]]), array([[[0.25415039]],\n",
            "\n",
            "       [[0.05264282]],\n",
            "\n",
            "       [[0.69335938]]]), array([[[0.58935547]],\n",
            "\n",
            "       [[0.03955078]],\n",
            "\n",
            "       [[0.37109375]]]), array([[[0.68408203]],\n",
            "\n",
            "       [[0.02474976]],\n",
            "\n",
            "       [[0.29101562]]]), array([[[0.79833984]],\n",
            "\n",
            "       [[0.0178833 ]],\n",
            "\n",
            "       [[0.18383789]]]), array([[[0.60107422]],\n",
            "\n",
            "       [[0.02867126]],\n",
            "\n",
            "       [[0.37011719]]]), array([[[0.03114319]],\n",
            "\n",
            "       [[0.03338623]],\n",
            "\n",
            "       [[0.93554688]]]), array([[[0.01994324]],\n",
            "\n",
            "       [[0.18237305]],\n",
            "\n",
            "       [[0.79785156]]]), array([[[0.07159424]],\n",
            "\n",
            "       [[0.07086182]],\n",
            "\n",
            "       [[0.85742188]]]), array([[[0.10662842]],\n",
            "\n",
            "       [[0.03979492]],\n",
            "\n",
            "       [[0.85351562]]]), array([[[0.31616211]],\n",
            "\n",
            "       [[0.03527832]],\n",
            "\n",
            "       [[0.6484375 ]]]), array([[[0.02703857]],\n",
            "\n",
            "       [[0.03808594]],\n",
            "\n",
            "       [[0.93505859]]]), array([[[0.17675781]],\n",
            "\n",
            "       [[0.04284668]],\n",
            "\n",
            "       [[0.78027344]]]), array([[[0.17956543]],\n",
            "\n",
            "       [[0.03707886]],\n",
            "\n",
            "       [[0.78320312]]]), array([[[0.09899902]],\n",
            "\n",
            "       [[0.05792236]],\n",
            "\n",
            "       [[0.84326172]]]), array([[[0.09100342]],\n",
            "\n",
            "       [[0.07635498]],\n",
            "\n",
            "       [[0.83251953]]]), array([[[0.54296875]],\n",
            "\n",
            "       [[0.0242157 ]],\n",
            "\n",
            "       [[0.43286133]]]), array([[[0.48999023]],\n",
            "\n",
            "       [[0.03311157]],\n",
            "\n",
            "       [[0.47705078]]]), array([[[0.15466309]],\n",
            "\n",
            "       [[0.03070068]],\n",
            "\n",
            "       [[0.81445312]]]), array([[[0.12963867]],\n",
            "\n",
            "       [[0.06173706]],\n",
            "\n",
            "       [[0.80859375]]]), array([[[0.54589844]],\n",
            "\n",
            "       [[0.04910278]],\n",
            "\n",
            "       [[0.4050293 ]]]), array([[[0.58642578]],\n",
            "\n",
            "       [[0.03045654]],\n",
            "\n",
            "       [[0.38305664]]]), array([[[0.05444336]],\n",
            "\n",
            "       [[0.03872681]],\n",
            "\n",
            "       [[0.90673828]]]), array([[[0.18688965]],\n",
            "\n",
            "       [[0.05987549]],\n",
            "\n",
            "       [[0.75341797]]]), array([[[0.24291992]],\n",
            "\n",
            "       [[0.05737305]],\n",
            "\n",
            "       [[0.69970703]]]), array([[[0.19433594]],\n",
            "\n",
            "       [[0.06933594]],\n",
            "\n",
            "       [[0.73632812]]]), array([[[0.13000488]],\n",
            "\n",
            "       [[0.05422974]],\n",
            "\n",
            "       [[0.81591797]]]), array([[[0.04641724]],\n",
            "\n",
            "       [[0.04455566]],\n",
            "\n",
            "       [[0.90917969]]]), array([[[0.08032227]],\n",
            "\n",
            "       [[0.07080078]],\n",
            "\n",
            "       [[0.84912109]]]), array([[[0.50878906]],\n",
            "\n",
            "       [[0.03152466]],\n",
            "\n",
            "       [[0.4597168 ]]]), array([[[0.02671814]],\n",
            "\n",
            "       [[0.01757812]],\n",
            "\n",
            "       [[0.95556641]]]), array([[[0.06082153]],\n",
            "\n",
            "       [[0.01779175]],\n",
            "\n",
            "       [[0.92138672]]]), array([[[0.15869141]],\n",
            "\n",
            "       [[0.04211426]],\n",
            "\n",
            "       [[0.79931641]]]), array([[[0.20129395]],\n",
            "\n",
            "       [[0.03549194]],\n",
            "\n",
            "       [[0.76318359]]]), array([[[0.21386719]],\n",
            "\n",
            "       [[0.11218262]],\n",
            "\n",
            "       [[0.67382812]]]), array([[[0.03948975]],\n",
            "\n",
            "       [[0.02589417]],\n",
            "\n",
            "       [[0.93457031]]]), array([[[0.09472656]],\n",
            "\n",
            "       [[0.037323  ]],\n",
            "\n",
            "       [[0.86816406]]]), array([[[0.23693848]],\n",
            "\n",
            "       [[0.05383301]],\n",
            "\n",
            "       [[0.70898438]]]), array([[[0.2130127 ]],\n",
            "\n",
            "       [[0.06420898]],\n",
            "\n",
            "       [[0.72265625]]]), array([[[0.11590576]],\n",
            "\n",
            "       [[0.07397461]],\n",
            "\n",
            "       [[0.81005859]]]), array([[[0.05529785]],\n",
            "\n",
            "       [[0.03646851]],\n",
            "\n",
            "       [[0.90820312]]]), array([[[0.07318115]],\n",
            "\n",
            "       [[0.05697632]],\n",
            "\n",
            "       [[0.86962891]]]), array([[[0.18225098]],\n",
            "\n",
            "       [[0.02954102]],\n",
            "\n",
            "       [[0.78808594]]]), array([[[0.09405518]],\n",
            "\n",
            "       [[0.07232666]],\n",
            "\n",
            "       [[0.83349609]]]), array([[[0.75195312]],\n",
            "\n",
            "       [[0.02520752]],\n",
            "\n",
            "       [[0.22290039]]]), array([[[0.14208984]],\n",
            "\n",
            "       [[0.06610107]],\n",
            "\n",
            "       [[0.79199219]]]), array([[[0.13024902]],\n",
            "\n",
            "       [[0.06567383]],\n",
            "\n",
            "       [[0.80419922]]]), array([[[0.76367188]],\n",
            "\n",
            "       [[0.01576233]],\n",
            "\n",
            "       [[0.22070312]]]), array([[[0.04162598]],\n",
            "\n",
            "       [[0.06848145]],\n",
            "\n",
            "       [[0.89013672]]]), array([[[0.03131104]],\n",
            "\n",
            "       [[0.03607178]],\n",
            "\n",
            "       [[0.93261719]]]), array([[[0.09954834]],\n",
            "\n",
            "       [[0.05795288]],\n",
            "\n",
            "       [[0.84228516]]]), array([[[0.13330078]],\n",
            "\n",
            "       [[0.07281494]],\n",
            "\n",
            "       [[0.79394531]]])]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summac_scores = score_zs1[\"scores\"]"
      ],
      "metadata": {
        "id": "aX7fz446PZVI"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summac_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTrY_SPWPnyO",
        "outputId": "35b0d439-a4cc-4712-c04f-caa8c1b66e7f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.69085693359375,\n",
              " 0.23065185546875,\n",
              " 0.0718994140625,\n",
              " 0.118865966796875,\n",
              " 0.0321044921875,\n",
              " 0.05865478515625,\n",
              " 0.240020751953125,\n",
              " 0.19793701171875,\n",
              " 0.72186279296875,\n",
              " 0.02447509765625,\n",
              " 0.356109619140625,\n",
              " 0.028228759765625,\n",
              " 0.4222869873046875,\n",
              " 0.238006591796875,\n",
              " 0.1130828857421875,\n",
              " 0.063812255859375,\n",
              " 0.023040771484375,\n",
              " 0.27874755859375,\n",
              " 0.15771484375,\n",
              " 0.1813201904296875,\n",
              " 0.333648681640625,\n",
              " -0.003875732421875,\n",
              " 0.17059326171875,\n",
              " 0.126251220703125,\n",
              " 0.6371612548828125,\n",
              " 0.182647705078125,\n",
              " 0.090576171875,\n",
              " 0.3489990234375,\n",
              " 0.69677734375,\n",
              " 0.24627685546875,\n",
              " 0.190643310546875,\n",
              " 0.084716796875,\n",
              " 0.7009048461914062,\n",
              " 0.1357421875,\n",
              " 0.0947265625,\n",
              " 0.0301513671875,\n",
              " 0.081756591796875,\n",
              " -0.0489044189453125,\n",
              " 0.461456298828125,\n",
              " 0.598785400390625,\n",
              " -0.39404296875,\n",
              " 0.1522216796875,\n",
              " 0.22979736328125,\n",
              " 0.784942626953125,\n",
              " 0.24688720703125,\n",
              " 0.4996337890625,\n",
              " 0.40277099609375,\n",
              " 0.10302734375,\n",
              " -0.13970947265625,\n",
              " 0.0120849609375,\n",
              " -0.04986572265625,\n",
              " -0.0123291015625,\n",
              " 0.259521484375,\n",
              " 0.4701385498046875,\n",
              " 0.0465087890625,\n",
              " 0.022705078125,\n",
              " 0.005279541015625,\n",
              " 0.246307373046875,\n",
              " 0.02655029296875,\n",
              " 0.103607177734375,\n",
              " 0.02557373046875,\n",
              " -0.0372314453125,\n",
              " 0.0657958984375,\n",
              " 0.5293121337890625,\n",
              " 0.45947265625,\n",
              " 0.348663330078125,\n",
              " 0.06268310546875,\n",
              " 0.62841796875,\n",
              " 0.061065673828125,\n",
              " 0.475830078125,\n",
              " 0.030731201171875,\n",
              " 0.5945663452148438,\n",
              " 0.187744140625,\n",
              " -0.01092529296875,\n",
              " -0.0264892578125,\n",
              " -0.136993408203125,\n",
              " 0.220184326171875,\n",
              " 0.0380859375,\n",
              " 0.3172607421875,\n",
              " -0.0395355224609375,\n",
              " -0.049957275390625,\n",
              " 0.2423858642578125,\n",
              " 0.25274658203125,\n",
              " 0.066650390625,\n",
              " -0.1119384765625,\n",
              " -0.058135986328125,\n",
              " 0.07122802734375,\n",
              " 0.149658203125,\n",
              " -0.14581298828125,\n",
              " 0.32989501953125,\n",
              " 0.214111328125,\n",
              " 0.2430419921875,\n",
              " 0.1190643310546875,\n",
              " 0.19097900390625,\n",
              " 0.301239013671875,\n",
              " -0.063995361328125,\n",
              " 0.3119354248046875,\n",
              " 0.5432281494140625,\n",
              " 0.354095458984375,\n",
              " 0.46405029296875,\n",
              " 0.04473876953125,\n",
              " 0.171630859375,\n",
              " 0.0032958984375,\n",
              " 0.15765380859375,\n",
              " -0.04376220703125,\n",
              " -0.06719970703125,\n",
              " -0.09051513671875,\n",
              " 0.34417724609375,\n",
              " 0.133544921875,\n",
              " 0.1839599609375,\n",
              " -0.05755615234375,\n",
              " 0.06903076171875,\n",
              " 0.02593994140625,\n",
              " 0.15570068359375,\n",
              " 0.11004638671875,\n",
              " 0.269805908203125,\n",
              " 0.46075439453125,\n",
              " -0.00872802734375,\n",
              " 0.011505126953125,\n",
              " 0.112548828125,\n",
              " 0.106231689453125,\n",
              " -0.0323486328125,\n",
              " 0.05010986328125,\n",
              " 0.06201171875,\n",
              " -0.12139892578125,\n",
              " 0.0648193359375,\n",
              " -0.032073974609375,\n",
              " 0.5455322265625,\n",
              " 0.1280517578125,\n",
              " 0.139007568359375,\n",
              " 0.1429443359375,\n",
              " -0.01361083984375,\n",
              " 0.055267333984375,\n",
              " 0.02679443359375,\n",
              " -0.026580810546875,\n",
              " 0.0416259765625,\n",
              " -0.022613525390625,\n",
              " -0.01171875,\n",
              " 0.00189208984375,\n",
              " 0.8260345458984375,\n",
              " 0.11138916015625,\n",
              " 0.38262939453125,\n",
              " 0.01409912109375,\n",
              " 0.540374755859375,\n",
              " -0.0025634765625,\n",
              " 0.05126953125,\n",
              " -0.04315185546875,\n",
              " 0.267608642578125,\n",
              " 0.0946044921875,\n",
              " 0.3170166015625,\n",
              " 0.007720947265625,\n",
              " 0.05029296875,\n",
              " -0.0225830078125,\n",
              " -0.029205322265625,\n",
              " -0.0264129638671875,\n",
              " 0.5138092041015625,\n",
              " -0.0047607421875,\n",
              " 0.238433837890625,\n",
              " 0.0616455078125,\n",
              " 0.236297607421875,\n",
              " 0.08599853515625,\n",
              " -0.00946044921875,\n",
              " 0.717254638671875,\n",
              " 0.235504150390625,\n",
              " 0.8096466064453125,\n",
              " 0.650848388671875,\n",
              " 0.044403076171875,\n",
              " 0.28399658203125,\n",
              " 0.0830078125,\n",
              " 0.08685302734375,\n",
              " 0.11614990234375,\n",
              " 0.370819091796875,\n",
              " 0.014404296875,\n",
              " 0.2047119140625,\n",
              " 0.02252197265625,\n",
              " -0.08624267578125,\n",
              " 0.00909423828125,\n",
              " 0.241729736328125,\n",
              " 0.210968017578125,\n",
              " -0.061279296875,\n",
              " 0.163330078125,\n",
              " -0.094329833984375,\n",
              " 0.0040283203125,\n",
              " 0.000396728515625,\n",
              " 0.122802734375,\n",
              " 0.211212158203125,\n",
              " 0.078582763671875,\n",
              " 0.248779296875,\n",
              " 0.058624267578125,\n",
              " 0.0008544921875,\n",
              " 0.00103759765625,\n",
              " 0.06085205078125,\n",
              " -0.031494140625,\n",
              " 0.020233154296875,\n",
              " 0.057891845703125,\n",
              " 0.00762939453125,\n",
              " 0.4520111083984375,\n",
              " 0.02239990234375,\n",
              " 0.26318359375,\n",
              " 0.102142333984375,\n",
              " -0.014007568359375,\n",
              " 0.35498046875,\n",
              " 0.01519775390625,\n",
              " 0.073699951171875,\n",
              " 0.2154541015625,\n",
              " 0.106597900390625,\n",
              " 0.18212890625,\n",
              " 0.190521240234375,\n",
              " 0.01556396484375,\n",
              " -0.004547119140625,\n",
              " 0.131103515625,\n",
              " 0.140625,\n",
              " 0.81878662109375,\n",
              " 0.25628662109375,\n",
              " 0.02838134765625,\n",
              " 0.07769775390625,\n",
              " 0.0092620849609375,\n",
              " 0.158660888671875,\n",
              " 0.5473175048828125,\n",
              " 0.11126708984375,\n",
              " 0.05926513671875,\n",
              " 0.0360107421875,\n",
              " 0.448486328125,\n",
              " 0.253326416015625,\n",
              " -0.1536102294921875,\n",
              " 0.029571533203125,\n",
              " 0.025115966796875,\n",
              " 0.016754150390625,\n",
              " 0.3373260498046875,\n",
              " 0.11651611328125,\n",
              " 0.0225677490234375,\n",
              " 0.0940093994140625,\n",
              " 0.22442626953125,\n",
              " 0.016021728515625,\n",
              " 0.30517578125,\n",
              " 0.03680419921875,\n",
              " 0.2945556640625,\n",
              " 0.01837158203125,\n",
              " 0.0166015625,\n",
              " 0.1138916015625,\n",
              " 0.099853515625,\n",
              " 0.309234619140625,\n",
              " 0.7186126708984375,\n",
              " 0.5221405029296875,\n",
              " 0.04449462890625,\n",
              " 0.18450927734375,\n",
              " 0.18768310546875,\n",
              " 0.754791259765625,\n",
              " 0.05413818359375,\n",
              " 0.607513427734375,\n",
              " 0.128204345703125,\n",
              " 0.243438720703125,\n",
              " 0.090362548828125,\n",
              " 0.00518798828125,\n",
              " 0.0921630859375,\n",
              " 0.02197265625,\n",
              " 0.0791015625,\n",
              " 0.103271484375,\n",
              " -0.0806884765625,\n",
              " 0.073394775390625,\n",
              " -0.0838775634765625,\n",
              " 0.5905609130859375,\n",
              " 0.0792236328125,\n",
              " 0.00787353515625,\n",
              " 0.410491943359375,\n",
              " 0.11767578125,\n",
              " 0.04443359375,\n",
              " 0.01470947265625,\n",
              " 0.2225341796875,\n",
              " 0.03118896484375,\n",
              " 0.35943603515625,\n",
              " 0.271514892578125,\n",
              " 0.7816162109375,\n",
              " 0.01910400390625,\n",
              " 0.120941162109375,\n",
              " 0.252593994140625,\n",
              " 0.199951171875,\n",
              " -0.0089111328125,\n",
              " -0.12487030029296875,\n",
              " 0.11773681640625,\n",
              " 0.04901123046875,\n",
              " 0.0972900390625,\n",
              " 0.02313232421875,\n",
              " 0.15509033203125,\n",
              " 0.072509765625,\n",
              " 0.03155517578125,\n",
              " 0.1229248046875,\n",
              " 0.3507537841796875,\n",
              " 0.123046875,\n",
              " 0.05657958984375,\n",
              " 0.197021484375,\n",
              " 0.01654052734375,\n",
              " 0.1053466796875,\n",
              " 0.0260009765625,\n",
              " -0.02799224853515625,\n",
              " 0.055572509765625,\n",
              " 0.07843017578125,\n",
              " 0.2857666015625,\n",
              " 0.607177734375,\n",
              " -0.02191162109375,\n",
              " 0.38140869140625,\n",
              " 0.22052001953125,\n",
              " 0.05169677734375,\n",
              " -0.0263824462890625,\n",
              " -0.00433349609375,\n",
              " 0.06207275390625,\n",
              " 0.435150146484375,\n",
              " 0.117584228515625,\n",
              " 0.0289306640625,\n",
              " 0.1126708984375,\n",
              " 0.381317138671875,\n",
              " 0.657745361328125,\n",
              " 0.0954132080078125,\n",
              " 0.0325927734375,\n",
              " -0.009918212890625,\n",
              " 0.072479248046875,\n",
              " 0.009552001953125,\n",
              " 0.222747802734375,\n",
              " -0.08984375,\n",
              " 0.47796630859375,\n",
              " -0.024444580078125,\n",
              " 0.0037841796875,\n",
              " 0.270538330078125,\n",
              " -0.078155517578125,\n",
              " 0.1885986328125,\n",
              " 0.081817626953125,\n",
              " 0.1077880859375,\n",
              " 0.0545806884765625,\n",
              " 0.436248779296875,\n",
              " 0.21844482421875,\n",
              " 0.0670166015625,\n",
              " 0.10699462890625,\n",
              " 0.487518310546875,\n",
              " 0.076202392578125,\n",
              " 0.025054931640625,\n",
              " 0.186431884765625,\n",
              " 0.17401123046875,\n",
              " 0.087158203125,\n",
              " 0.020843505859375,\n",
              " 0.00982666015625,\n",
              " 0.770782470703125,\n",
              " 0.935760498046875,\n",
              " 0.06390380859375,\n",
              " 0.50640869140625,\n",
              " 0.0189208984375,\n",
              " -0.13350677490234375,\n",
              " 0.41339111328125,\n",
              " 0.0579833984375,\n",
              " 0.403564453125,\n",
              " 0.382537841796875,\n",
              " 0.01171875,\n",
              " 0.09503173828125,\n",
              " -0.14227294921875,\n",
              " 0.22906494140625,\n",
              " 0.00140380859375,\n",
              " 0.1566162109375,\n",
              " 0.071990966796875,\n",
              " 0.091522216796875,\n",
              " 0.059478759765625,\n",
              " 0.0383758544921875,\n",
              " 0.05780029296875,\n",
              " 0.057159423828125,\n",
              " 0.080352783203125,\n",
              " -0.017822265625,\n",
              " 0.27471923828125,\n",
              " 0.067413330078125,\n",
              " 0.285186767578125,\n",
              " 0.1180419921875,\n",
              " 0.0177001953125,\n",
              " 0.7516021728515625,\n",
              " -0.0369873046875,\n",
              " 0.165802001953125,\n",
              " -0.01190185546875,\n",
              " 0.349151611328125,\n",
              " 0.08770751953125,\n",
              " 0.0526123046875,\n",
              " 0.368804931640625,\n",
              " 0.017730712890625,\n",
              " 0.0007476806640625,\n",
              " -0.00189208984375,\n",
              " 0.0792236328125,\n",
              " 0.146728515625,\n",
              " 0.03619384765625,\n",
              " 0.12200927734375,\n",
              " -0.02923583984375,\n",
              " 0.05242919921875,\n",
              " 0.459625244140625,\n",
              " 0.067840576171875,\n",
              " -0.0899658203125,\n",
              " 0.03497314453125,\n",
              " 0.2320556640625,\n",
              " 0.24310302734375,\n",
              " -0.02227783203125,\n",
              " 0.227203369140625,\n",
              " 0.1494140625,\n",
              " 0.417327880859375,\n",
              " 0.140655517578125,\n",
              " -0.00762939453125,\n",
              " 0.02435302734375,\n",
              " 0.5489349365234375,\n",
              " -0.0205078125,\n",
              " 0.376922607421875,\n",
              " 0.012420654296875,\n",
              " -0.04998779296875,\n",
              " 0.05328369140625,\n",
              " 0.07891845703125,\n",
              " -0.024444580078125,\n",
              " 0.056488037109375,\n",
              " 0.328948974609375,\n",
              " 0.03656005859375,\n",
              " 0.10888671875,\n",
              " 0.109130859375,\n",
              " 0.0859375,\n",
              " 0.09130859375,\n",
              " 0.3243408203125,\n",
              " 0.152191162109375,\n",
              " 0.014068603515625,\n",
              " 0.09521484375,\n",
              " 0.176055908203125,\n",
              " 0.007720947265625,\n",
              " 0.080657958984375,\n",
              " 0.023895263671875,\n",
              " 0.0125732421875,\n",
              " 0.31988525390625,\n",
              " 0.74896240234375,\n",
              " 0.59027099609375,\n",
              " 0.1837158203125,\n",
              " 0.2685394287109375,\n",
              " 0.0050048828125,\n",
              " 0.13934326171875,\n",
              " 0.62164306640625,\n",
              " 0.1641845703125,\n",
              " 0.31011962890625,\n",
              " 0.03460693359375,\n",
              " -0.052642822265625,\n",
              " 0.071136474609375,\n",
              " -0.22405242919921875,\n",
              " 0.30767822265625,\n",
              " 0.01025390625,\n",
              " 0.04681396484375,\n",
              " 0.019866943359375,\n",
              " -0.0877227783203125,\n",
              " 0.0679931640625,\n",
              " 0.099761962890625,\n",
              " 0.0384063720703125,\n",
              " 0.550537109375,\n",
              " 0.64459228515625,\n",
              " 0.236083984375,\n",
              " 0.235992431640625,\n",
              " -0.0035400390625,\n",
              " 0.08123779296875,\n",
              " 0.0106201171875,\n",
              " 0.27508544921875,\n",
              " 0.150634765625,\n",
              " 0.307769775390625,\n",
              " -0.028961181640625,\n",
              " 0.458526611328125,\n",
              " 0.4129638671875,\n",
              " 0.204132080078125,\n",
              " 0.14892578125,\n",
              " 0.30328369140625,\n",
              " 0.5589447021484375,\n",
              " 0.05145263671875,\n",
              " 0.030853271484375,\n",
              " 0.006317138671875,\n",
              " 0.133453369140625,\n",
              " -0.0789337158203125,\n",
              " 0.480316162109375,\n",
              " 0.023345947265625,\n",
              " 0.011932373046875,\n",
              " 0.104248046875,\n",
              " 0.08294677734375,\n",
              " 0.05303955078125,\n",
              " 0.0355224609375,\n",
              " 0.044708251953125,\n",
              " 0.34185791015625,\n",
              " 0.4517822265625,\n",
              " 0.0149993896484375,\n",
              " -0.15074920654296875,\n",
              " 0.02032470703125,\n",
              " 0.1578369140625,\n",
              " 0.042236328125,\n",
              " 0.099517822265625,\n",
              " 0.01251220703125,\n",
              " 0.1453857421875,\n",
              " 0.475677490234375,\n",
              " 0.279876708984375,\n",
              " -0.085784912109375,\n",
              " 0.098480224609375,\n",
              " 0.4390869140625,\n",
              " -0.00439453125,\n",
              " 0.750457763671875,\n",
              " 0.225128173828125,\n",
              " 0.33306884765625,\n",
              " 0.0279541015625,\n",
              " 0.11669921875,\n",
              " 0.09228515625,\n",
              " 0.059234619140625,\n",
              " 0.2929840087890625,\n",
              " 0.0755615234375,\n",
              " 0.1273193359375,\n",
              " 0.6470947265625,\n",
              " 0.01690673828125,\n",
              " 0.000946044921875,\n",
              " 0.31182861328125,\n",
              " 0.1810302734375,\n",
              " 0.133392333984375,\n",
              " 0.1984710693359375,\n",
              " 0.051513671875,\n",
              " 0.2386474609375,\n",
              " 0.34014892578125,\n",
              " 0.14825439453125,\n",
              " 0.57952880859375,\n",
              " 0.08123779296875,\n",
              " 0.3980712890625,\n",
              " 0.408355712890625,\n",
              " 0.0653076171875,\n",
              " 0.62176513671875,\n",
              " 0.021453857421875,\n",
              " 0.3724212646484375,\n",
              " 0.12744140625,\n",
              " 0.12896728515625,\n",
              " 0.249908447265625,\n",
              " -0.00445556640625,\n",
              " 0.7230377197265625,\n",
              " 0.1300048828125,\n",
              " 0.050567626953125,\n",
              " -0.030609130859375,\n",
              " 0.08331298828125,\n",
              " -0.0759124755859375,\n",
              " -0.017822265625,\n",
              " -0.0721435546875,\n",
              " -0.008026123046875,\n",
              " 0.15850830078125,\n",
              " 0.51361083984375,\n",
              " 0.107940673828125,\n",
              " 0.05291748046875,\n",
              " -0.02288818359375,\n",
              " 0.306060791015625,\n",
              " 0.05859375,\n",
              " 0.2969970703125,\n",
              " 0.38800048828125,\n",
              " 0.220123291015625,\n",
              " 0.14105224609375,\n",
              " 0.259124755859375,\n",
              " -0.02581787109375,\n",
              " 0.7674102783203125,\n",
              " -0.038726806640625,\n",
              " 0.171875,\n",
              " 0.0679931640625,\n",
              " 0.2623291015625,\n",
              " 0.00164794921875,\n",
              " 0.04437255859375,\n",
              " 0.356781005859375,\n",
              " 0.0777587890625,\n",
              " 0.2489013671875,\n",
              " -0.09918212890625,\n",
              " 0.0889892578125,\n",
              " 0.129608154296875,\n",
              " 0.017822265625,\n",
              " -0.0567474365234375,\n",
              " 0.119720458984375,\n",
              " -0.060943603515625,\n",
              " -0.003509521484375,\n",
              " 0.036376953125,\n",
              " 0.0322265625,\n",
              " 0.0384521484375,\n",
              " 0.17999267578125,\n",
              " 0.12823486328125,\n",
              " 0.133148193359375,\n",
              " 0.0224609375,\n",
              " 0.390838623046875,\n",
              " 0.336639404296875,\n",
              " 0.0025634765625,\n",
              " 0.0511474609375,\n",
              " 0.5820770263671875,\n",
              " 0.092254638671875,\n",
              " 0.1939697265625,\n",
              " 0.001953125,\n",
              " 0.202301025390625,\n",
              " 0.11505126953125,\n",
              " 0.23626708984375,\n",
              " 0.16748046875,\n",
              " 0.18719482421875,\n",
              " 0.6961517333984375,\n",
              " 0.1529541015625,\n",
              " 0.02783203125,\n",
              " 0.302001953125,\n",
              " -0.0067138671875,\n",
              " 0.033843994140625,\n",
              " -0.03302001953125,\n",
              " 0.7178802490234375,\n",
              " 0.175018310546875,\n",
              " 0.01519775390625,\n",
              " -0.0357666015625,\n",
              " 0.2230224609375,\n",
              " 0.23895263671875,\n",
              " 0.0535888671875,\n",
              " 0.44586181640625,\n",
              " 0.18328857421875,\n",
              " -0.016632080078125,\n",
              " 0.7437973022460938,\n",
              " 0.158294677734375,\n",
              " -0.005279541015625,\n",
              " 0.1429443359375,\n",
              " 0.24920654296875,\n",
              " 0.6344146728515625,\n",
              " 0.033203125,\n",
              " 0.171966552734375,\n",
              " 0.071319580078125,\n",
              " 0.36187744140625,\n",
              " 0.45843505859375,\n",
              " 0.05517578125,\n",
              " 0.14569091796875,\n",
              " 0.0284881591796875,\n",
              " -0.01495361328125,\n",
              " 0.07000732421875,\n",
              " -0.0111846923828125,\n",
              " 0.143035888671875,\n",
              " 0.059173583984375,\n",
              " 0.00433349609375,\n",
              " -0.0205078125,\n",
              " 0.14666748046875,\n",
              " 0.326324462890625,\n",
              " -0.079010009765625,\n",
              " -0.0030517578125,\n",
              " -0.0006103515625,\n",
              " -0.0035400390625,\n",
              " 0.0535888671875,\n",
              " -0.040924072265625,\n",
              " 0.00238037109375,\n",
              " 0.159454345703125,\n",
              " 0.124664306640625,\n",
              " 0.116546630859375,\n",
              " -0.0702972412109375,\n",
              " 0.645751953125,\n",
              " 0.9114456176757812,\n",
              " 0.008544921875,\n",
              " 0.5693359375,\n",
              " 0.42279052734375,\n",
              " 0.08417510986328125,\n",
              " 0.0487060546875,\n",
              " 0.23297119140625,\n",
              " 0.06292724609375,\n",
              " 0.11004638671875,\n",
              " 0.528839111328125,\n",
              " 0.03375244140625,\n",
              " 0.158935546875,\n",
              " 0.2796630859375,\n",
              " 0.10595703125,\n",
              " 0.037109375,\n",
              " 0.24395751953125,\n",
              " 0.084075927734375,\n",
              " 0.59600830078125,\n",
              " 0.11065673828125,\n",
              " 0.2125701904296875,\n",
              " 0.255035400390625,\n",
              " 0.20538330078125,\n",
              " 0.0521240234375,\n",
              " 0.700347900390625,\n",
              " 0.023956298828125,\n",
              " 0.15789794921875,\n",
              " 0.547210693359375,\n",
              " 0.181640625,\n",
              " 0.8034515380859375,\n",
              " -0.0104827880859375,\n",
              " 0.7335205078125,\n",
              " 0.0363311767578125,\n",
              " 0.0258026123046875,\n",
              " 0.5269775390625,\n",
              " 0.11444091796875,\n",
              " 0.347930908203125,\n",
              " 0.04754638671875,\n",
              " 0.0142974853515625,\n",
              " 0.2997894287109375,\n",
              " 0.50897216796875,\n",
              " -0.010589599609375,\n",
              " 0.23785400390625,\n",
              " 0.16357421875,\n",
              " 0.3280029296875,\n",
              " 0.24346923828125,\n",
              " 0.0540771484375,\n",
              " 0.02691650390625,\n",
              " 0.2464599609375,\n",
              " 0.028717041015625,\n",
              " 0.49676513671875,\n",
              " 0.11431884765625,\n",
              " 0.04925537109375,\n",
              " 0.05303955078125,\n",
              " 0.2223052978515625,\n",
              " 0.322509765625,\n",
              " 0.2662353515625,\n",
              " 0.509735107421875,\n",
              " 0.509521484375,\n",
              " 0.02874755859375,\n",
              " 0.155792236328125,\n",
              " 0.1298828125,\n",
              " 0.05828857421875,\n",
              " 0.2581787109375,\n",
              " 0.6190032958984375,\n",
              " 0.589111328125,\n",
              " 0.482025146484375,\n",
              " 0.244354248046875,\n",
              " 0.6453857421875,\n",
              " 0.290802001953125,\n",
              " 0.8579254150390625,\n",
              " 0.147552490234375,\n",
              " 0.1446533203125,\n",
              " 0.346527099609375,\n",
              " 0.605010986328125,\n",
              " 0.085052490234375,\n",
              " 0.05963134765625,\n",
              " 0.74261474609375,\n",
              " 0.40875244140625,\n",
              " 0.100006103515625,\n",
              " 0.1365966796875,\n",
              " 0.384429931640625,\n",
              " 0.2210693359375,\n",
              " -0.001068115234375,\n",
              " 0.419281005859375,\n",
              " 0.347320556640625,\n",
              " 0.071136474609375,\n",
              " 0.8124008178710938,\n",
              " 0.078094482421875,\n",
              " 0.5513153076171875,\n",
              " 0.01336669921875,\n",
              " 0.329254150390625,\n",
              " 0.148895263671875,\n",
              " 0.270111083984375,\n",
              " 0.322845458984375,\n",
              " 0.1162109375,\n",
              " 0.169891357421875,\n",
              " 0.543914794921875,\n",
              " -0.03607177734375,\n",
              " 0.0151824951171875,\n",
              " 0.472808837890625,\n",
              " 0.0684051513671875,\n",
              " 0.4794158935546875,\n",
              " 0.685089111328125,\n",
              " 0.7365570068359375,\n",
              " 0.176849365234375,\n",
              " 0.172454833984375,\n",
              " 0.398712158203125,\n",
              " 0.299896240234375,\n",
              " 0.6563262939453125,\n",
              " 0.33905029296875,\n",
              " 0.7171478271484375,\n",
              " 0.5646514892578125,\n",
              " 0.43511962890625,\n",
              " 0.646148681640625,\n",
              " 0.148895263671875,\n",
              " 0.7630157470703125,\n",
              " 0.4782562255859375,\n",
              " -0.003082275390625,\n",
              " 0.038665771484375,\n",
              " -0.0198974609375,\n",
              " 0.7287445068359375,\n",
              " 0.8837356567382812,\n",
              " 0.00775146484375,\n",
              " 0.333251953125,\n",
              " 0.6279296875,\n",
              " 0.0029296875,\n",
              " 0.19403076171875,\n",
              " 0.299896240234375,\n",
              " 0.386474609375,\n",
              " 0.144683837890625,\n",
              " 0.573272705078125,\n",
              " 0.203948974609375,\n",
              " 0.73333740234375,\n",
              " 0.6197662353515625,\n",
              " 0.41693115234375,\n",
              " 0.296295166015625,\n",
              " 0.1746826171875,\n",
              " 0.41986083984375,\n",
              " 0.4874267578125,\n",
              " 0.137115478515625,\n",
              " 0.0061187744140625,\n",
              " 0.1318359375,\n",
              " -0.073822021484375,\n",
              " 0.1673583984375,\n",
              " 0.1987457275390625,\n",
              " 0.369476318359375,\n",
              " 0.264892578125,\n",
              " -0.00678253173828125,\n",
              " 0.162811279296875,\n",
              " 0.05712890625,\n",
              " 0.0098876953125,\n",
              " 0.4818115234375,\n",
              " -0.02044677734375,\n",
              " 0.12506103515625,\n",
              " 0.8224029541015625,\n",
              " -0.03094482421875,\n",
              " 0.1878509521484375,\n",
              " 0.2606201171875,\n",
              " 0.022247314453125,\n",
              " 0.153564453125,\n",
              " 0.308013916015625,\n",
              " 0.1883544921875,\n",
              " 0.7861785888671875,\n",
              " 0.49151611328125,\n",
              " 0.4322662353515625,\n",
              " 0.19683837890625,\n",
              " 0.271026611328125,\n",
              " 0.1326904296875,\n",
              " -0.016143798828125,\n",
              " 0.0728759765625,\n",
              " 0.167755126953125,\n",
              " 0.77825927734375,\n",
              " 0.16583251953125,\n",
              " 0.00042724609375,\n",
              " 0.237640380859375,\n",
              " 0.369842529296875,\n",
              " 0.137176513671875,\n",
              " -0.02008056640625,\n",
              " 0.0456695556640625,\n",
              " 0.099609375,\n",
              " 0.136505126953125,\n",
              " 0.0107421875,\n",
              " 0.422119140625,\n",
              " 0.5628204345703125,\n",
              " -0.006744384765625,\n",
              " 0.202392578125,\n",
              " 0.291412353515625,\n",
              " 0.8369522094726562,\n",
              " 0.12432861328125,\n",
              " 0.1271514892578125,\n",
              " 0.01617431640625,\n",
              " 0.12701416015625,\n",
              " 0.46722412109375,\n",
              " 0.0202789306640625,\n",
              " 0.0093994140625,\n",
              " 0.2081298828125,\n",
              " 0.3990631103515625,\n",
              " 0.344268798828125,\n",
              " 0.10833740234375,\n",
              " -0.127838134765625,\n",
              " 0.05755615234375,\n",
              " 0.268341064453125,\n",
              " 0.16534423828125,\n",
              " 0.6199493408203125,\n",
              " 0.63092041015625,\n",
              " 0.01544189453125,\n",
              " 0.8318634033203125,\n",
              " 0.2149658203125,\n",
              " 0.1142578125,\n",
              " 0.31292724609375,\n",
              " 0.005279541015625,\n",
              " 0.1044921875,\n",
              " 0.072418212890625,\n",
              " 0.33111572265625,\n",
              " 0.32855224609375,\n",
              " 0.1925048828125,\n",
              " 0.1171875,\n",
              " 0.20208740234375,\n",
              " 0.342437744140625,\n",
              " 0.39923095703125,\n",
              " 0.0411376953125,\n",
              " 0.272216796875,\n",
              " 0.2038726806640625,\n",
              " 0.346588134765625,\n",
              " 0.0543060302734375,\n",
              " -0.02475738525390625,\n",
              " 0.1751708984375,\n",
              " 0.435272216796875,\n",
              " 0.118896484375,\n",
              " 0.04632568359375,\n",
              " 0.132476806640625,\n",
              " 0.0357666015625,\n",
              " -0.07098388671875,\n",
              " -0.0218505859375,\n",
              " 0.1949462890625,\n",
              " 0.074737548828125,\n",
              " 0.0341796875,\n",
              " 0.57781982421875,\n",
              " 0.5452880859375,\n",
              " 0.6068878173828125,\n",
              " 0.662353515625,\n",
              " 0.02667236328125,\n",
              " -0.031768798828125,\n",
              " 0.38592529296875,\n",
              " 0.024505615234375,\n",
              " 0.19873046875,\n",
              " 0.37152099609375,\n",
              " 0.672210693359375,\n",
              " -0.0222625732421875,\n",
              " 0.030914306640625,\n",
              " 0.138916015625,\n",
              " 0.281890869140625,\n",
              " 0.1019134521484375,\n",
              " -0.0018310546875,\n",
              " 0.495880126953125,\n",
              " 0.11322021484375,\n",
              " 0.031158447265625,\n",
              " 0.222930908203125,\n",
              " 0.067535400390625,\n",
              " 0.17034912109375,\n",
              " 0.109222412109375,\n",
              " 0.14019775390625,\n",
              " 0.479736328125,\n",
              " 0.082275390625,\n",
              " 0.4101715087890625,\n",
              " 0.0113525390625,\n",
              " 0.34149169921875,\n",
              " 0.4073486328125,\n",
              " 0.599578857421875,\n",
              " 0.634033203125,\n",
              " 0.5244598388671875,\n",
              " 0.2548828125,\n",
              " 0.018829345703125,\n",
              " -0.015960693359375,\n",
              " 0.8513031005859375,\n",
              " 0.1027679443359375,\n",
              " 0.7118988037109375,\n",
              " 0.136688232421875,\n",
              " 0.0795440673828125,\n",
              " 0.141326904296875,\n",
              " -0.041839599609375,\n",
              " 0.03125,\n",
              " 0.14764404296875,\n",
              " 0.0013427734375,\n",
              " 0.452423095703125,\n",
              " 0.281494140625,\n",
              " 0.0157470703125,\n",
              " 0.37579345703125,\n",
              " 0.462646484375,\n",
              " 0.10455322265625,\n",
              " 0.490081787109375,\n",
              " -0.019588470458984375,\n",
              " 0.36358642578125,\n",
              " 0.25341796875,\n",
              " -0.01129150390625,\n",
              " 0.090667724609375,\n",
              " 0.00799560546875,\n",
              " 0.23211669921875,\n",
              " 0.635833740234375,\n",
              " 0.042388916015625,\n",
              " -0.017822265625,\n",
              " 0.737884521484375,\n",
              " 0.1996307373046875,\n",
              " 0.01727294921875,\n",
              " 0.47869873046875,\n",
              " 0.12921142578125,\n",
              " 0.4167633056640625,\n",
              " 0.079986572265625,\n",
              " -0.032958984375,\n",
              " 0.06219482421875,\n",
              " 0.703704833984375,\n",
              " 0.215057373046875,\n",
              " 0.1162109375,\n",
              " 0.1947021484375,\n",
              " 0.3189697265625,\n",
              " -0.1135101318359375,\n",
              " 0.15740966796875,\n",
              " 0.22772216796875,\n",
              " 0.055267333984375,\n",
              " 0.07330322265625,\n",
              " 0.401702880859375,\n",
              " 0.0263671875,\n",
              " 0.4642333984375,\n",
              " 0.163299560546875,\n",
              " 0.582672119140625,\n",
              " 0.5488433837890625,\n",
              " 0.098602294921875,\n",
              " 0.452545166015625,\n",
              " 0.707244873046875,\n",
              " 0.058624267578125,\n",
              " 0.01708984375,\n",
              " 0.024322509765625,\n",
              " 0.6922454833984375,\n",
              " 0.10595703125,\n",
              " 0.483367919921875,\n",
              " 0.29833984375,\n",
              " 0.05926513671875,\n",
              " 0.18804931640625,\n",
              " 0.094573974609375,\n",
              " 0.296630859375,\n",
              " 0.34515380859375,\n",
              " 0.406707763671875,\n",
              " 0.005096435546875,\n",
              " -0.0014495849609375,\n",
              " -0.007965087890625,\n",
              " 0.0228271484375,\n",
              " 0.14593505859375,\n",
              " 0.094207763671875,\n",
              " 0.5889892578125,\n",
              " 0.42779541015625,\n",
              " 0.0662841796875,\n",
              " 0.253814697265625,\n",
              " 0.2469329833984375,\n",
              " -0.0526123046875,\n",
              " 0.159637451171875,\n",
              " 0.320648193359375,\n",
              " 0.477142333984375,\n",
              " 0.0525970458984375,\n",
              " 0.785400390625,\n",
              " 0.021026611328125,\n",
              " 0.204071044921875,\n",
              " -0.05035400390625,\n",
              " -0.0042724609375,\n",
              " 0.758544921875,\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['summa_c_scores'] = summac_scores"
      ],
      "metadata": {
        "id": "mvLDgTQTPiyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting SummaC scores\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(df['summa_c_scores'], bins=20, color='green', alpha=0.7)\n",
        "plt.title('Distribution of SummaC Scores')\n",
        "plt.xlabel('SummaC Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_NivxEc3i2I0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNAsvO5FDI1U"
      },
      "source": [
        "# **BERT SCORE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 775
        },
        "id": "HeGu4_GcD399",
        "outputId": "38220c94-876b-4930-ea0b-5e493f589e6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Collecting transformers\n",
            "  Using cached transformers-4.40.1-py3-none-any.whl (9.0 MB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers)\n",
            "  Using cached huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
            "  Using cached tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Installing collected packages: huggingface-hub, tokenizers, transformers\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.17.0\n",
            "    Uninstalling huggingface-hub-0.17.0:\n",
            "      Successfully uninstalled huggingface-hub-0.17.0\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.2\n",
            "    Uninstalling tokenizers-0.15.2:\n",
            "      Successfully uninstalled tokenizers-0.15.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.35.2\n",
            "    Uninstalling transformers-4.35.2:\n",
            "      Successfully uninstalled transformers-4.35.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "summac 0.0.4 requires huggingface-hub<=0.17.0, but you have huggingface-hub 0.23.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed huggingface-hub-0.23.0 tokenizers-0.19.1 transformers-4.40.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "huggingface_hub",
                  "transformers"
                ]
              },
              "id": "4a98e93cc40248388715f0460a3859b5"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install --upgrade transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQEWO0103KCT",
        "outputId": "1d411791-b282-41c1-d3fd-2557bc371806"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Fit the TF-IDF vectorizer on the entire corpus of text data\n",
        "corpus = df['preprocessed_patent'].tolist() + df['preprocessed_summary'].tolist()\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit(corpus)\n",
        "\n",
        "# Implement BERT score calculation\n",
        "bert_scores = []\n",
        "for index, row in df.iterrows():\n",
        "    _, _, f1 = score([row['preprocessed_patent']], [row['preprocessed_summary']], lang='en')  # Specify the language as 'en' (English)\n",
        "    bert_scores.append(f1.item())  # Assuming you're interested in F1 score\n",
        "\n",
        "df['bert_scores'] = bert_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKeGdh_EJD2k"
      },
      "outputs": [],
      "source": [
        "# Now use BERT model to compare the two rows: Patent Document and Patent Summary\n",
        "def calculate_bert_score(document, summary):\n",
        "    _, _, bert_scores = score([summary], [document], lang='en', verbose=False)\n",
        "    return bert_scores.mean().item()\n",
        "\n",
        "# Calculate BERT scores for each row\n",
        "bert_scores = []\n",
        "for index, row in df.iterrows():\n",
        "    bert_score = calculate_bert_score(row['preprocessed_patent'], row['preprocessed_summary'])\n",
        "    bert_scores.append(bert_score)\n",
        "\n",
        "# Assign BERT scores to DataFrame\n",
        "df['bert_scores'] = bert_scores\n",
        "\n",
        "# Display DataFrame with BERT scores\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting BERT scores\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(df['bert_scores'], bins=20, color='red', alpha=0.7, label='BERT Scores')\n",
        "plt.title('Distribution of BERT Scores')\n",
        "plt.xlabel('Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RzhqQmPdvdd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmywC9nhx79p"
      },
      "source": [
        "# **BLEU SCORE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "pc8EvD3t1MNg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "483ed772-05f7-4d57-bd52-8ca5944de2e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ],
      "source": [
        "# Compute BLEU score\n",
        "bleu_scores = []\n",
        "for index, row in df.iterrows():\n",
        "    candidate = row['preprocessed_summary'].split()\n",
        "    reference = row['preprocessed_patent'].split()\n",
        "    bleu = sentence_bleu([reference], candidate)\n",
        "    bleu_scores.append(bleu)\n",
        "\n",
        "df['bleu_scores'] = bleu_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "IZXq3xUE2wCS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "f512563b-4f5d-48e0-cfc3-8fede9cdb4d1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Plotting BLEU scores\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(df['bleu_scores'], bins=20, color='blue', alpha=0.7)\n",
        "plt.title('Distribution of BLEU Scores')\n",
        "plt.xlabel('BLEU Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfeFRfYuyptk"
      },
      "source": [
        "# **METEOR SCORE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "1NM4Zt565IKK"
      },
      "outputs": [],
      "source": [
        "# Compute METEOR score\n",
        "meteor_scores = []\n",
        "for index, row in df.iterrows():\n",
        "    hypothesis_tokens = word_tokenize(row['preprocessed_summary'])\n",
        "    reference_tokens = word_tokenize(row['preprocessed_patent'])\n",
        "    meteor = single_meteor_score(reference_tokens, hypothesis_tokens)\n",
        "    meteor_scores.append(meteor)\n",
        "\n",
        "df['meteor_scores'] = meteor_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "BLD27g725QKw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "0d2cd401-a85b-4127-b182-8d8770d29ec2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Plotting METEOR scores\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(df['meteor_scores'], bins=20, color='green', alpha=0.7)\n",
        "plt.title('Distribution of METEOR Scores')\n",
        "plt.xlabel('METEOR Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf9Dazxkyx7Y"
      },
      "source": [
        "# **COSINE SIMILARITY**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "23qM8szp5LBQ"
      },
      "outputs": [],
      "source": [
        "# Fit the TF-IDF vectorizer on the entire corpus of text data\n",
        "corpus = df['preprocessed_patent'].tolist() + df['preprocessed_summary'].tolist()\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit(corpus)\n",
        "\n",
        "# Define text_to_vector function using the fitted TF-IDF vectorizer\n",
        "def text_to_vector(text):\n",
        "    vector = vectorizer.transform([text])\n",
        "    return vector.toarray()[0]\n",
        "\n",
        "# Compute cosine similarity\n",
        "cosine_similarities = []\n",
        "for index, row in df.iterrows():\n",
        "    candidate = row['preprocessed_summary']\n",
        "    reference = row['preprocessed_patent']\n",
        "    candidate_vector = text_to_vector(candidate)\n",
        "    reference_vector = text_to_vector(reference)\n",
        "    cosine_sim = cosine_similarity([candidate_vector], [reference_vector])[0][0]\n",
        "    cosine_similarities.append(cosine_sim)\n",
        "\n",
        "df['cosine_similarity'] = cosine_similarities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "hn0EvI5W5Rxx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "e2c58c77-6b63-4ede-dc7f-8bbf1116e170"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Plotting cosine similarity scores\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(df['cosine_similarity'], bins=20, color='orange', alpha=0.7)\n",
        "plt.title('Distribution of Cosine Similarity Scores')\n",
        "plt.xlabel('Cosine Similarity')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjAmXIDCLblJ"
      },
      "source": [
        "# **SUMAC SCORE**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(df[[\"rouge_scores\", \"summa_c_scores\", \"bleu_scores\", \"cosine_similarity\"]].head(1100))"
      ],
      "metadata": {
        "id": "PyZlKr66Q4IR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "goitiwJKRIDq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}